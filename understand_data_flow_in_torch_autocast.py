# -*- coding: utf-8 -*-
"""understand data flow in torch autocast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GyKFR6S34c5I57WeCqIKQxhXr-Dq4l7

In this assignment, we are going to learn how torch autocast affects the workflow during the forward pass pf the models. For this exercise, we look into OPT-125M model. Let's first load the model and tokenizer using HF.
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "facebook/OPT-125M"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()

# let's decide on the 16bit dtype we want to use. Not all GPUs support bfloat16
dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

"""In order to see the workflow, we will print the input and output of each layer. For this we add the following hook to the target layers' forward function."""

# Add hooks to track dtypes
import torch.nn as nn
WATCH = (nn.Linear, nn.LayerNorm, nn.Embedding)
hooks = []
def make_hook(name):
    def hook(m, inp, out):
        indt = inp[0].dtype if isinstance(inp, (tuple, list)) else getattr(inp, 'dtype', None)
        outdt = out[0].dtype if isinstance(out, (tuple, list)) else getattr(out, 'dtype', None)
        print(f"[{name}] in={indt} -> out={outdt}")
    return hook

for n, m in model.named_modules():
    if isinstance(m, WATCH):
        hooks.append(m.register_forward_hook(make_hook(n)))

"""Let's define the sample input that we want to run through the model."""

# sample input
text = "Profiling AMP forward pass."
inputs = tokenizer(text, return_tensors="pt").to("cuda")

"""We want to run the model in 4 settings:
- No torch autocast   and casting the model parameters to FP32.
- No torch autocast   and casting the model parameters to BF16 or FP16.
- With torch autocast and casting the model parameters to FP32.
- With torch autocast and casting the model parameters to BF16 or FP16.
"""

# No torch autocast   and casting the model parameters to FP32
# See how the entire model runs in FP32
model = model.to(torch.float32)
with torch.inference_mode():
    _ = model(**inputs)

# [int64 tokens]  (shared input)
#         |
#    +----+---------------------+
#    |                          |
# [embed_tok] int64‚Üífp32    [embed_pos] int64‚Üífp32
#    |                          |
#    +-------- sum (fp32+fp32‚Üífp32) --------+
#                                           |
#                                         [LN1] fp32‚Üífp32
#                                           |
#                       +---------+---------+---------+
#                       |         |                   |
#                    [q_proj]  [k_proj]           [v_proj]
#                     fp32‚Üífp32  fp32‚Üífp32         fp32‚Üífp32
#                       \         |                 /
#                        \        |                /
#                         +----[attn + softmax]----+   (fp32‚Üífp32)
#                                           |
#                                     [out_proj] fp32‚Üífp32
#                                           |
#                                  (residual add) fp32‚Üífp32
#                                           |
#                                         [LN2] fp32‚Üífp32
#                                           |
#                                        [fc1] fp32‚Üífp32
#                                           |
#                                        [fc2] fp32‚Üífp32
#                                           |
#                                  (residual add) fp32‚Üífp32

# No torch autocast and casting the model parameters to BF16 or FP16.
# See how the entire model runs in BF16
model = model.to(dtype_16bit)
with torch.inference_mode():
    _ = model(**inputs)
# [int64 tokens]  (shared input)
#         |
#    +----+---------------------+
#    |                          |
# [embed_tok] int64‚Üíbf16    [embed_pos] int64‚Üíbf16
#    |                          |
#    +-------- sum (bf16+bf16‚Üíbf16) --------+
#                                           |
#                                         [LN1] bf16‚Üíbf16
#                                           |
#                       +---------+---------+---------+
#                       |         |                   |
#                    [q_proj]  [k_proj]           [v_proj]
#                     bf16‚Üíbf16  bf16‚Üíbf16         bf16‚Üíbf16
#                       \         |                 /
#                        \        |                /
#                         +----[attn + softmax]----+   (bf16‚Üíbf16)
#                                           |
#                                     [out_proj] bf16‚Üíbf16
#                                           |
#                                  (residual add) bf16‚Üíbf16
#                                           |
#                                         [LN2] bf16‚Üíbf16
#                                           |
#                                        [fc1] bf16‚Üíbf16
#                                           |
#                                        [fc2] bf16‚Üíbf16
#                                           |
#                                  (residual add) bf16‚Üíbf16

# With torch autocast and casting the model parameters to BF16 or FP16
# See how the LN runs in FP32 and how linear layers run in BF16 even though their inputs are in FP32!
model = model.to(dtype_16bit)
with torch.inference_mode():
    with torch.autocast(device_type="cuda", dtype=dtype_16bit):
        _ = model(**inputs)

# [int64 tokens]  (shared input)
#         |
#    +----+---------------------+
#    |                          |
# [embed_tok] int64‚Üíbf16    [embed_pos] int64‚Üíbf16
#    |                          |
#    +-------- sum (bf16+bf16‚Üíbf16) --------+
#                                           |
#                                         [LN1] bf16‚Üífp32
#                                           |
#                       +---------+---------+---------+
#                       |         |                   |
#                    [q_proj]  [k_proj]           [v_proj]
#                     fp32‚Üíbf16  fp32‚Üíbf16         fp32‚Üíbf16
#                       \         |                 /
#                        \        |                /
#                         +----[attn + softmax]----+   (bf16‚Üíbf16)
#                                           |
#                                     [out_proj] bf16‚Üíbf16
#                                           |
#                 (residual add with skip: bf16 + bf16 ‚Üí bf16)
#                                           |
#                                         [LN2] bf16‚Üífp32
#                                           |
#                                        [fc1] fp32‚Üíbf16
#                                           |
#                                        [fc2] bf16‚Üíbf16
#                                           |
#                 (residual add with skip: bf16 + bf16 ‚Üí bf16)

# With torch autocast and casting the model parameters to FP32
# See how linear layers run in BF16 even though their parameters are in FP32!
# See how the residual sum runs in FP32 even though one input is in BF16 and the other in FP32.
model = model.to(torch.float32)
with torch.inference_mode():
    with torch.autocast(device_type="cuda", dtype=dtype_16bit):
        _ = model(**inputs)
# [int64 tokens]  (shared input)
#         |
#    +----+---------------------+
#    |                          |
# [embed_tok] int64‚Üífp32    [embed_pos] int64‚Üífp32
#    |                          |
#    +-------- sum (fp32+fp32‚Üífp32) --------+
#                                           |
#                                         [LN1] fp32‚Üífp32
#                                           |
#                       +---------+---------+---------+
#                       |         |                   |
#                    [q_proj]  [k_proj]           [v_proj]
#                     fp32‚Üíbf16  fp32‚Üíbf16         fp32‚Üíbf16
#                       \         |                 /
#                        \        |                /
#                         +----[attn + softmax]----+   (bf16‚Üíbf16)
#                                           |
#                                     [out_proj] bf16‚Üíbf16
#                                           |
#               (residual add: bf16 + fp32 ‚Üí fp32)   ‚Üê promotion to fp32
#                                           |
#                                         [LN2] fp32‚Üífp32
#                                           |
#                                        [fc1] fp32‚Üíbf16
#                                           |
#                                        [fc2] bf16‚Üíbf16
#                                           |
#               (residual add: bf16 + fp32 ‚Üí fp32)   ‚Üê promotion to fp32

"""We saw above that it is possible to feed a bfloat16 (bf16) tensor as input to a linear layer whose weights are stored in float32 (fp32), and the operation completes successfully.

This works because, under the hood, automatic mixed precision (AMP) autocast intercepts the call to the linear kernel and casts the weights to bf16 so that the matrix multiplication can proceed with matching dtypes. In other words, autocast ensures type compatibility by automatically downcasting fp32 weights to bf16 on-the-fly, without requiring explicit intervention from the user.

In the absence of autocast, such an operation would raise a runtime error. PyTorch enforces strict dtype checks: you cannot multiply an fp32 weight matrix with a bf16 input tensor directly, since there‚Äôs no implicit casting at the operator boundary.
"""

layer = torch.nn.Linear(2, 2).to(device="cuda", dtype=dtype_16bit) # 16bit layer
inp = torch.randn(2, 2).to(device="cuda", dtype=torch.float32) # 32bit input
try:
    out = layer(inp)
except RuntimeError as e:
    print("ERROR!", e)

"""Let's try again with autocast"""

layer = torch.nn.Linear(2, 2).to(device="cuda", dtype=dtype_16bit) # 16bit layer
inp = torch.randn(2, 2).to(device="cuda", dtype=torch.float32) # 32bit input
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = layer(inp)
print("SUCCESS!")

"""We can also see how autocast affect the loss computation. Let's check the MSE loss."""

gt = torch.randn(2, 2).to(device="cuda", dtype=dtype_16bit)
pred = torch.randn(2, 2).to(device="cuda", dtype=dtype_16bit)

# without autocast
out = nn.MSELoss()(pred, gt)
print(out.dtype) # dtype_16bit

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = nn.MSELoss()(pred, gt)
print(out.dtype) # FP32

"""No active policy for ReLU, that is, whatever data format goes in, same comes out. See below."""

import torch
import torch.nn as nn
dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

x16 = torch.randn(2, 2).to(device="cuda", dtype=dtype_16bit)
x32 = torch.randn(2, 2).to(device="cuda", dtype=torch.float32)

# without autocast
out = nn.ReLU()(x16)
print(out.dtype) # dtype_16bit

# without autocast
out = nn.ReLU()(x32)
print(out.dtype) # FP32

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = nn.ReLU()(x16)
print(out.dtype) # dtype_16bit

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = nn.ReLU()(x32)
print(out.dtype) # FP32

"""FP32 policy for Softmax, that is, whatever data format goes in, FP32 comes out. See below."""

import torch
import torch.nn as nn
dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

x16 = torch.randn(2, 2).to(device="cuda", dtype=dtype_16bit)
x32 = torch.randn(2, 2).to(device="cuda", dtype=torch.float32)

# without autocast
out = nn.Softmax()(x16)
print(out.dtype) # dtype_16bit

# without autocast
out = nn.Softmax()(x32)
print(out.dtype) # FP32

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = nn.Softmax()(x16)
print(out.dtype) # FP32

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out = nn.Softmax()(x32)
print(out.dtype) # FP32

"""üòé Now, let's have some fun! What do you think the output data type would be for the following operations? Why sum and mean are different?!"""

import torch
import torch.nn as nn
dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

x16 = torch.randn(2, 20000).to(device="cuda", dtype=dtype_16bit)

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out_max = x16.max()
    out_min = x16.min()
    out_sum = x16.sum()
    out_mean = x16.mean()
    out_prod = x16.prod()
    out_exp = x16.exp()
    out_log = x16.log()

    print("out_max:", out_max.dtype)
    print("out_min", out_min.dtype)
    print("out_sum:", out_sum.dtype)
    print("out_mean", out_mean.dtype)
    print("out_prod:", out_prod.dtype)
    print("out_exp:", out_exp.dtype)
    print("out_log:", out_log.dtype)

import torch
import torch.nn as nn
dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

x32 = torch.randn(2, 20000).to(device="cuda", dtype=torch.float32)

# with autocast
with torch.autocast(device_type="cuda", dtype=dtype_16bit):
    out_relu = nn.ReLU()(x32)
    print("out_relu:", out_relu.dtype)


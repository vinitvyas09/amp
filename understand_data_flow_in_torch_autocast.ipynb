{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y8v9vXRc_vY"
   },
   "source": [
    "In this assignment, we are going to learn how torch autocast affects the workflow during the forward pass pf the models. For this exercise, we look into OPT-125M model. Let's first load the model and tokenizer using HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aaHaJ4S6c_vc",
    "outputId": "bff0a48e-75c9-4f05-80e1-91cf7bc38bc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106b00ab48d94021927114e8e6472aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/OPT-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()\n",
    "\n",
    "# let's decide on the 16bit dtype we want to use. Not all GPUs support bfloat16\n",
    "dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd57bwU3c_ve"
   },
   "source": [
    "In order to see the workflow, we will print the input and output of each layer. For this we add the following hook to the target layers' forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5ok4bX31c_vf"
   },
   "outputs": [],
   "source": [
    "# Add hooks to track dtypes\n",
    "import torch.nn as nn\n",
    "WATCH = (nn.Linear, nn.LayerNorm, nn.Embedding)\n",
    "hooks = []\n",
    "def make_hook(name):\n",
    "    def hook(m, inp, out):\n",
    "        indt = inp[0].dtype if isinstance(inp, (tuple, list)) else getattr(inp, 'dtype', None)\n",
    "        outdt = out[0].dtype if isinstance(out, (tuple, list)) else getattr(out, 'dtype', None)\n",
    "        print(f\"[{name}] in={indt} -> out={outdt}\")\n",
    "    return hook\n",
    "\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, WATCH):\n",
    "        hooks.append(m.register_forward_hook(make_hook(n)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX_D3wlbc_vg"
   },
   "source": [
    "Let's define the sample input that we want to run through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r-DTfhhUc_vi"
   },
   "outputs": [],
   "source": [
    "# sample input\n",
    "text = \"Profiling AMP forward pass.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NJilJFYc_vk"
   },
   "source": [
    "We want to run the model in 4 settings:\n",
    "- No torch autocast   and casting the model parameters to FP32.\n",
    "- No torch autocast   and casting the model parameters to BF16 or FP16.\n",
    "- With torch autocast and casting the model parameters to FP32.\n",
    "- With torch autocast and casting the model parameters to BF16 or FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zNnvFmr0c_vl",
    "outputId": "2ff32812-af8c-499e-f820-ad255239eb2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model.decoder.embed_tokens] in=torch.int64 -> out=torch.float32\n",
      "[model.decoder.embed_positions] in=torch.int64 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.q_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.k_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.v_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.out_proj] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.fc1] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.fc2] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[lm_head] in=torch.float32 -> out=torch.float32\n"
     ]
    }
   ],
   "source": [
    "# No torch autocast   and casting the model parameters to FP32\n",
    "# See how the entire model runs in FP32\n",
    "model = model.to(torch.float32)\n",
    "with torch.inference_mode():\n",
    "    _ = model(**inputs)\n",
    "\n",
    "# [int64 tokens]  (shared input)\n",
    "#         |\n",
    "#    +----+---------------------+\n",
    "#    |                          |\n",
    "# [embed_tok] int64→fp32    [embed_pos] int64→fp32\n",
    "#    |                          |\n",
    "#    +-------- sum (fp32+fp32→fp32) --------+\n",
    "#                                           |\n",
    "#                                         [LN1] fp32→fp32\n",
    "#                                           |\n",
    "#                       +---------+---------+---------+\n",
    "#                       |         |                   |\n",
    "#                    [q_proj]  [k_proj]           [v_proj]\n",
    "#                     fp32→fp32  fp32→fp32         fp32→fp32\n",
    "#                       \\         |                 /\n",
    "#                        \\        |                /\n",
    "#                         +----[attn + softmax]----+   (fp32→fp32)\n",
    "#                                           |\n",
    "#                                     [out_proj] fp32→fp32\n",
    "#                                           |\n",
    "#                                  (residual add) fp32→fp32\n",
    "#                                           |\n",
    "#                                         [LN2] fp32→fp32\n",
    "#                                           |\n",
    "#                                        [fc1] fp32→fp32\n",
    "#                                           |\n",
    "#                                        [fc2] fp32→fp32\n",
    "#                                           |\n",
    "#                                  (residual add) fp32→fp32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z3Sloq5nc_vm",
    "outputId": "cbeaf32e-f6d2-4300-8fec-8f56ec3954a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model.decoder.embed_tokens] in=torch.int64 -> out=torch.bfloat16\n",
      "[model.decoder.embed_positions] in=torch.int64 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.q_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.k_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.v_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.fc1] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.final_layer_norm] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[lm_head] in=torch.bfloat16 -> out=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# No torch autocast and casting the model parameters to BF16 or FP16.\n",
    "# See how the entire model runs in BF16\n",
    "model = model.to(dtype_16bit)\n",
    "with torch.inference_mode():\n",
    "    _ = model(**inputs)\n",
    "# [int64 tokens]  (shared input)\n",
    "#         |\n",
    "#    +----+---------------------+\n",
    "#    |                          |\n",
    "# [embed_tok] int64→bf16    [embed_pos] int64→bf16\n",
    "#    |                          |\n",
    "#    +-------- sum (bf16+bf16→bf16) --------+\n",
    "#                                           |\n",
    "#                                         [LN1] bf16→bf16\n",
    "#                                           |\n",
    "#                       +---------+---------+---------+\n",
    "#                       |         |                   |\n",
    "#                    [q_proj]  [k_proj]           [v_proj]\n",
    "#                     bf16→bf16  bf16→bf16         bf16→bf16\n",
    "#                       \\         |                 /\n",
    "#                        \\        |                /\n",
    "#                         +----[attn + softmax]----+   (bf16→bf16)\n",
    "#                                           |\n",
    "#                                     [out_proj] bf16→bf16\n",
    "#                                           |\n",
    "#                                  (residual add) bf16→bf16\n",
    "#                                           |\n",
    "#                                         [LN2] bf16→bf16\n",
    "#                                           |\n",
    "#                                        [fc1] bf16→bf16\n",
    "#                                           |\n",
    "#                                        [fc2] bf16→bf16\n",
    "#                                           |\n",
    "#                                  (residual add) bf16→bf16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T1YAH2pmc_vn",
    "outputId": "7dede868-d0ae-41b3-c1f1-440b458581d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model.decoder.embed_tokens] in=torch.int64 -> out=torch.bfloat16\n",
      "[model.decoder.embed_positions] in=torch.int64 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.0.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.1.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.2.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.3.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.4.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.5.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.6.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.7.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.8.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.9.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.10.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[model.decoder.layers.11.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.final_layer_norm] in=torch.bfloat16 -> out=torch.float32\n",
      "[lm_head] in=torch.float32 -> out=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# With torch autocast and casting the model parameters to BF16 or FP16\n",
    "# See how the LN runs in FP32 and how linear layers run in BF16 even though their inputs are in FP32!\n",
    "model = model.to(dtype_16bit)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "        _ = model(**inputs)\n",
    "\n",
    "# [int64 tokens]  (shared input)\n",
    "#         |\n",
    "#    +----+---------------------+\n",
    "#    |                          |\n",
    "# [embed_tok] int64→bf16    [embed_pos] int64→bf16\n",
    "#    |                          |\n",
    "#    +-------- sum (bf16+bf16→bf16) --------+\n",
    "#                                           |\n",
    "#                                         [LN1] bf16→fp32\n",
    "#                                           |\n",
    "#                       +---------+---------+---------+\n",
    "#                       |         |                   |\n",
    "#                    [q_proj]  [k_proj]           [v_proj]\n",
    "#                     fp32→bf16  fp32→bf16         fp32→bf16\n",
    "#                       \\         |                 /\n",
    "#                        \\        |                /\n",
    "#                         +----[attn + softmax]----+   (bf16→bf16)\n",
    "#                                           |\n",
    "#                                     [out_proj] bf16→bf16\n",
    "#                                           |\n",
    "#                 (residual add with skip: bf16 + bf16 → bf16)\n",
    "#                                           |\n",
    "#                                         [LN2] bf16→fp32\n",
    "#                                           |\n",
    "#                                        [fc1] fp32→bf16\n",
    "#                                           |\n",
    "#                                        [fc2] bf16→bf16\n",
    "#                                           |\n",
    "#                 (residual add with skip: bf16 + bf16 → bf16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CFmsMsgRc_vn",
    "outputId": "323257c0-ccd0-4f66-a521-c7ddfe4d0e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model.decoder.embed_tokens] in=torch.int64 -> out=torch.float32\n",
      "[model.decoder.embed_positions] in=torch.int64 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.0.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.0.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.1.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.1.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.2.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.2.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.3.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.3.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.4.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.4.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.5.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.5.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.6.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.6.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.7.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.7.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.8.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.8.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.9.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.9.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.10.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.10.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.self_attn.q_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.k_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.v_proj] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.self_attn.out_proj] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[model.decoder.layers.11.fc1] in=torch.float32 -> out=torch.bfloat16\n",
      "[model.decoder.layers.11.fc2] in=torch.bfloat16 -> out=torch.bfloat16\n",
      "[model.decoder.final_layer_norm] in=torch.float32 -> out=torch.float32\n",
      "[lm_head] in=torch.float32 -> out=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# With torch autocast and casting the model parameters to FP32\n",
    "# See how linear layers run in BF16 even though their parameters are in FP32!\n",
    "# See how the residual sum runs in FP32 even though one input is in BF16 and the other in FP32.\n",
    "model = model.to(torch.float32)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "        _ = model(**inputs)\n",
    "# [int64 tokens]  (shared input)\n",
    "#         |\n",
    "#    +----+---------------------+\n",
    "#    |                          |\n",
    "# [embed_tok] int64→fp32    [embed_pos] int64→fp32\n",
    "#    |                          |\n",
    "#    +-------- sum (fp32+fp32→fp32) --------+\n",
    "#                                           |\n",
    "#                                         [LN1] fp32→fp32\n",
    "#                                           |\n",
    "#                       +---------+---------+---------+\n",
    "#                       |         |                   |\n",
    "#                    [q_proj]  [k_proj]           [v_proj]\n",
    "#                     fp32→bf16  fp32→bf16         fp32→bf16\n",
    "#                       \\         |                 /\n",
    "#                        \\        |                /\n",
    "#                         +----[attn + softmax]----+   (bf16→bf16)\n",
    "#                                           |\n",
    "#                                     [out_proj] bf16→bf16\n",
    "#                                           |\n",
    "#               (residual add: bf16 + fp32 → fp32)   ← promotion to fp32\n",
    "#                                           |\n",
    "#                                         [LN2] fp32→fp32\n",
    "#                                           |\n",
    "#                                        [fc1] fp32→bf16\n",
    "#                                           |\n",
    "#                                        [fc2] bf16→bf16\n",
    "#                                           |\n",
    "#               (residual add: bf16 + fp32 → fp32)   ← promotion to fp32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XykynLqEc_vo"
   },
   "source": [
    "We saw above that it is possible to feed a bfloat16 (bf16) tensor as input to a linear layer whose weights are stored in float32 (fp32), and the operation completes successfully.\n",
    "\n",
    "This works because, under the hood, automatic mixed precision (AMP) autocast intercepts the call to the linear kernel and casts the weights to bf16 so that the matrix multiplication can proceed with matching dtypes. In other words, autocast ensures type compatibility by automatically downcasting fp32 weights to bf16 on-the-fly, without requiring explicit intervention from the user.\n",
    "\n",
    "In the absence of autocast, such an operation would raise a runtime error. PyTorch enforces strict dtype checks: you cannot multiply an fp32 weight matrix with a bf16 input tensor directly, since there’s no implicit casting at the operator boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pL5lv1lFc_vp",
    "outputId": "c8251d78-1eda-4651-8f1a-0b265f6f6305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! mat1 and mat2 must have the same dtype, but got Float and BFloat16\n"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.Linear(2, 2).to(device=\"cuda\", dtype=dtype_16bit) # 16bit layer\n",
    "inp = torch.randn(2, 2).to(device=\"cuda\", dtype=torch.float32) # 32bit input\n",
    "try:\n",
    "    out = layer(inp)\n",
    "except RuntimeError as e:\n",
    "    print(\"ERROR!\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGiTBWUnc_vq"
   },
   "source": [
    "Let's try again with autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GH6cO1jcc_vq",
    "outputId": "6ce8d27f-2733-41fb-9899-28dc17bf425a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.Linear(2, 2).to(device=\"cuda\", dtype=dtype_16bit) # 16bit layer\n",
    "inp = torch.randn(2, 2).to(device=\"cuda\", dtype=torch.float32) # 32bit input\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = layer(inp)\n",
    "print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWkr8N4g49cM"
   },
   "source": [
    "We can also see how autocast affect the loss computation. Let's check the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "n9XlgAuvc_vq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3906, device='cuda:0', dtype=torch.bfloat16)\n",
      "torch.bfloat16\n",
      "torch.float32\n",
      "tensor(2.3890, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "gt = torch.randn(2, 2).to(device=\"cuda\", dtype=dtype_16bit)\n",
    "pred = torch.randn(2, 2).to(device=\"cuda\", dtype=dtype_16bit)\n",
    "\n",
    "# without autocast\n",
    "out = nn.MSELoss()(pred, gt)\n",
    "print(out)\n",
    "print(out.dtype) # dtype_16bit\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = nn.MSELoss()(pred, gt)\n",
    "print(out.dtype) # FP32\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEiHxWQl07ki"
   },
   "source": [
    "No active policy for ReLU, that is, whatever data format goes in, same comes out. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3bVTQolzuRa",
    "outputId": "759a4138-7677-4bdc-d9e8-0b2cd2b94601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.float32\n",
      "torch.bfloat16\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "x16 = torch.randn(2, 2).to(device=\"cuda\", dtype=dtype_16bit)\n",
    "x32 = torch.randn(2, 2).to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# without autocast\n",
    "out = nn.ReLU()(x16)\n",
    "print(out.dtype) # dtype_16bit\n",
    "\n",
    "# without autocast\n",
    "out = nn.ReLU()(x32)\n",
    "print(out.dtype) # FP32\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = nn.ReLU()(x16)\n",
    "print(out.dtype) # dtype_16bit\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = nn.ReLU()(x32)\n",
    "print(out.dtype) # FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwuBuOFQ1kEE"
   },
   "source": [
    "FP32 policy for Softmax, that is, whatever data format goes in, FP32 comes out. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pcz8iX9c0kx3",
    "outputId": "aa027c5a-878f-45f6-d7ad-e239aba2f09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1776: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "x16 = torch.randn(2, 2).to(device=\"cuda\", dtype=dtype_16bit)\n",
    "x32 = torch.randn(2, 2).to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# without autocast\n",
    "out = nn.Softmax()(x16)\n",
    "print(out.dtype) # dtype_16bit\n",
    "\n",
    "# without autocast\n",
    "out = nn.Softmax()(x32)\n",
    "print(out.dtype) # FP32\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = nn.Softmax()(x16)\n",
    "print(out.dtype) # FP32\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out = nn.Softmax()(x32)\n",
    "print(out.dtype) # FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCZN7kwWo8Lp"
   },
   "source": [
    "😎 Now, let's have some fun! What do you think the output data type would be for the following operations? Why sum and mean are different?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5rQ7A9Q1cHp",
    "outputId": "4da45d92-8a9e-4aa8-f6a2-d23f30f2cdfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_max: torch.bfloat16 tensor(4.4688, device='cuda:0', dtype=torch.bfloat16)\n",
      "out_min: torch.bfloat16 tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16)\n",
      "out_sum: torch.float32 tensor(221.7129, device='cuda:0')\n",
      "out_mean: torch.bfloat16 tensor(0.0056, device='cuda:0', dtype=torch.bfloat16)\n",
      "out_prod: torch.float32 tensor(-0., device='cuda:0')\n",
      "out_exp: torch.float32 torch.Size([2, 20000])\n",
      "out_log: torch.float32 torch.Size([2, 20000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "x16 = torch.randn(2, 20000).to(device=\"cuda\", dtype=dtype_16bit)\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out_max = x16.max()\n",
    "    out_min = x16.min()\n",
    "    out_sum = x16.sum()\n",
    "    out_mean = x16.mean()\n",
    "    out_prod = x16.prod()\n",
    "    out_exp = x16.exp()\n",
    "    out_log = x16.log()\n",
    "\n",
    "    print(\"out_max:\", out_max.dtype, out_max)\n",
    "    print(\"out_min:\", out_min.dtype, out_min)\n",
    "    print(\"out_sum:\", out_sum.dtype, out_sum)\n",
    "    print(\"out_mean:\", out_mean.dtype, out_mean)\n",
    "    print(\"out_prod:\", out_prod.dtype, out_prod)\n",
    "    print(\"out_exp:\", out_exp.dtype, out_exp.shape)\n",
    "    print(\"out_log:\", out_log.dtype, out_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffY3XJFRkfbA",
    "outputId": "dbff18ea-4420-4e54-86c7-5449bebb1638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_relu: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "x32 = torch.randn(2, 20000).to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# with autocast\n",
    "with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "    out_relu = nn.ReLU()(x32)\n",
    "    print(\"out_relu:\", out_relu.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZsnt84hX9bc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

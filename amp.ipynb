{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocast / AMP in PyTorch: A Deep Practical Reference\n",
    "\n",
    "This notebook is a **learning tool** for understanding and *actually feeling* the impact of mixed-precision training.\n",
    "\n",
    "It is organized into **three sections**:\n",
    "\n",
    "| # | Section | What you get |\n",
    "|---|---|---|\n",
    "| **1** | **Theory** | Floating-point range vs precision, FP16/BF16/FP32/TF32 comparison tables, underflow/overflow, what AMP is really doing |\n",
    "| **2** | **What the literature says** | Paper-driven mental models (Micikevicius et al., Kalamkar et al., ZeRO, NVIDIA guidance), written explanations — *no experiments* |\n",
    "| **3** | **Practicalities** | Hands-on experiments + graphs: progressive mixed-precision implementation from scratch, operator policy probes, dtype flow through a transformer, loss/gradient/scale curves under different precision regimes |\n",
    "\n",
    "---\n",
    "\n",
    "**After completing this notebook you should be able to answer (and debug) questions like:**\n",
    "\n",
    "- Why does FP16 often need **loss scaling**, but BF16 often does not?\n",
    "- What does `autocast` *actually* do per operation (matmul vs softmax vs layernorm)?\n",
    "- Why do people talk about **FP32 master weights** and **optimizer state precision**?\n",
    "- Why does adding `1e-4` to `1.0` in FP16 produce exactly `1.0`? (with the exact bit-level explanation)\n",
    "- How can you *see* autocast happening inside a transformer forward pass?\n",
    "- What fails if you try to \"just train in half precision everywhere\"?\n",
    "- Why does naive BF16 often work where naive FP16 fails?\n",
    "- What is the \"sum vs mean\" mystery under autocast?\n",
    "- How does the gradient distribution relate to FP16's representable range?\n",
    "- How many bytes per parameter does mixed-precision Adam training actually use?\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "- Read the markdown, then run the code cells.\n",
    "- Most experiments are designed to run in a few minutes on a single GPU.\n",
    "- CPU-only runs are supported for the *conceptual* demos, but some mixed-precision behaviors (and speedups) are fundamentally GPU-driven.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "**Section 1 — Theory**\n",
    "- Quick reference cheat sheet (the table you should memorize)\n",
    "- Floating-point: range vs precision, IEEE 754 anatomy\n",
    "- Number line visualization: where representable floats live\n",
    "- FP16 vs BF16 vs FP32 vs TF32 (tables you can trust)\n",
    "- The bit-level addition trap (why `1 + 1e-4 = 1` in FP16) — with step-by-step binary alignment\n",
    "- Underflow, overflow, accumulation error\n",
    "- What AMP is (autocast + grad scaling)\n",
    "- Master weights, optimizer state, and accumulation\n",
    "\n",
    "**Section 2 — What the literature says**\n",
    "- Micikevicius et al. — *Mixed Precision Training*\n",
    "- Kalamkar et al. — *A Study of BFLOAT16 for Deep Learning Training*\n",
    "- NVIDIA mixed precision guidance\n",
    "- BF16 design intent\n",
    "- PyTorch AMP operator policy\n",
    "- Rajbhandari et al. — ZeRO and optimizer state precision\n",
    "- LLM training stacks (FSDP/ZeRO) and where AMP fits\n",
    "- FP8 and 8-bit optimizers\n",
    "\n",
    "**Section 3 — Practicalities**\n",
    "- Progressive mixed-precision implementation from scratch (FP32 → naive FP16 → **naive BF16** → master weights → loss scaling → PyTorch AMP)\n",
    "- Build an operator policy table *from your local PyTorch*\n",
    "- The \"sum vs mean\" mystery\n",
    "- Visualize dtype flow through a transformer (4 configurations)\n",
    "- Gradient underflow + the effect of loss scaling\n",
    "- **Micikevicius-style gradient histogram analysis**\n",
    "- Weight update stagnation\n",
    "- Train a tiny causal LM under different precision regimes (FP32, FP16 naive, BF16 naive, AMP FP16, AMP BF16)\n",
    "- Plot and interpret loss/time/scale/gradient curves + summary bar charts\n",
    "\n",
    "---\n",
    "\n",
    "## Quick glossary\n",
    "\n",
    "| Term | Meaning |\n",
    "|---|---|\n",
    "| **AMP** | Automatic Mixed Precision (in PyTorch: `torch.amp`) |\n",
    "| **autocast** | Context manager that applies a per-operation dtype policy |\n",
    "| **GradScaler / loss scaling** | Rescales loss to avoid FP16 gradient underflow |\n",
    "| **master weights** | Keep weights in FP32 for updates, cast for compute |\n",
    "| **underflow** | Magnitude too small → becomes 0 (or subnormal/denormal) |\n",
    "| **overflow** | Magnitude too large → becomes `inf` |\n",
    "| **TF32** | TensorFloat-32 — an NVIDIA format with FP32 range but 10-bit mantissa, used transparently in Ampere+ matmuls |\n",
    "| **ULP** | Unit in the Last Place — the spacing between adjacent representable floats |\n",
    "| **epsilon** | Smallest number such that `1.0 + eps > 1.0` in a given format |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need:\n",
    "- Python 3.10+\n",
    "- PyTorch 2.x\n",
    "- `matplotlib`, `numpy`, `pandas`\n",
    "\n",
    "### Install (CPU-only quick start)\n",
    "```bash\n",
    "pip install torch numpy pandas matplotlib\n",
    "```\n",
    "\n",
    "### Install (CUDA)\n",
    "Install the correct PyTorch + CUDA build from the [official PyTorch instructions](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is written to *degrade gracefully*:\n",
    "- If BF16 is not supported on your GPU, BF16 experiments will be skipped.\n",
    "- If FP16 training without scaling explodes (often does), we record that as a result rather than pretending it \"worked\".\n",
    "\n",
    "> **Note:** on Apple Silicon, this notebook defaults to **CPU** for maximum compatibility. If you want to try the Apple GPU backend, set `USE_MPS_IF_AVAILABLE = True` in the first code cell. AMP/autocast behavior is best-defined on CUDA; CPU/MPS support exists but has different operator coverage and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core imports + environment report\n",
    "import os, math, time, random, struct, platform\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: numpy. Install with: pip install numpy\") from e\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: pandas. Install with: pip install pandas\") from e\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.ticker as ticker\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: matplotlib. Install with: pip install matplotlib\") from e\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 100,\n",
    "})\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"PyTorch is required. CPU-only: pip install torch\"\n",
    "    ) from e\n",
    "\n",
    "# Prefer torch.amp (newer API)\n",
    "if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\"):\n",
    "    autocast = torch.amp.autocast\n",
    "    GradScaler = torch.amp.GradScaler\n",
    "else:\n",
    "    autocast = torch.cuda.amp.autocast\n",
    "    GradScaler = torch.cuda.amp.GradScaler\n",
    "\n",
    "def amp_autocast(dev: torch.device, dtype: torch.dtype | None, enabled: bool = True, cache_enabled: bool = True):\n",
    "    # Best-effort autocast context manager that degrades gracefully.\n",
    "    if (not enabled) or (dtype is None):\n",
    "        return nullcontext()\n",
    "    try:\n",
    "        return autocast(device_type=dev.type, dtype=dtype, enabled=True, cache_enabled=cache_enabled)\n",
    "    except TypeError:\n",
    "        # Older signatures may not support cache_enabled.\n",
    "        try:\n",
    "            return autocast(device_type=dev.type, dtype=dtype, enabled=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "            return nullcontext()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "        return nullcontext()\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# ---- User knobs -------------------------------------------------------------\n",
    "# Leave as-is for \"works everywhere\" defaults; tweak if you have specific HW.\n",
    "PREFERRED_DEVICE = None   # one of: \"cuda\", \"mps\", \"cpu\" (or None for auto)\n",
    "USE_MPS_IF_AVAILABLE = False  # Apple Silicon: set True to try MPS when no CUDA\n",
    "\n",
    "def choose_device():\n",
    "    if PREFERRED_DEVICE is not None:\n",
    "        pref = str(PREFERRED_DEVICE).lower()\n",
    "        if pref == \"cuda\" and torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        if pref == \"mps\" and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        if pref == \"cpu\":\n",
    "            return torch.device(\"cpu\")\n",
    "        print(f\"[warn] Requested device '{PREFERRED_DEVICE}' not available; falling back to auto.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if USE_MPS_IF_AVAILABLE and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = choose_device()\n",
    "\n",
    "def supports_dtype_on_device(dtype: torch.dtype, dev: torch.device) -> bool:\n",
    "    try:\n",
    "        torch.tensor([0.0], device=dev, dtype=dtype)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"Python  {platform.python_version()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA:   {torch.version.cuda}\")\n",
    "    print(f\"GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"BF16:   {'supported' if torch.cuda.is_bf16_supported() else 'NOT supported'}\")\n",
    "elif device.type == \"mps\":\n",
    "    print(\"MPS:    available (Apple Silicon)\")\n",
    "    print(f\"FP16:   {'supported' if supports_dtype_on_device(torch.float16, device) else 'NOT supported'}\")\n",
    "    print(f\"BF16:   {'supported' if supports_dtype_on_device(torch.bfloat16, device) else 'NOT supported'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Floating-Point Formats for Deep Learning\n",
    "\n",
    "This is the table you should memorize. Every design decision in AMP traces back to these numbers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate the cheat sheet from your local PyTorch (so the numbers are trustworthy)\n",
    "\n",
    "def _ulp_at_one(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "formats = [\n",
    "    # FP8 is increasingly common for LLM training/inference, but support depends on HW + kernel stack.\n",
    "    (\"FP8 (E4M3)\", getattr(torch, \"float8_e4m3fn\", None), 4, 3),\n",
    "    (\"FP8 (E5M2)\", getattr(torch, \"float8_e5m2\", None), 5, 2),\n",
    "    (\"FP16 (IEEE half)\", torch.float16, 5, 10),\n",
    "    (\"BF16 (brain float)\", torch.bfloat16, 8, 7),\n",
    "    (\"FP32 (single)\", torch.float32, 8, 23),\n",
    "    (\"FP64 (double)\", torch.float64, 11, 52),\n",
    "]\n",
    "\n",
    "for name, dt, exp_b, mant_b in formats:\n",
    "    if dt is None:\n",
    "        continue\n",
    "    try:\n",
    "        fi = torch.finfo(dt)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    ulp1 = _ulp_at_one(dt)\n",
    "    sub = _smallest_subnormal(dt)\n",
    "\n",
    "    rows.append({\n",
    "        \"Format\": name,\n",
    "        \"Total bits\": fi.bits,\n",
    "        \"Exponent bits\": exp_b,\n",
    "        \"Mantissa bits\": mant_b,\n",
    "        \"Precision bits (incl hidden 1)\": mant_b + 1,\n",
    "        \"Approx decimal digits\": round((mant_b + 1) * math.log10(2), 1),\n",
    "        \"Exponent range\": f\"[{-2**(exp_b-1)+2}, {2**(exp_b-1)-1}]\",\n",
    "        \"epsilon (ULP at 1.0)\": f\"{fi.eps:.2e}\",\n",
    "        \"ULP at 1.0\": f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\",\n",
    "        \"Min normal\": f\"{fi.tiny:.2e}\",\n",
    "        \"Min subnormal\": f\"{sub:.2e}\" if sub is not None else \"n/a\",\n",
    "        \"Max finite\": f\"{fi.max:.2e}\",\n",
    "    })\n",
    "\n",
    "# Add TF32 manually (not a storable dtype in PyTorch, but important to know)\n",
    "tf32_row = {\n",
    "    \"Format\": \"TF32 (tensor float)\",\n",
    "    \"Total bits\": \"19*\",\n",
    "    \"Exponent bits\": 8,\n",
    "    \"Mantissa bits\": 10,\n",
    "    \"Precision bits (incl hidden 1)\": 11,\n",
    "    \"Approx decimal digits\": 3.3,\n",
    "    \"Exponent range\": \"[-126, 127]\",\n",
    "    \"epsilon (ULP at 1.0)\": \"9.77e-04\",\n",
    "    \"ULP at 1.0\": \"9.77e-04\",\n",
    "    \"Min normal\": \"1.18e-38\",\n",
    "    \"Min subnormal\": \"n/a (internal)\",\n",
    "    \"Max finite\": \"3.40e+38\",\n",
    "}\n",
    "\n",
    "# Insert TF32 right after FP32 if present; otherwise append.\n",
    "insert_at = None\n",
    "for i, r in enumerate(rows):\n",
    "    if r[\"Format\"].startswith(\"FP32\"):\n",
    "        insert_at = i + 1\n",
    "        break\n",
    "if insert_at is None:\n",
    "    rows.append(tf32_row)\n",
    "else:\n",
    "    rows.insert(insert_at, tf32_row)\n",
    "\n",
    "df_cheat = pd.DataFrame(rows).set_index(\"Format\")\n",
    "display(df_cheat)\n",
    "\n",
    "print()\n",
    "print(\"* TF32 is not a storage format. It is used internally by Tensor Cores on\")\n",
    "print(\"  Ampere+ GPUs for FP32 matmuls: FP32 range, but only 10 mantissa bits.\")\n",
    "print(\"  Your 'FP32 baseline' on Ampere+ may secretly be TF32 precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read this table\n",
    "\n",
    "Think of a floating-point format as a **measuring tape**:\n",
    "- **Exponent bits** determine the **length** of the tape (range: how big/small magnitudes you can reach).\n",
    "- **Mantissa bits** determine how **fine the tick marks** are (precision: how many significant digits you keep).\n",
    "\n",
    "| Format | Range | Precision | Training implication |\n",
    "|---|---|---|---|\n",
    "| **FP8** | Very narrow | Very low | Needs careful scaling (per-tensor/per-channel), mostly used with specialized kernels/hardware |\n",
    "| **FP16** | Narrow (5-bit exp) | Moderate (10-bit mantissa) | Underflow risk for gradients, overflow risk for activations → needs **loss scaling** |\n",
    "| **BF16** | Wide (8-bit exp, same as FP32) | Low (7-bit mantissa) | Rarely underflows → usually **no loss scaling** needed, but coarse rounding in reductions |\n",
    "| **FP32** | Wide | High | Stable baseline; slower and more memory |\n",
    "| **TF32** | Wide | Moderate (10-bit mantissa) | Compute-only on Ampere+ GPUs: FP32 matmuls may silently use TF32 for speed |\n",
    "| **FP64** | Very wide | Very high | Mostly for numeric reference/debugging (too slow for large training) |\n",
    "\n",
    "**Two immediate consequences:**\n",
    "1. FP16 has more mantissa bits than BF16 → **better precision per value**.\n",
    "2. BF16 has the same exponent width as FP32 → **dramatically better range** than FP16.\n",
    "\n",
    "So: FP16 fails first due to **range** (underflow/overflow). BF16 fails first due to **precision** (rounding/accumulation error). FP8 fails due to both unless extra care is taken.\n",
    "\n",
    "Autocast exists to route computations so that you get the performance of 16-bit compute without the worst numeric failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 — Theory\n",
    "\n",
    "The core trick of autocast is simple to state:\n",
    "\n",
    "> **Run the *right* operations in lower precision for speed/memory, while keeping *numerically sensitive* operations in FP32.**\n",
    "\n",
    "But to understand *why* this works (and when it doesn't), we need to understand what floating-point formats can and cannot represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Where floating-point lives during training (and why autocast exists)\n",
    "\n",
    "A single training step can be decomposed into:\n",
    "\n",
    "1. **Forward**: parameters + activations → logits\n",
    "2. **Loss**: logits + targets → scalar loss\n",
    "3. **Backward**: loss → gradients for parameters\n",
    "4. **Optimizer update**: parameters + gradients (+ optimizer state) → new parameters\n",
    "\n",
    "Different tensors have different numeric requirements:\n",
    "\n",
    "| Tensor | Typical AMP dtype | Why |\n",
    "|---|---|---|\n",
    "| Activations / matmul results | FP16/BF16 (where safe) | Saves memory + uses Tensor Cores |\n",
    "| Softmax / LayerNorm stats / reductions | FP32 | Protects against overflow + rounding accumulation |\n",
    "| Gradients | Often FP32 *storage* (even if compute is mixed) | Stable updates + compatibility with optimizers |\n",
    "| Parameters (\"master weights\") | FP32 | Prevents update stagnation |\n",
    "| Optimizer state (Adam moments) | FP32 | Long-horizon accumulation is precision-sensitive |\n",
    "\n",
    "**Autocast's job** is mostly about **(1) and (2)**: choose per-op dtypes during the forward pass.\n",
    "\n",
    "**GradScaler's job** is mostly about **(3)** when FP16 is involved: keep gradients from underflowing to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Floating-point is \"range + precision\", not just \"more bits = better\"\n",
    "\n",
    "A binary floating-point number is roughly:\n",
    "\n",
    "$$(-1)^{\\text{sign}} \\times (1.\\text{mantissa}) \\times 2^{\\text{exponent}}$$\n",
    "\n",
    "The bit budget is split across:\n",
    "\n",
    "- **Exponent bits** → *range* (how large/small magnitudes you can represent)\n",
    "- **Mantissa (fraction) bits** → *precision* (how many significant bits you keep)\n",
    "\n",
    "For deep learning training, the key question is not \"can I store 3.14159?\" but:\n",
    "\n",
    "- Can I represent **tiny gradients** without them becoming 0 (underflow)?\n",
    "- Can I represent **large activations** without them becoming `inf` (overflow)?\n",
    "- Can I sum many numbers without destroying meaning via rounding?\n",
    "\n",
    "These failure modes show up differently in FP16 and BF16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 IEEE 754 anatomy (sign, exponent bias, hidden bit)\n",
    "\n",
    "A *normalized* binary floating-point value is encoded as:\n",
    "- **sign bit** $s$ (0 = positive, 1 = negative)\n",
    "- **exponent field** $E$ (stored with a **bias** so we can represent negative exponents)\n",
    "- **mantissa / fraction field** $m$\n",
    "\n",
    "For normalized numbers:\n",
    "\n",
    "$$\\text{value} = (-1)^s \\times (1 + m) \\times 2^{(E - \\text{bias})}$$\n",
    "\n",
    "Key details:\n",
    "- The leading `1.` is **implicit** (the \"hidden bit\"), giving you 1 extra bit of effective precision for free.\n",
    "- Exponent all-zeros and all-ones are **reserved**: `E=0` → subnormals / zero; `E=all ones` → `inf` / `nan`.\n",
    "- The **bias** is $2^{(\\text{exp\\_bits} - 1)} - 1$. For FP32: $127$. For FP16: $15$. For BF16: $127$.\n",
    "\n",
    "Let's decode $\\pi$ in all three formats to see this concretely."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bit-level decoding of pi across FP32, FP16, BF16\n",
    "\n",
    "def bits_f32(x: float) -> str:\n",
    "    (u32,) = struct.unpack(\">I\", struct.pack(\">f\", float(x)))\n",
    "    return f\"{u32:032b}\"\n",
    "\n",
    "def bits_f16(x: float) -> str:\n",
    "    u16 = np.frombuffer(np.float16(x).tobytes(), dtype=np.uint16)[0]\n",
    "    return f\"{int(u16):016b}\"\n",
    "\n",
    "def bits_bf16(x: float) -> str:\n",
    "    t = torch.tensor(float(x), dtype=torch.bfloat16)\n",
    "    i16 = int(t.view(torch.int16).item()) & 0xFFFF\n",
    "    return f\"{i16:016b}\"\n",
    "\n",
    "def decode_float(bits: str, exp_bits: int, mant_bits: int, bias: int):\n",
    "    s = int(bits[0], 2)\n",
    "    E = int(bits[1:1+exp_bits], 2)\n",
    "    M_bits = bits[1+exp_bits:]\n",
    "    assert len(M_bits) == mant_bits\n",
    "\n",
    "    if E == 0:\n",
    "        exp = 1 - bias\n",
    "        mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "        val = ((-1)**s) * mant * (2**exp)\n",
    "        return \"subnormal/zero\", s, E, exp, mant, val\n",
    "\n",
    "    if E == (2**exp_bits - 1):\n",
    "        return \"inf/nan\", s, E, None, None, None\n",
    "\n",
    "    exp = E - bias\n",
    "    mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "    val = ((-1)**s) * (1.0 + mant) * (2**exp)\n",
    "    return \"normal\", s, E, exp, mant, val\n",
    "\n",
    "x = math.pi\n",
    "rows = []\n",
    "\n",
    "for name, get_bits, eb, mb, bias in [\n",
    "    (\"float32\", bits_f32, 8, 23, 127),\n",
    "    (\"float16\", bits_f16, 5, 10, 15),\n",
    "    (\"bfloat16\", bits_bf16, 8, 7, 127),\n",
    "]:\n",
    "    b = get_bits(x)\n",
    "    kind, s, E, exp, mant, val = decode_float(b, eb, mb, bias)\n",
    "    rows.append({\n",
    "        \"dtype\": name,\n",
    "        \"bits\": f\"{b[:1]}|{b[1:1+eb]}|{b[1+eb:]}\",\n",
    "        \"sign\": s, \"E(stored)\": E, \"exponent\": exp,\n",
    "        \"1+mantissa\": round(1 + mant, 8) if mant is not None else None,\n",
    "        \"decoded\": round(val, 10) if val is not None else None,\n",
    "        \"error vs pi\": f\"{abs(val - math.pi):.2e}\" if val is not None else None,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(f\"\\nTrue pi = {math.pi}\")\n",
    "print(\"Notice: BF16 and FP16 both decode to 3.140625, but via different bit patterns.\")\n",
    "print(\"FP16 has more mantissa bits (10) giving finer precision; BF16 has fewer (7) but same exponent range as FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Normal vs subnormal numbers (and \"flush-to-zero\")\n",
    "\n",
    "**Subnormals** (also called denormals) extend the representable range closer to 0 by giving up the implicit leading `1.`:\n",
    "\n",
    "$$\\text{subnormal value} = (-1)^s \\times (0.\\text{mantissa}) \\times 2^{(1 - \\text{bias})}$$\n",
    "\n",
    "They matter because **gradients can be very small**. But subnormals can be slow on some hardware, so many compute paths enable **FTZ/DAZ** (\"flush-to-zero\" / \"denormals-are-zero\"), which means extremely small values become exactly 0.\n",
    "\n",
    "**Practical lesson:** It is not enough to know the *spec* of a dtype. You also need to know what your hardware/kernel path does with subnormals.\n",
    "\n",
    "Let's probe whether the smallest subnormal survives on your device."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Subnormal survival probe\n",
    "\n",
    "def subnormal_survives(dtype, dev):\n",
    "    z = torch.tensor(0.0, dtype=dtype, device=dev)\n",
    "    o = torch.tensor(1.0, dtype=dtype, device=dev)\n",
    "    sub = torch.nextafter(z, o)\n",
    "    return {\n",
    "        \"dtype\": str(dtype), \"device\": dev.type,\n",
    "        \"nextafter(0,1)\": f\"{float(sub):.6e}\",\n",
    "        \"is_zero\": bool((sub == 0).item()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    try:\n",
    "        rows.append(subnormal_survives(dt, device))\n",
    "    except Exception as e:\n",
    "        rows.append({\"dtype\": str(dt), \"device\": device.type, \"error\": type(e).__name__})\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 ULP: spacing grows with magnitude\n",
    "\n",
    "A float format has *roughly constant relative precision* but *variable absolute precision*.\n",
    "\n",
    "- Near 1.0, FP16 spacing is ~$10^{-3}$.\n",
    "- Near 1024, FP16 spacing is ~$1$.\n",
    "\n",
    "This is the concrete reason \"tiny updates disappear\" when weights are stored in low precision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ULP vs magnitude for each dtype\n",
    "\n",
    "def ulp(x: torch.Tensor, dtype: torch.dtype):\n",
    "    x = x.to(dtype)\n",
    "    return (torch.nextafter(x, x * 2) - x).abs().to(torch.float32)\n",
    "\n",
    "ks = torch.arange(-10, 21, device=device)\n",
    "x = (2.0 ** ks).to(torch.float32)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, color in [(torch.float16, \"C0\"), (torch.bfloat16, \"C1\"), (torch.float32, \"C2\")]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    u = ulp(x, dt).cpu().numpy()\n",
    "    plt.plot(ks.cpu().numpy(), np.log2(u + 1e-45), marker=\"o\", markersize=4, label=str(dt), color=color)\n",
    "\n",
    "plt.title(\"ULP (spacing between adjacent floats) vs magnitude\")\n",
    "plt.xlabel(\"log2(|x|)\")\n",
    "plt.ylabel(\"log2(ULP(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Number line: where representable floats actually live\n",
    "\n",
    "The spacing between representable numbers is *not uniform* — it depends on the magnitude. Near zero, floats are dense; as magnitude grows, they spread apart. And crucially, **different formats have different densities at every scale**.\n",
    "\n",
    "This visualization plots the actual representable numbers in each format within a small interval. Think of it as zooming into the \"measuring tape\" to see the tick marks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number line: representable floats in [1.0, 2.0) for each dtype\n",
    "# This interval is illuminating because ULP is constant within a power-of-2 interval\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 6), sharex=True)\n",
    "\n",
    "for ax, (dt, label, color) in zip(axes, [\n",
    "    (torch.float32, \"FP32 (23-bit mantissa: 8,388,608 values in [1,2))\", \"C2\"),\n",
    "    (torch.bfloat16, \"BF16 (7-bit mantissa: 128 values in [1,2))\", \"C1\"),\n",
    "    (torch.float16, \"FP16 (10-bit mantissa: 1,024 values in [1,2))\", \"C0\"),\n",
    "    (None, \"Comparison overlay\", \"k\"),\n",
    "]):\n",
    "    if dt is not None:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        two = torch.tensor(2.0, dtype=dt)\n",
    "        vals = [float(one)]\n",
    "        cur = one\n",
    "        while True:\n",
    "            cur = torch.nextafter(cur, two)\n",
    "            if float(cur) >= 2.0:\n",
    "                break\n",
    "            vals.append(float(cur))\n",
    "            if len(vals) > 2000:\n",
    "                break\n",
    "        vals = np.array(vals)\n",
    "        ax.eventplot([vals], lineoffsets=0, linelengths=0.6, colors=color, linewidths=0.5)\n",
    "        ax.set_ylabel(label, fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.text(1.0, 0.35, f\"{len(vals)} representable values\", fontsize=8, color=color)\n",
    "    else:\n",
    "        # Overlay: show a narrow window [1.0, 1.02] with all three\n",
    "        for dt2, c2, yoff in [(torch.float16, \"C0\", 0.3), (torch.bfloat16, \"C1\", 0.0), (torch.float32, \"C2\", -0.3)]:\n",
    "            one2 = torch.tensor(1.0, dtype=dt2)\n",
    "            limit = torch.tensor(1.02, dtype=dt2)\n",
    "            vs = [float(one2)]\n",
    "            cur2 = one2\n",
    "            for _ in range(200):\n",
    "                cur2 = torch.nextafter(cur2, limit)\n",
    "                if float(cur2) >= 1.02:\n",
    "                    break\n",
    "                vs.append(float(cur2))\n",
    "            vs = np.array(vs)\n",
    "            ax.eventplot([vs], lineoffsets=yoff, linelengths=0.25, colors=c2, linewidths=1.0)\n",
    "        ax.set_ylabel(\"Zoomed [1.0, 1.02]\", fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim(1.0, 1.02)\n",
    "        ax.legend([\"FP16\", \"BF16\", \"FP32\"], fontsize=7, loc=\"upper right\")\n",
    "\n",
    "axes[0].set_xlim(1.0, 2.0)\n",
    "for ax in axes[:3]:\n",
    "    ax.set_xlim(1.0, 2.0)\n",
    "axes[-1].set_xlim(1.0, 1.02)\n",
    "axes[-1].set_xlabel(\"value\")\n",
    "fig.suptitle(\"Representable floats in [1.0, 2.0): density depends on mantissa bits\", fontsize=11, y=1.01)\n",
    "plt.tight_layout();\n",
    "print(\"Key insight: BF16 has ~8x fewer representable values than FP16 in [1,2),\")\n",
    "print(\"but FP16 has ~8,192x fewer than FP32. This is the precision-range tradeoff made visible.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 The bit-level addition trap (why `1 + 1e-4 = 1` in FP16)\n",
    "\n",
    "This is the single most important numeric fact for understanding **weight update stagnation**.\n",
    "\n",
    "When adding two floating-point numbers, the hardware must **align exponents** by shifting the smaller number's mantissa to the right. If the shift pushes all significant bits past the mantissa width, the smaller number is effectively lost.\n",
    "\n",
    "**Concrete example (from the FP16 bit-level):**\n",
    "\n",
    "- `1.0` in FP16: exponent = $2^0$, mantissa = all zeros.\n",
    "- `1e-4` in FP16: exponent = $2^{-14}$, mantissa encodes ~1.639.\n",
    "- To add them, we must shift `1e-4`'s mantissa by **14 positions** to align with `1.0`'s exponent.\n",
    "- FP16 has only **10 mantissa bits**. After shifting 14 positions right, *all* significant bits fall off the edge.\n",
    "- Result: `1.0 + 1e-4 = 1.0` exactly.\n",
    "\n",
    "This is exactly what happens during training: if `learning_rate * gradient` is smaller than the ULP at the weight's magnitude, the weight **never changes**.\n",
    "\n",
    "FP16's epsilon is ~$9.77 \\times 10^{-4}$. Any update smaller than this relative to the weight magnitude is silently dropped."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate the addition trap across dtypes\n",
    "\n",
    "print(\"Does 1.0 + delta produce a value > 1.0?\\n\")\n",
    "\n",
    "deltas = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "rows = []\n",
    "for delta in deltas:\n",
    "    row = {\"delta\": f\"{delta:.0e}\"}\n",
    "    for name, dt in [(\"FP16\", torch.float16), (\"BF16\", torch.bfloat16), (\"FP32\", torch.float32)]:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        d = torch.tensor(delta, dtype=dt)\n",
    "        result = one + d\n",
    "        changed = float(result) != float(one)\n",
    "        row[name] = \"YES\" if changed else \"no (lost!)\"\n",
    "    rows.append(row)\n",
    "\n",
    "df_add = pd.DataFrame(rows)\n",
    "display(df_add)\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"- FP16 loses updates smaller than ~1e-3 relative to weight magnitude.\")\n",
    "print(\"- BF16 loses updates smaller than ~8e-3 (even coarser!).\")\n",
    "print(\"- FP32 loses updates smaller than ~1e-7.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights: typical lr*grad products\")\n",
    "print(\"are often 1e-5 to 1e-7, which FP16 and BF16 both silently discard.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bit-level mechanics: *why* `1.0 + 1e-4 = 1.0` in FP16\n",
    "\n",
    "The table above shows *that* small updates get lost. Let's see *why* at the bit level.\n",
    "\n",
    "When hardware adds two floats, it must **align their exponents** by right-shifting the smaller number's mantissa. If the shift exceeds the mantissa width, the smaller number's bits fall off completely.\n",
    "\n",
    "This is the exact same mechanism that causes **weight update stagnation** during training: `weight += lr * gradient` produces the same weight if `lr * gradient` is too small relative to the weight's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step-by-step bit alignment: why 1.0 + 1e-4 = 1.0 in FP16\n",
    "\n",
    "print(\"=== Bit-level addition in FP16: 1.0 + 1e-4 ===\\n\")\n",
    "\n",
    "# 1.0 in FP16\n",
    "one_f16 = np.float16(1.0)\n",
    "one_bits = f\"{int(np.frombuffer(one_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "print(f\"1.0 in FP16:   {one_bits[0]}|{one_bits[1:6]}|{one_bits[6:]}\")\n",
    "print(f\"               s| exp  | mantissa\")\n",
    "print(f\"               = (-1)^0 × 1.0000000000 × 2^(15 - 15) = 1.0\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 1e-4 in FP16\n",
    "small_f16 = np.float16(1e-4)\n",
    "small_bits = f\"{int(np.frombuffer(small_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "E_small = int(small_bits[1:6], 2)\n",
    "print(f\"1e-4 in FP16:  {small_bits[0]}|{small_bits[1:6]}|{small_bits[6:]}\")\n",
    "print(f\"               = (-1)^0 × 1.{small_bits[6:]} × 2^({E_small} - 15) = 2^{{{E_small - 15}}}\")\n",
    "print(f\"               ≈ {float(small_f16):.6e}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Addition step: align exponents ---\")\n",
    "print()\n",
    "shift = 15 - E_small\n",
    "print(f\"To add these, we align the smaller exponent ({E_small - 15}) to the larger (0).\")\n",
    "print(f\"This means shifting 1e-4's mantissa RIGHT by {shift} positions.\")\n",
    "print()\n",
    "print(f\"  1.0:     1.{'0' * 10}         (exponent = 0)\")\n",
    "print(f\"+ 1e-4:    0.{'0' * (shift - 1)}1{'?' * max(0, 10 - shift)}   (shifted {shift} positions right)\")\n",
    "print()\n",
    "print(f\"FP16 mantissa is only 10 bits wide.\")\n",
    "print(f\"After shifting right by {shift}, ALL significant bits of 1e-4 are\")\n",
    "print(f\"beyond the 10-bit mantissa boundary → they are discarded.\")\n",
    "print()\n",
    "print(f\"Result: 1.0 + 1e-4 = 1.0  (the small value vanished completely)\")\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "result = np.float16(1.0) + np.float16(1e-4)\n",
    "eps_f16 = np.finfo(np.float16).eps\n",
    "print(f\"Verification:  np.float16(1.0) + np.float16(1e-4) = {result}\")\n",
    "print(f\"FP16 epsilon:  {eps_f16:.4e}\")\n",
    "print(f\"1e-4 < eps?    {1e-4 < eps_f16}  → update is below FP16's resolution at magnitude 1.0\")\n",
    "print()\n",
    "print(\"Training implication: if weight ≈ 1.0 and lr × grad ≈ 1e-4,\")\n",
    "print(\"the weight NEVER changes in FP16. This is weight update stagnation.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 FP16 vs BF16 vs FP32: the complete numeric comparison\n",
    "\n",
    "We generated the cheat sheet above. Here we dig deeper into what those numbers mean for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed format facts table\n",
    "\n",
    "def _ulp_at_one_v2(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal_v2(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dtype_row(name, dtype, exp_bits, mant_bits, exp_min, exp_max):\n",
    "    fi = torch.finfo(dtype)\n",
    "    ulp1 = _ulp_at_one_v2(dtype)\n",
    "    sub = _smallest_subnormal_v2(dtype)\n",
    "    return {\n",
    "        \"dtype\": name,\n",
    "        \"bits\": fi.bits,\n",
    "        \"exp_bits\": exp_bits,\n",
    "        \"mant_bits\": mant_bits,\n",
    "        \"precision_bits\": mant_bits + 1,\n",
    "        \"decimal_digits\": round((mant_bits + 1) * math.log10(2), 2),\n",
    "        \"exp_range\": f\"[{exp_min}, {exp_max}]\",\n",
    "        \"epsilon\": f\"{fi.eps:.2e}\",\n",
    "        \"ulp(1.0)\": f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\",\n",
    "        \"min_normal\": f\"{fi.tiny:.2e}\",\n",
    "        \"min_subnormal\": f\"{sub:.2e}\" if sub is not None else \"n/a\",\n",
    "        \"max_finite\": f\"{fi.max:.2e}\",\n",
    "    }\n",
    "\n",
    "dtype_info = pd.DataFrame([\n",
    "    *( [dtype_row(\"float8_e4m3fn\", torch.float8_e4m3fn, 4, 3, -6, 7)] if hasattr(torch, \"float8_e4m3fn\") else [] ),\n",
    "    *( [dtype_row(\"float8_e5m2\", torch.float8_e5m2, 5, 2, -14, 15)] if hasattr(torch, \"float8_e5m2\") else [] ),\n",
    "    dtype_row(\"float16\", torch.float16, 5, 10, -14, 15),\n",
    "    dtype_row(\"bfloat16\", torch.bfloat16, 8, 7, -126, 127),\n",
    "    dtype_row(\"float32\", torch.float32, 8, 23, -126, 127),\n",
    "    dtype_row(\"float64\", torch.float64, 11, 52, -1022, 1023),\n",
    "]).set_index(\"dtype\")\n",
    "\n",
    "display(dtype_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the table\n",
    "\n",
    "**Precision** (how fine the tick marks are):\n",
    "- FP64: ULP at 1.0 is ~2e-16. This is essentially \"reference precision\" for most deep learning numerics.\n",
    "- FP32: ULP at 1.0 is ~$1.2 \\times 10^{-7}$. Updates as small as $10^{-7}$ are captured.\n",
    "- FP16: ULP at 1.0 is ~$9.8 \\times 10^{-4}$. Updates smaller than $10^{-3}$ are lost.\n",
    "- BF16: ULP at 1.0 is ~$7.8 \\times 10^{-3}$. Updates smaller than $10^{-2}$ are lost. Even coarser than FP16!\n",
    "- FP8: ULP at 1.0 is huge. FP8 is *not* a drop-in training dtype without extra scaling strategies and specialized kernels.\n",
    "\n",
    "**Range** (how long the measuring tape is):\n",
    "- FP16: smallest normal is ~$6 \\times 10^{-5}$. Gradients below this become zero.\n",
    "- BF16: smallest normal is ~$1.2 \\times 10^{-38}$, same as FP32. Gradients essentially never underflow.\n",
    "- FP8: range depends on format (E4M3 vs E5M2), but is far smaller than FP16/BF16/FP32.\n",
    "- This is why **FP16 needs loss scaling** but **BF16 usually does not**.\n",
    "\n",
    "### 1.2.1 TF32 — the hidden precision mode\n",
    "\n",
    "On NVIDIA Ampere+ GPUs, FP32 matmuls can automatically use **TF32** internally:\n",
    "- Same 8-bit exponent as FP32 (full range)\n",
    "- But only 10 bits of mantissa (same as FP16 precision)\n",
    "- Transparent: your code says `float32`, but Tensor Cores use TF32 for speed\n",
    "\n",
    "This means your \"FP32 baseline\" on modern GPUs may actually be **TF32 precision** for matmuls. When comparing FP32 vs AMP, be aware of this hidden variable. You can control it with `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Why autocast targets matmul/linear first: FP32 accumulation\n",
    "\n",
    "Deep learning is dominated by large matrix multiplications (GEMMs): `Linear`, attention projections, and MLPs.\n",
    "\n",
    "On modern accelerators, these kernels typically:\n",
    "- **multiply** in FP16/BF16 (or TF32/FP8, depending on mode)\n",
    "- **accumulate** partial sums in **FP32**\n",
    "\n",
    "This is a sweet spot:\n",
    "- massive speedup (Tensor Cores / specialized units)\n",
    "- much better numeric behavior than \"pure FP16 accumulation\"\n",
    "\n",
    "Autocast heavily leans on this: it prefers to cast matmul-like ops down because they are both *fast* and comparatively *stable* (relative to softmax, layernorm, exp/log, large reductions)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Matmul accuracy across dtypes (vs FP64 reference)\n",
    "\n",
    "M = 256 if device.type != \"cuda\" else 512\n",
    "a = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "b = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "\n",
    "ref = (a.double() @ b.double()).float()\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    try:\n",
    "        c_dt = a.to(dt) @ b.to(dt)\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"matmul_output_dtype\": \"-\",\n",
    "            \"max_abs_err\": \"-\",\n",
    "            \"mean_abs_err\": \"-\",\n",
    "            \"max_rel_err\": \"-\",\n",
    "            \"mean_rel_err\": \"-\",\n",
    "            \"note\": f\"matmul failed ({type(e).__name__})\",\n",
    "        })\n",
    "        continue\n",
    "    c = c_dt.float()\n",
    "    abs_err = (c - ref).abs()\n",
    "    rel_err = abs_err / (ref.abs() + 1e-6)\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"matmul_output_dtype\": str(c_dt.dtype).replace(\"torch.\", \"\"),\n",
    "        \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "        \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "        \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "        \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "        \"note\": \"\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- Errors come from (1) input rounding to dt and (2) output rounding back to dt.\")\n",
    "print(\"- On CUDA, FP16/BF16 matmuls usually accumulate in FP32 internally, which helps stability.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 The three numeric disasters that show up during training\n",
    "\n",
    "### (A) Underflow — values become 0\n",
    "\n",
    "- Common in **gradients**, especially late in training or in deep nets with tiny signals.\n",
    "- Most harmful in **FP16** due to narrow exponent range (5 bits → min normal $\\approx 6 \\times 10^{-5}$).\n",
    "- BF16 has the same exponent range as FP32, so underflow is rare.\n",
    "\n",
    "### (B) Overflow — values become `inf`\n",
    "\n",
    "- Common in **activations** (exponentials, attention logits) or badly-initialized models.\n",
    "- FP16 max is only ~65,504. Easy to exceed.\n",
    "- BF16 max is ~$3.4 \\times 10^{38}$, same as FP32.\n",
    "\n",
    "### (C) Accumulation / cancellation error\n",
    "\n",
    "Even when values are in range, precision limits corrupt sums and products:\n",
    "- Adding many small numbers to a large accumulator can lose the small contributions (same mechanism as the `1 + 1e-4` trap).\n",
    "- For reductions (layernorm statistics, softmax normalization, large sums), frameworks often keep accumulation in FP32.\n",
    "\n",
    "**Autocast** is partly about preventing A/B (range disasters), and partly about routing sensitive reductions so C doesn't destroy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0 Accumulation error, made concrete\n",
    "\n",
    "**Accumulation error** is just \"rounding happens at every add/multiply, and it compounds\".\n",
    "\n",
    "The simplest microscope is:\n",
    "\n",
    "> start at a value (like 1.0), add a tiny increment many times, and see when the increment stops \"counting\".\n",
    "\n",
    "This is the same mechanism behind:\n",
    "- weight-update stagnation (updates below ULP get rounded away)\n",
    "- instability in reductions (sums/means/variances done in low precision)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Accumulation microscope: 1.0 + N * delta, computed sequentially in different dtypes\n",
    "\n",
    "N = 20000\n",
    "base = 1.0\n",
    "delta = 1e-3\n",
    "\n",
    "steps_i = torch.arange(N + 1, device=device, dtype=torch.int64)\n",
    "expected64 = base + delta * steps_i.double()\n",
    "\n",
    "rows = []\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "every = max(1, (N + 1) // 800)\n",
    "\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32, torch.float64]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    deltas = torch.full((N,), delta, device=device, dtype=dt)\n",
    "    start = torch.tensor([base], device=device, dtype=dt)\n",
    "    cs = torch.cat([start, deltas]).cumsum(0)\n",
    "    err = (cs.double() - expected64).cpu().numpy()\n",
    "\n",
    "    label = str(dt).replace(\"torch.\", \"\")\n",
    "    plt.plot(steps_i.cpu().numpy()[::every], err[::every], label=label, alpha=0.85)\n",
    "\n",
    "    final = float(cs[-1].cpu())\n",
    "    rows.append({\n",
    "        \"dtype\": label,\n",
    "        \"final\": f\"{final:.6f}\",\n",
    "        \"expected\": f\"{float(expected64[-1].cpu()):.6f}\",\n",
    "        \"abs_err\": f\"{abs(final - float(expected64[-1].cpu())):.3e}\",\n",
    "    })\n",
    "\n",
    "plt.axhline(0.0, color=\"k\", lw=1, alpha=0.3)\n",
    "plt.title(\"Accumulation error: sequentially adding delta many times\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cumsum(dtype) - reference (float64 arithmetic)\")\n",
    "plt.legend()\n",
    "plt.tight_layout();\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- If the error curve flattens, your increments stopped affecting the accumulator.\")\n",
    "print(\"- This is why AMP promotes certain reductions (like sum/prod and norm stats) to FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LayerNorm-like statistics are especially sensitive:\n",
    "# mean/var of (large offset + small noise) is a classic cancellation problem.\n",
    "\n",
    "M = 200_000\n",
    "x = (torch.randn(M, device=device, dtype=torch.float32) * 0.1) + 1000.0\n",
    "\n",
    "ref_mu = float(x.double().mean())\n",
    "ref_var = float(((x.double() - ref_mu) ** 2).mean())\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    xd = x.to(dt)\n",
    "    mu = xd.mean()\n",
    "    var = ((xd - mu) ** 2).mean()\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"mean\": f\"{float(mu):.6f}\",\n",
    "        \"var\": f\"{float(var):.6e}\",\n",
    "        \"mean_abs_err\": f\"{abs(float(mu) - ref_mu):.3e}\",\n",
    "        \"var_rel_err\": f\"{abs(float(var) - ref_var) / (ref_var + 1e-30):.3e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nThis is a simplified version of why LayerNorm/Softmax reductions are often forced to FP32 under autocast.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Where does exp() overflow by dtype?\n",
    "\n",
    "x = torch.linspace(-20, 20, 400, device=device)\n",
    "\n",
    "def safe_exp(x, dtype):\n",
    "    return torch.exp(x.to(dtype)).to(torch.float32).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, label in [(torch.float32, \"FP32\"), (torch.bfloat16, \"BF16\")]:\n",
    "    y = safe_exp(x, dt)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y, 1e-30, 1e30)), label=label)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    y16 = safe_exp(x, torch.float16)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y16, 1e-30, 1e30)), label=\"FP16\")\n",
    "\n",
    "plt.axhline(np.log10(65504), color=\"r\", linestyle=\"--\", alpha=0.5, label=\"FP16 max (65504)\")\n",
    "plt.title(\"log10(exp(x)) computed in different dtypes — FP16 overflows early\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(exp(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Loss functions are \"log-sum-exp machines\" (and dtype matters)\n",
    "\n",
    "Many deep learning losses contain exponentials and logs. A classic example:\n",
    "\n",
    "$$\\log(1 + e^x) \\quad \\text{(softplus)}$$\n",
    "\n",
    "- The naive formula overflows quickly in FP16 (for $x > 11$, $e^x > 65504$).\n",
    "- Stable implementations (e.g., `F.softplus`) avoid overflow by rewriting the expression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softplus across dtypes\n",
    "\n",
    "def naive_softplus(x):\n",
    "    return torch.log1p(torch.exp(x))\n",
    "\n",
    "x = torch.linspace(-80, 80, 2000, device=device)\n",
    "ref = F.softplus(x.double()).float()  # high-precision reference\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    y_naive = naive_softplus(x.to(dt)).float()\n",
    "    y_stable = F.softplus(x.to(dt)).float()\n",
    "    err_naive = (y_naive - ref).abs().cpu().numpy()\n",
    "    err_stable = (y_stable - ref).abs().cpu().numpy()\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_naive + 1e-12), label=f\"naive {dt}\")\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_stable + 1e-12), ls=\"--\", label=f\"stable {dt}\")\n",
    "\n",
    "plt.title(\"Softplus error: naive vs stable implementation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(|error|) vs FP64 reference\")\n",
    "plt.legend(ncols=2)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Softmax: the overflow trap and the stability rewrite\n",
    "\n",
    "Softmax is everywhere in transformers (attention). Naive softmax:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "This can overflow in low precision because $e^x$ explodes quickly. The stable rewrite subtracts the max:\n",
    "\n",
    "$$\\text{softmax}(x) = \\text{softmax}(x - \\max(x))$$\n",
    "\n",
    "PyTorch's `F.softmax` uses a stable implementation. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softmax\n",
    "\n",
    "def naive_softmax(x, dim=-1):\n",
    "    ex = torch.exp(x)\n",
    "    return ex / ex.sum(dim=dim, keepdim=True)\n",
    "\n",
    "logits = torch.tensor([0.0, 20.0, 40.0, 80.0], device=device)\n",
    "ref = F.softmax(logits.double(), dim=0).float()\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    x = logits.to(dt)\n",
    "    try:\n",
    "        naive = naive_softmax(x, dim=0).float()\n",
    "        naive_ok = bool(torch.isfinite(naive).all().item())\n",
    "    except Exception:\n",
    "        naive = torch.full_like(ref, float(\"nan\"))\n",
    "        naive_ok = False\n",
    "    stable = F.softmax(x, dim=0).float()\n",
    "    stable_ok = bool(torch.isfinite(stable).all().item())\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt),\n",
    "        \"naive_finite\": naive_ok,\n",
    "        \"stable_finite\": stable_ok,\n",
    "        \"max_abs_err(stable vs ref)\": f\"{float((stable - ref).abs().max()):.2e}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 What AMP actually is\n",
    "\n",
    "In PyTorch, AMP is two complementary tools:\n",
    "\n",
    "1. **`autocast`** (forward + loss)\n",
    "   - A context manager that applies a **per-operation dtype policy**.\n",
    "   - It *temporarily* casts inputs/weights for each operation. It does **not** permanently change model parameters.\n",
    "   - Matmuls/linear → lower precision. Softmax/layernorm/losses → FP32.\n",
    "\n",
    "2. **`GradScaler`** (backward + optimizer step)\n",
    "   - Primarily for **FP16 training** (BF16 usually doesn't need it).\n",
    "   - Multiplies loss by a scale factor $S$ before backward → gradients are $S\\times$ larger → fewer underflow to zero.\n",
    "   - Before optimizer step, divides gradients by $S$. If overflow is detected (`inf`/`nan`), skips the step and reduces $S$.\n",
    "\n",
    "**Clean mental model:**\n",
    "- `autocast` protects you from **bad forward dtypes**.\n",
    "- `GradScaler` protects you from **bad backward magnitudes** (FP16 gradient underflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The canonical AMP training loop (four conceptual changes)\n",
    "\n",
    "Start with FP32 training:\n",
    "```python\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "logits = model(x)\n",
    "loss = loss_fn(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "AMP adds four things:\n",
    "```python\n",
    "scaler = GradScaler()\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "with autocast(device_type=\"cuda\", dtype=torch.float16):   # 1. wrap forward + loss\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "scaler.scale(loss).backward()    # 2. scale loss before backward\n",
    "scaler.step(optimizer)           # 3. unscale + check for inf/nan + step\n",
    "scaler.update()                  # 4. adjust scale factor\n",
    "```\n",
    "\n",
    "That's the \"small code change\" people talk about. But the *reason* it works is the theory above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Master weights and optimizer state (why \"just casting the model\" is not the same)\n",
    "\n",
    "There are **three** numeric objects in training that matter:\n",
    "\n",
    "1. **Parameters** (weights) — used in forward/backward\n",
    "2. **Gradients** — produced by backward\n",
    "3. **Optimizer state** (e.g., Adam's first and second moment estimates $m_t, v_t$) — long-lived accumulators\n",
    "\n",
    "A classic mixed precision recipe:\n",
    "- Keep a **master copy of weights in FP32**.\n",
    "- Do forward/backward in FP16/BF16 where safe.\n",
    "- Maintain optimizer state (Adam moments) in FP32.\n",
    "\n",
    "**Why FP32 master weights?**\n",
    "\n",
    "Because 16-bit formats have coarse spacing. A small update $\\Delta w = \\text{lr} \\times \\text{grad}$ can be *below the ULP* at the magnitude of $w$, so the weight never changes. We showed this in the `1 + 1e-4` demo above.\n",
    "\n",
    "Over many steps, these tiny updates *accumulate* in FP32 and eventually become large enough to appear in the 16-bit copy. This is the key insight from the Micikevicius et al. (2017) paper.\n",
    "\n",
    "**Why FP32 optimizer state?**\n",
    "\n",
    "Adam's moments are exponential moving averages. They accumulate information over the entire training run. Even small rounding errors compound over thousands of steps. Keeping moments in FP32 prevents this drift.\n",
    "\n",
    "**Memory implication:**\n",
    "\n",
    "Mixed precision doesn't eliminate FP32 — it just limits FP32 to parameters + optimizer state (which is fixed-size), while saving on activations (which scale with batch size and sequence length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Autocast is an operator policy (not a global cast)\n",
    "\n",
    "Autocast does **not** \"turn the whole model into FP16\". Instead it applies a per-operation policy:\n",
    "\n",
    "| Policy | Operations | Rationale |\n",
    "|---|---|---|\n",
    "| **Lower precision** (FP16/BF16) | `linear`, `matmul`, `mm`, `bmm`, convolutions | Compute-bound → Tensor Core speedup |\n",
    "| **Force FP32** | `softmax`, `layer_norm`, `log_softmax`, `mse_loss`, `cross_entropy`, `sum`, `prod`, `exp`, `log`, ... | Numerically sensitive: overflow/underflow/accumulation risk |\n",
    "| **Promote to widest** | Binary ops when inputs differ | If one input is FP32, the op runs in FP32 |\n",
    "| **Pass-through** | `relu`, `dropout`, `max`, `min`, `mean`, ... | Element-wise, no numeric risk; output matches input dtype |\n",
    "\n",
    "The exact policy is PyTorch-version-dependent. In Section 3 we will probe it empirically on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 summary\n",
    "\n",
    "| Concept | Key takeaway |\n",
    "|---|---|\n",
    "| **FP16** | Better precision than BF16, but narrow range → needs loss scaling and careful op policies |\n",
    "| **BF16** | FP32-like range → often trains without scaling, but coarse precision → reductions need care |\n",
    "| **TF32** | Your \"FP32 baseline\" on Ampere+ GPUs may secretly be TF32 (10-bit mantissa) for matmuls |\n",
    "| **AMP** | `autocast` (forward op policy) + `GradScaler` (backward magnitude control) |\n",
    "| **Master weights** | Keep FP32 copy for updates; prevents stagnation from ULP rounding |\n",
    "| **Optimizer state** | Keep in FP32; long-horizon accumulation is precision-sensitive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 — What the literature says\n",
    "\n",
    "This section is intentionally *written*: the point is to build a paper-and-doc-driven mental model that you can carry into real training code.\n",
    "\n",
    "No experiments here — only explanations. Think of this section as: \"what each source contributes, and how it maps to PyTorch AMP.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 A reading map\n",
    "\n",
    "| Source | What it gives you | Maps to |\n",
    "|---|---|---|\n",
    "| Gupta et al. (2015) | Why limited precision can still train; stochastic rounding + scaling intuition | Section 1.3 (rounding/accumulation), Section 3 debugging |\n",
    "| Micikevicius et al. (2017) | The canonical mixed precision recipe (master weights + loss scaling) | Sections 1.4–1.6, scaling experiments |\n",
    "| Kalamkar et al. (2019) | BF16 bit-level analysis, proof that BF16 avoids FP16's underflow | Section 1.2, BF16 training runs |\n",
    "| NVIDIA mixed precision guidance | Engineering intuition + failure modes | Underflow/overflow + \"sensitive ops in FP32\" |\n",
    "| PyTorch `torch.amp` docs | The actual API + gotchas | `autocast` + `GradScaler` loops in Section 3 |\n",
    "| PyTorch autocast op reference | *The* per-op policy | Probed empirically in Section 3.2 |\n",
    "| Rajbhandari et al. (2020) — ZeRO | Memory breakdown of mixed-precision training at scale | Optimizer state precision, Section 2.6 |\n",
    "| Distributed training docs (FSDP/ZeRO/DeepSpeed) | Where dtypes live in large systems | Section 2.7 |\n",
    "| FP8 literature (e.g., NVIDIA) | Why FP8 needs scaling (E4M3 vs E5M2) + where it fits vs AMP | Section 1 tables, Section 3 practical guidance |\n",
    "| Dettmers et al. (8-bit optimizers) | Optimizer-state precision as the next bottleneck after AMP | Section 2.6 memory discussion |\n",
    "\n",
    "If you only read one thing: read the Micikevicius paper, then read the PyTorch autocast op reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Micikevicius et al. (2017): *Mixed Precision Training*\n",
    "\n",
    "> arXiv:1710.03740 — ICLR 2018\n",
    "\n",
    "This is the foundational paper for everything in this notebook. It introduced the three-part recipe that all modern AMP implementations are based on.\n",
    "\n",
    "**The problem:** FP16 arithmetic is fast (2–8$\\times$ throughput on Tensor Cores) and memory-efficient (half the bytes), but naively training in FP16 breaks. Models either diverge, produce NaN losses, or silently stagnate. The paper identifies three distinct failure modes:\n",
    "\n",
    "1. **Weight update stagnation.** When a weight $w$ is stored in FP16, the update $\\Delta w = \\eta \\cdot g$ can be smaller than the ULP (unit in the last place) at $|w|$. The update is rounded away and the weight never changes. The paper's solution: maintain an FP32 \"master copy\" of all parameters. Forward and backward compute use the FP16 cast, but the actual parameter update happens in FP32 where the precision is sufficient to register small changes. The updated FP32 value is then cast back to FP16 for the next forward pass.\n",
    "\n",
    "2. **Gradient underflow.** Even with FP32 master weights, gradients themselves are computed in FP16 during the backward pass. FP16's smallest normalized number is approximately $6 \\times 10^{-5}$. The paper's analysis of gradient histograms across several production models (image classifiers, speech models, generative models) shows that a significant fraction of gradient values fall below this threshold. Once they underflow to zero, the update signal is lost. The solution: **loss scaling**. Multiply the scalar loss by a large constant $S$ before calling `backward()`. Since backpropagation is a chain of multiplications, every gradient in the network is scaled up by $S$. After backward, divide gradients by $S$ before the optimizer step. If $S$ is chosen well, gradients that would have underflowed are now safely within FP16's representable range, and the division by $S$ restores correct magnitudes. The paper demonstrates that a fixed $S$ works for many models, but notes that $S$ too large can cause gradient *overflow*. This motivates **dynamic loss scaling**: start with a large $S$, and if `inf`/`nan` gradients are detected, skip the update and reduce $S$. If training is stable, periodically increase $S$ to maximize headroom.\n",
    "\n",
    "3. **Accumulation error in reductions.** Large dot products and reductions (e.g., summing thousands of values) accumulate rounding errors that compound in FP16. The paper recommends performing these accumulations in FP32, even when the operands are FP16. Modern Tensor Cores implement this: they accept FP16 inputs but accumulate in FP32.\n",
    "\n",
    "**What to remember:** AMP is not a single trick. It is the *combination* of (a) per-op precision policies, (b) loss scaling, and (c) FP32 master weights/optimizer state. Removing any one of these can cause training to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kalamkar et al. (2019): *A Study of BFLOAT16 for Deep Learning Training*\n",
    "\n",
    "> arXiv:1905.12322\n",
    "\n",
    "This paper provides the empirical and theoretical justification for BF16 as a training format.\n",
    "\n",
    "**The key insight** is architectural: BF16 uses **8 exponent bits** (same as FP32), giving it the same dynamic range — roughly $10^{-38}$ to $10^{38}$. This means BF16 can represent the same extreme magnitudes as FP32. The gradient underflow problem that plagues FP16 (5-bit exponent → range only $10^{-5}$ to $6.5 \\times 10^4$) essentially does not exist for BF16.\n",
    "\n",
    "**The tradeoff:** BF16 sacrifices mantissa bits (7 vs FP16's 10 vs FP32's 23). This means worse *precision* per individual value. But the paper demonstrates empirically that deep neural networks are remarkably robust to precision loss — they are much more sensitive to *range* loss. Training with BF16 across a variety of models (ResNets, Transformers, LSTMs) converges to the same final accuracy as FP32, often without any loss scaling at all.\n",
    "\n",
    "**Conversion simplicity:** Since BF16 and FP32 share the same exponent field, conversion is trivial: truncate (or round) the bottom 16 mantissa bits. FP32 → FP16 conversion is harder because the exponent field must also be narrowed, risking overflow or underflow in the conversion itself.\n",
    "\n",
    "**Practical implication for AMP:** BF16 autocast is often a \"drop-in\" replacement that doesn't require a `GradScaler`. This is explicitly confirmed by modern frameworks: DeepSpeed documentation states, \"Training with bfloat16 does not require loss scaling.\"\n",
    "\n",
    "**When BF16 can still struggle:** Precision-sensitive accumulations (layernorm statistics, attention score normalization, large batch reductions) can still degrade with BF16's 7-bit mantissa. This is why autocast routes these operations to FP32 even in BF16 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NVIDIA mixed precision guidance (the engineering perspective)\n",
    "\n",
    "NVIDIA's developer blog and documentation provide the practitioner's view of mixed precision. The key engineering decomposition:\n",
    "\n",
    "1. **Tensor Core compute** wants FP16/BF16 inputs. Modern GPUs (Volta+) have dedicated matrix-multiply-accumulate units that operate on 16-bit inputs with FP32 accumulation. These deliver 2–8$\\times$ the throughput of FP32 CUDA cores for matmuls, which dominate deep learning compute.\n",
    "\n",
    "2. **Dimension alignment matters.** Tensor Cores process tiles of fixed size (typically 8 or 16 elements). If your tensor dimensions aren't multiples of 8, you either waste hardware or fall back to slower code paths. This is a practical detail that affects whether you actually see the theoretical speedup.\n",
    "\n",
    "3. **Memory bandwidth matters as much as compute.** 16-bit formats halve the bytes transferred between GPU memory (HBM) and compute cores (SMs). For memory-bandwidth-bound models (which many transformer models are at inference time), this alone can yield significant speedup even without Tensor Core benefits.\n",
    "\n",
    "4. **Some ops are numerically sensitive.** NVIDIA's guidance identifies the same ops that PyTorch's autocast routes to FP32: exponentials, logs, softmax, normalization statistics, and large reductions. The rationale is the same: overflow/underflow risk and accumulation error.\n",
    "\n",
    "The practical outcome is the \"few lines of code\" AMP recipe. But the engineering reason it works is the numeric analysis in the theory section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BF16: \"FP32 range, fewer bits of precision\"\n",
    "\n",
    "BF16 was originally designed for Google's TPUs. The design goal was explicit: make it possible to train neural networks in 16-bit formats *without* the underflow problems of FP16.\n",
    "\n",
    "**The design decision:** Given 16 bits, how should you split them between exponent and mantissa?\n",
    "\n",
    "- FP16 (IEEE): 5 exponent + 10 mantissa → good precision, but range only $[6 \\times 10^{-5}, 6.5 \\times 10^4]$.\n",
    "- BF16 (Google/Intel): 8 exponent + 7 mantissa → FP32-like range $[10^{-38}, 3.4 \\times 10^{38}]$, coarser precision.\n",
    "\n",
    "For training, range wins. Gradients span many orders of magnitude, and losing any of them to underflow is worse than representing them imprecisely. Networks are fundamentally robust to noise (they're trained with stochastic gradient descent after all), but they cannot learn from zero gradients.\n",
    "\n",
    "**In practice, for many transformer trainings on modern GPUs:**\n",
    "- BF16 autocast is \"drop-in\" without loss scaling.\n",
    "- FP16 autocast typically needs a `GradScaler`.\n",
    "- Both still benefit from FP32 master weights and FP32 optimizer state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 PyTorch AMP docs: the operator policy is the decoder ring\n",
    "\n",
    "A common mistake is to treat autocast like a global switch (\"my model is now FP16\"). But autocast is really a **per-operation dispatch table**:\n",
    "\n",
    "- Some ops are eligible for lower precision (FP16/BF16). These are the compute-heavy ops where Tensor Cores give speedup.\n",
    "- Some ops are forced to FP32. These are the numerically sensitive ops where lower precision would cause training failures.\n",
    "- Some ops promote to the widest input type. These are binary operations where mixed-dtype inputs would be ambiguous.\n",
    "- Unlisted ops run in whatever dtype they receive (pass-through).\n",
    "\n",
    "**The key documentation page** is the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). It lists every CUDA op and its autocast behavior. This is the authoritative answer to \"why did this op run in FP32?\" — faster and more reliable than any blog post.\n",
    "\n",
    "**A common debugging pattern:** when AMP training fails, the first thing to check is which ops are running in which dtype. The dtype hooks we build in Section 3 make this visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Rajbhandari et al. (2020): ZeRO and optimizer state precision\n",
    "\n",
    "> arXiv:1910.02054 — *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models*\n",
    "\n",
    "While ZeRO is primarily about distributed training, its memory analysis (Section 2) is essential for understanding where mixed precision fits.\n",
    "\n",
    "**The memory breakdown for a model with $\\Psi$ parameters trained with Adam in mixed precision:**\n",
    "\n",
    "| Component | Dtype | Bytes per parameter |\n",
    "|---|---|---|\n",
    "| FP16 parameters (forward/backward) | FP16 | 2 |\n",
    "| FP16 gradients | FP16 | 2 |\n",
    "| FP32 master weights | FP32 | 4 |\n",
    "| FP32 Adam first moment ($m$) | FP32 | 4 |\n",
    "| FP32 Adam second moment ($v$) | FP32 | 4 |\n",
    "| **Total** | | **16 bytes/param** |\n",
    "\n",
    "For a 7B parameter model: $7 \\times 10^9 \\times 16 = 112$ GB just for parameters + optimizer state — before any activations.\n",
    "\n",
    "**Key insight:** Mixed precision doesn't eliminate FP32 from the system. It moves FP32 to where it's needed (optimizer state, master weights) and uses 16-bit where it's safe (activations, compute). The memory savings come primarily from activations (which scale with batch size and sequence length), not from the fixed-size parameter/optimizer memory.\n",
    "\n",
    "**Practical takeaway:** \"AMP\" in a distributed stack is not only about autocast during forward/backward. It is about **where each tensor lives and in what dtype** across the entire training loop: parameters, gradients, optimizer state, communication buffers, and activation checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 LLM training stacks (FSDP / ZeRO / DeepSpeed): where precision choices multiply\n",
    "\n",
    "At LLM scale, training systems shard parameters, gradients, and optimizer state across multiple GPUs. Mixed precision gets more complicated because dtype decisions affect:\n",
    "\n",
    "- **Parameter storage:** \"working\" params in BF16/FP16 for forward/backward; FP32 master copy for updates.\n",
    "- **Gradient communication:** All-reduce operations can accumulate rounding errors. Some stacks reduce in FP32 for stability, others use BF16 + gradient compression.\n",
    "- **Optimizer state:** Typically FP32 (Adam moments need high precision over long training). Some frameworks offer FP16 optimizer states as a memory optimization, but this trades stability for memory.\n",
    "- **Activation checkpointing:** Recomputed activations use the same autocast policy as the original forward pass, but the checkpointing mechanism itself must preserve dtypes correctly.\n",
    "\n",
    "**FSDP mixed precision** (PyTorch): lets you separately control `param_dtype` (for sharded parameters), `reduce_dtype` (for gradient all-reduce), and `buffer_dtype`. FP16 + scaler or BF16 without scaler.\n",
    "\n",
    "**DeepSpeed ZeRO:** explicit config knobs. `fp16.loss_scale = 0` enables dynamic loss scaling; `bf16.enabled = true` with no loss scaling.\n",
    "\n",
    "**The practical lesson:** When debugging AMP issues in distributed training, the dtype of every tensor transfer (parameter broadcast, gradient reduce, optimizer state scatter/gather) is a potential source of numeric problems. This is well beyond the scope of a single `autocast` context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 How to think about the autocast policy (conceptual categories)\n",
    "\n",
    "Autocast decisions fall into a few conceptual buckets. These aren't arbitrary — each traces back to a specific numeric risk:\n",
    "\n",
    "**1. Lower precision eligible (matmul-like ops)**\n",
    "\n",
    "These are the ops where Tensor Cores provide speedup and numeric risk is low. The key property: these operations multiply pairs of values and accumulate in FP32 (on Tensor Cores), so the inputs can be 16-bit without precision loss in the output.\n",
    "\n",
    "**2. Force FP32 (numerically sensitive ops)**\n",
    "\n",
    "Two sub-categories:\n",
    "- **Overflow/underflow risk:** `exp`, `log`, `softmax`, `log_softmax`. These can produce extreme magnitudes.\n",
    "- **Accumulation risk:** `sum`, `prod`, `layer_norm`, `batch_norm`, `cross_entropy`, `mse_loss`. These reduce many values and small rounding errors compound.\n",
    "\n",
    "**3. Promote to widest (binary ops with mixed inputs)**\n",
    "\n",
    "If you add a BF16 tensor to an FP32 tensor, the result is FP32. This prevents silent precision loss when autocast and non-autocast regions interact.\n",
    "\n",
    "**4. Pass-through (element-wise, no numeric risk)**\n",
    "\n",
    "`relu`, `sigmoid`, `tanh`, `dropout`, `max`, `min`, `mean`. These just transform individual values without extreme magnitudes or accumulation. Whatever goes in comes out.\n",
    "\n",
    "**Practitioner's two rules:**\n",
    "- If an op creates very large/small magnitudes ($e^x$, $\\log x$, softmax), it needs FP32.\n",
    "- If an op reduces many values ($\\sum$, variance, cross-entropy), it needs FP32 accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Dynamic loss scaling: what `GradScaler` is doing\n",
    "\n",
    "Loss scaling multiplies the scalar loss by a factor $S$ before calling `backward()`.\n",
    "\n",
    "Since backpropagation is linear (gradients are proportional to the loss), every gradient in the network is multiplied by $S$. This shifts the entire gradient distribution toward larger magnitudes, rescuing values that would have underflowed to zero in FP16.\n",
    "\n",
    "**The protocol:**\n",
    "\n",
    "1. Compute loss normally under autocast.\n",
    "2. Multiply loss by $S$ and call `backward()`.\n",
    "3. Before the optimizer step, **unscale** all gradients by dividing by $S$.\n",
    "4. Check for `inf`/`nan` in gradients:\n",
    "   - If found: **skip** the optimizer step, reduce $S$ (usually halve it).\n",
    "   - If not found: proceed with the step. Periodically increase $S$ (e.g., double it every $N$ successful steps) to maximize headroom.\n",
    "\n",
    "**Why it's safe:** scaling and unscaling cancel out exactly if no overflow occurs. The optimizer sees the same gradients it would have without scaling.\n",
    "\n",
    "**Critical detail:** gradient clipping must happen **after** unscaling. PyTorch's `scaler.unscale_(optimizer)` does the unscale explicitly; `scaler.step(optimizer)` then checks for inf/nan before calling `optimizer.step()`.\n",
    "\n",
    "**Why BF16 doesn't need this:** BF16 has the same exponent range as FP32. Gradients that would be $10^{-10}$ in FP32 are also representable in BF16 (though with less precision). No underflow → no need for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Gupta et al. (2015): *Deep Learning with Limited Numerical Precision*\n",
    "\n",
    "This earlier line of work is worth knowing because it frames low-precision training as a **numerical analysis problem**, not a hardware trick.\n",
    "\n",
    "**The core idea:** training can succeed even when weights/activations/gradients are represented in low precision, *if you control rounding and scaling*.\n",
    "\n",
    "Key takeaways often cited from this family of results:\n",
    "- **Rounding is the enemy.** Deterministic rounding can systematically bias updates. Stochastic rounding can remove that bias at the cost of noise.\n",
    "- **Scaling matters.** Keeping values in a representable dynamic range is the difference between \"noisy training\" and \"dead training\" (underflow to zeros).\n",
    "- **Noise tolerance is real.** SGD is already noisy; in many regimes, extra quantization noise is tolerable *as long as the signal is not destroyed*.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- Autocast is a modern, practical version of \"do low precision where it's safe\".\n",
    "- Loss scaling is a specialized scaling strategy focused on preserving *gradient* signal in FP16.\n",
    "- The accumulation demos in Section 1 (tiny increments and LayerNorm stats) are the same failure modes this literature is trying to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 FP8 for deep learning: why it exists and why it's not just \"AMP but smaller\"\n",
    "\n",
    "FP8 is attractive because it can further reduce memory bandwidth and increase tensor-core throughput compared to FP16/BF16.\n",
    "\n",
    "But FP8 is fundamentally different from FP16/BF16:\n",
    "- the representable grid is *much* coarser\n",
    "- range depends strongly on the FP8 variant (commonly summarized as **E4M3** vs **E5M2**)\n",
    "- most practical FP8 training systems rely on **explicit scaling** (per-tensor/per-channel) and carefully chosen accumulation dtypes\n",
    "\n",
    "**The mental model:** FP8 compute works when you pair it with an explicit scale factor that keeps values near the \"sweet spot\" of the format. This makes FP8 closer to \"learned quantization with dynamic scaling\" than to the drop-in nature of BF16 autocast.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- AMP is primarily about FP16/BF16 (and TF32) policies inside a framework like PyTorch.\n",
    "- FP8 usually requires a specialized kernel stack (often beyond the default `torch.amp.autocast`) that manages scaling metadata alongside tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 8-bit optimizer-state work: the next bottleneck after AMP\n",
    "\n",
    "Classic AMP reduces activation memory and speeds up matmuls, but for large models the **optimizer state** becomes a dominant fixed cost (ZeRO's 16 bytes/parameter for Adam mixed precision).\n",
    "\n",
    "This motivates a separate line of work: compressing the **optimizer state** (and sometimes gradients) to 8-bit representations while preserving training quality.\n",
    "\n",
    "**How it relates to autocast:**\n",
    "- Autocast is about *compute dtype choice per op* during forward/loss (and indirectly backward).\n",
    "- 8-bit optimizers are about *storage precision* and *update math* for long-lived optimizer tensors.\n",
    "\n",
    "They are complementary: you can use AMP for activations/compute and (in some stacks) use 8-bit optimizer states to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 summary\n",
    "\n",
    "| Paper / Source | Key contribution |\n",
    "|---|---|\n",
    "| **Gupta et al.** | Limited precision can work when rounding/scaling are designed, not ignored |\n",
    "| **Micikevicius et al.** | The three-part recipe: per-op policies + loss scaling + FP32 master weights |\n",
    "| **Kalamkar et al.** | BF16's 8-bit exponent matches FP32 → no underflow → no loss scaling needed |\n",
    "| **NVIDIA guidance** | Engineering view: Tensor Cores, dimension alignment, bandwidth savings |\n",
    "| **PyTorch AMP docs** | The per-op dispatch table (the \"decoder ring\" for debugging) |\n",
    "| **ZeRO (Rajbhandari et al.)** | Memory breakdown: 16 bytes/param with Adam mixed precision |\n",
    "| **Distributed stacks** | Precision applies to params, grads, optimizer state, and communication separately |\n",
    "| **FP8 literature** | FP8 needs explicit scaling + specialized kernels; not a drop-in autocast dtype |\n",
    "| **8-bit optimizers** | Optimizer-state precision is the next memory target after AMP |\n",
    "\n",
    "**The single most useful reference for debugging:** the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). Bookmark it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 — Practicalities (experiments + graphs)\n",
    "\n",
    "This section is where we turn everything into measurements.\n",
    "\n",
    "**Principles:**\n",
    "- Prefer experiments that are **small, fast, and explain a single idea**.\n",
    "- Log everything you might need for debugging (loss, grad norms, scaler scale, step time, NaN/inf).\n",
    "- Make results comparable across dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Controlling confounders (TF32, randomness, and fair comparisons)\n",
    "\n",
    "When comparing FP32 vs AMP, you can accidentally compare the wrong thing:\n",
    "\n",
    "1. **TF32 on Ampere+:** Many FP32 matmuls use TF32 internally (10-bit mantissa). Your \"FP32 baseline\" may not be strict FP32 precision.\n",
    "2. **Randomness:** Dropout, data sampling, and nondeterministic kernels introduce run-to-run variance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if device.type == \"cuda\":\n",
    "    print(\"TF32 matmul:\", torch.backends.cuda.matmul.allow_tf32)\n",
    "    print(\"TF32 cuDNN:\", torch.backends.cudnn.allow_tf32)\n",
    "\n",
    "    # Uncomment to disable TF32 for strict FP32 comparisons:\n",
    "    # torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    # torch.backends.cudnn.allow_tf32 = False\n",
    "else:\n",
    "    print(\"CUDA not available; TF32 not applicable\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building mixed precision from scratch (the progressive implementation)\n",
    "\n",
    "This is the most important experiment in the notebook. Instead of jumping straight to `torch.amp`, we will:\n",
    "\n",
    "1. Train a simple model in **FP32** (baseline).\n",
    "2. Try **naive FP16** (just cast everything to half) — watch it fail or stagnate.\n",
    "3. Add **FP32 master weights** — fix the stagnation.\n",
    "4. Add **manual loss scaling** — fix gradient underflow.\n",
    "5. Replace everything with **PyTorch AMP** — the clean two-line version.\n",
    "\n",
    "By building each piece manually, the \"magic\" of AMP becomes completely transparent.\n",
    "\n",
    "We'll use a small MLP on a synthetic regression task to keep things fast and focused. The model is intentionally simple — the numeric effects are the same at any scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Progressive mixed-precision: shared setup\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Synthetic dataset: regression\n",
    "N_SAMPLES = 4096\n",
    "INPUT_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "X_data = torch.randn(N_SAMPLES, INPUT_DIM, device=device)\n",
    "# Target: a noisy linear function (so the model CAN learn it)\n",
    "true_w = torch.randn(INPUT_DIM, OUTPUT_DIM, device=device) * 0.01\n",
    "Y_data = X_data @ true_w + torch.randn(N_SAMPLES, OUTPUT_DIM, device=device) * 0.01\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "PROG_STEPS = 300\n",
    "\n",
    "def get_batches(steps):\n",
    "    for _ in range(steps):\n",
    "        idx = torch.randint(0, N_SAMPLES, (BATCH_SIZE,))\n",
    "        yield X_data[idx], Y_data[idx]\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def grad_stats(model):\n",
    "    # Return fraction of zero gradients and median |grad|.\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().float().flatten())\n",
    "    if not grads:\n",
    "        return 0.0, 0.0\n",
    "    g = torch.cat(grads)\n",
    "    zero_frac = float((g == 0).float().mean())\n",
    "    median_abs = float(g.abs().median())\n",
    "    return zero_frac, median_abs\n",
    "\n",
    "print(f\"Dataset: {N_SAMPLES} samples, input_dim={INPUT_DIM}\")\n",
    "print(f\"Model: MLP {INPUT_DIM} -> {HIDDEN_DIM} -> {HIDDEN_DIM} -> {OUTPUT_DIM}\")\n",
    "print(f\"Training: {PROG_STEPS} steps, batch_size={BATCH_SIZE}, lr={LR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Step 0: FP32 baseline\n",
    "\n",
    "Everything in FP32. This is our reference for correct training behavior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 0: FP32 baseline\n",
    "set_seed(42)\n",
    "model_fp32 = SimpleMLP().to(device).float()\n",
    "opt_fp32 = torch.optim.Adam(model_fp32.parameters(), lr=LR)\n",
    "\n",
    "log_fp32 = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": []}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_fp32.zero_grad(set_to_none=True)\n",
    "    pred = model_fp32(xb)\n",
    "    loss = F.mse_loss(pred, yb)\n",
    "    loss.backward()\n",
    "    zf, mg = grad_stats(model_fp32)\n",
    "    opt_fp32.step()\n",
    "    log_fp32[\"loss\"].append(float(loss))\n",
    "    log_fp32[\"zero_grad_frac\"].append(zf)\n",
    "    log_fp32[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"FP32 baseline: final loss = {log_fp32['loss'][-1]:.6f}\")\n",
    "print(f\"  Zero-grad fraction: {log_fp32['zero_grad_frac'][-1]:.4f}\")\n",
    "print(f\"  Median |grad|: {log_fp32['median_grad'][-1]:.2e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Step 1: Naive FP16 (just cast everything to half)\n",
    "\n",
    "The simplest approach: `model.half()` and cast inputs to FP16. No master weights, no scaling, no autocast.\n",
    "\n",
    "This is what the Micikevicius paper warns against. Expect problems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1: Naive FP16\n",
    "log_fp16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_fp16_naive[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping naive FP16 demo: CPU/MPS fp16 matmuls are often unsupported or unrepresentative.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_fp16_naive = SimpleMLP().to(device).half()\n",
    "    opt_fp16_naive = torch.optim.Adam(model_fp16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_fp16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_fp16_naive(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_fp16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_fp16_naive)\n",
    "        opt_fp16_naive.step()\n",
    "        log_fp16_naive[\"loss\"].append(float(loss))\n",
    "        log_fp16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_fp16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive FP16: status = {log_fp16_naive['status']}\")\n",
    "    if log_fp16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_fp16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_fp16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_fp16_naive['median_grad'][-1]:.2e}\")\n",
    "    else:\n",
    "        print(\"  Training failed immediately.\")\n",
    "\n",
    "    print()\n",
    "    if log_fp16_naive[\"status\"] != \"ok\" or (log_fp16_naive[\"loss\"] and log_fp16_naive[\"loss\"][-1] > log_fp32[\"loss\"][-1] * 5):\n",
    "        print(\"As expected, naive FP16 is problematic. The combination of:\")\n",
    "        print(\"  1. Weight update stagnation (updates below FP16 ULP)\")\n",
    "        print(\"  2. Gradient underflow (small gradients become zero)\")\n",
    "        print(\"makes training unstable or ineffective.\")\n",
    "    else:\n",
    "        print(\"Naive FP16 happened to work for this model (it sometimes does for simple/small models).\")\n",
    "        print(\"This does NOT mean it's safe in general.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2b Step 1b: Naive BF16 (the same cast-everything approach, but with BF16)\n",
    "\n",
    "Now do the *exact same thing* but with BF16 instead of FP16. This is the key comparison that shows **why BF16's range matters more than FP16's precision** for training stability.\n",
    "\n",
    "Same code, same model, same learning rate — just a different 16-bit format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1b: Naive BF16\n",
    "log_bf16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if not supports_dtype_on_device(torch.bfloat16, device):\n",
    "    log_bf16_naive[\"status\"] = \"skipped_no_bf16\"\n",
    "    print(\"Skipping naive BF16 demo: BF16 not supported on this device.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_bf16_naive = SimpleMLP().to(device).to(torch.bfloat16)\n",
    "    opt_bf16_naive = torch.optim.Adam(model_bf16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_bf16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_bf16_naive(xb.to(torch.bfloat16))\n",
    "        loss = F.mse_loss(pred, yb.to(torch.bfloat16))\n",
    "        if not torch.isfinite(loss):\n",
    "            log_bf16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_bf16_naive)\n",
    "        opt_bf16_naive.step()\n",
    "        log_bf16_naive[\"loss\"].append(float(loss))\n",
    "        log_bf16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_bf16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive BF16: status = {log_bf16_naive['status']}\")\n",
    "    if log_bf16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_bf16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_bf16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_bf16_naive['median_grad'][-1]:.2e}\")\n",
    "\n",
    "    print()\n",
    "    if log_bf16_naive[\"status\"] == \"ok\" and log_bf16_naive[\"loss\"]:\n",
    "        if log_bf16_naive[\"loss\"][-1] < log_fp32[\"loss\"][-1] * 2:\n",
    "            print(\"BF16 naive training WORKS! This is the key insight:\")\n",
    "            print(\"  - BF16 has the same exponent range as FP32 (8-bit exponent)\")\n",
    "            print(\"  - Gradients don't underflow, so training progresses even without loss scaling\")\n",
    "            print(\"  - The coarser precision (7-bit mantissa) introduces noise, but SGD tolerates noise\")\n",
    "            print(\"  - Compare this to FP16 naive above: same approach, different outcome, entirely due to range.\")\n",
    "        else:\n",
    "            print(\"BF16 naive converged but to a higher loss — precision may have limited final accuracy.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Step 2: Add FP32 master weights\n",
    "\n",
    "The first fix from the Micikevicius paper: keep an FP32 copy of parameters for the optimizer update.\n",
    "\n",
    "**The flow:**\n",
    "1. Forward pass uses FP16 parameters (for speed/memory).\n",
    "2. Backward produces FP16 gradients.\n",
    "3. Copy FP16 gradients to FP32 master parameters.\n",
    "4. Optimizer updates FP32 master weights.\n",
    "5. Copy updated FP32 weights back to FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 2: FP16 + FP32 master weights\n",
    "log_master = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_master[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 master-weights demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_master = SimpleMLP().to(device).half()  # FP16 for forward/backward\n",
    "\n",
    "    # Create FP32 master copy\n",
    "    master_params = [p.detach().clone().float().requires_grad_(True) for p in model_master.parameters()]\n",
    "    opt_master = torch.optim.Adam(master_params, lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_master.zero_grad(set_to_none=True)\n",
    "        model_master.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward in FP16\n",
    "        pred = model_master(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_master[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master params\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float()\n",
    "\n",
    "        zf, mg = grad_stats(model_master)\n",
    "\n",
    "        # Update FP32 master weights\n",
    "        opt_master.step()\n",
    "\n",
    "        # Copy FP32 master -> FP16 model\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_master[\"loss\"].append(float(loss))\n",
    "        log_master[\"zero_grad_frac\"].append(zf)\n",
    "        log_master[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights: status = {log_master['status']}\")\n",
    "    if log_master[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_master['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_master['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Step 3: Add loss scaling\n",
    "\n",
    "The second fix: multiply the loss by a scale factor $S$ before backward, then unscale gradients before the optimizer step. This prevents small gradients from underflowing to zero in FP16."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: FP16 + FP32 master weights + loss scaling\n",
    "log_scaled = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_scaled[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 loss-scaling demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_scaled = SimpleMLP().to(device).half()\n",
    "    master_params_s = [p.detach().clone().float().requires_grad_(True) for p in model_scaled.parameters()]\n",
    "    opt_scaled = torch.optim.Adam(master_params_s, lr=LR)\n",
    "\n",
    "    LOSS_SCALE = 2**13  # 8192 — a common starting point\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_scaled.zero_grad(set_to_none=True)\n",
    "        model_scaled.zero_grad(set_to_none=True)\n",
    "\n",
    "        pred = model_scaled(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_scaled[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "\n",
    "        # Scale loss before backward\n",
    "        (loss * LOSS_SCALE).backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master, then UNSCALE\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float() / LOSS_SCALE\n",
    "\n",
    "        zf, mg = grad_stats(model_scaled)\n",
    "\n",
    "        opt_scaled.step()\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_scaled[\"loss\"].append(float(loss))\n",
    "        log_scaled[\"zero_grad_frac\"].append(zf)\n",
    "        log_scaled[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights + scaling: status = {log_scaled['status']}\")\n",
    "    if log_scaled[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_scaled['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_scaled['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Step 4: PyTorch AMP (the clean version)\n",
    "\n",
    "Now replace all the manual work with PyTorch's `autocast` + `GradScaler`. Two extra lines of code.\n",
    "\n",
    "Note: autocast handles per-op dtype policies automatically. The model stays in FP32, and autocast temporarily casts operations during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: PyTorch AMP\n",
    "set_seed(42)\n",
    "model_amp = SimpleMLP().to(device).float()  # FP32 — autocast handles casting\n",
    "opt_amp = torch.optim.Adam(model_amp.parameters(), lr=LR)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "elif device.type == \"cpu\":\n",
    "    amp_dtype = torch.bfloat16\n",
    "    use_scaler = False\n",
    "else:  # mps (experimental AMP coverage)\n",
    "    amp_dtype = torch.float16\n",
    "    use_scaler = False\n",
    "\n",
    "scaler = GradScaler(enabled=(use_scaler and device.type == \"cuda\")) if use_scaler else None\n",
    "\n",
    "log_amp = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\", \"dtype\": str(amp_dtype)}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_amp.zero_grad(set_to_none=True)\n",
    "\n",
    "    with amp_autocast(device, amp_dtype, enabled=True):\n",
    "        pred = model_amp(xb)\n",
    "        loss = F.mse_loss(pred, yb)\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "        log_amp[\"status\"] = \"non_finite_loss\"\n",
    "        break\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt_amp)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        opt_amp.step()\n",
    "\n",
    "    zf, mg = grad_stats(model_amp)\n",
    "    log_amp[\"loss\"].append(float(loss))\n",
    "    log_amp[\"zero_grad_frac\"].append(zf)\n",
    "    log_amp[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"PyTorch AMP ({log_amp['dtype']}): status = {log_amp['status']}\")\n",
    "if log_amp[\"loss\"]:\n",
    "    print(f\"  Final loss = {log_amp['loss'][-1]:.6f}\")\n",
    "    print(f\"  Zero-grad fraction: {log_amp['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare all progressive approaches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "configs = [\n",
    "    (\"FP32 baseline\", log_fp32, \"C0\"),\n",
    "    (\"Naive FP16\", log_fp16_naive, \"C3\"),\n",
    "    (\"Naive BF16\", log_bf16_naive, \"C5\"),\n",
    "    (\"FP16 + master wts\", log_master, \"C1\"),\n",
    "    (\"FP16 + master + scaling\", log_scaled, \"C4\"),\n",
    "    (f\"PyTorch AMP ({log_amp['dtype']})\", log_amp, \"C2\"),\n",
    "]\n",
    "\n",
    "# Loss curves\n",
    "for name, log, color in configs:\n",
    "    if log[\"loss\"]:\n",
    "        axes[0].plot(log[\"loss\"], label=name, color=color, alpha=0.8)\n",
    "axes[0].set_title(\"Training loss\")\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"MSE loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "# Zero gradient fraction\n",
    "for name, log, color in configs:\n",
    "    if log[\"zero_grad_frac\"]:\n",
    "        axes[1].plot(log[\"zero_grad_frac\"], label=name, color=color, alpha=0.8)\n",
    "axes[1].set_title(\"Fraction of zero gradients\")\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"fraction\")\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# Median gradient magnitude\n",
    "for name, log, color in configs:\n",
    "    if log[\"median_grad\"]:\n",
    "        axes[2].plot(log[\"median_grad\"], label=name, color=color, alpha=0.8)\n",
    "axes[2].set_title(\"Median |gradient|\")\n",
    "axes[2].set_xlabel(\"step\")\n",
    "axes[2].set_ylabel(\"|grad|\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Progressive Mixed Precision: from naive FP16 to PyTorch AMP\", fontsize=12, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 What to observe\n",
    "\n",
    "**Loss curves:**\n",
    "- FP32 baseline should decrease smoothly.\n",
    "- Naive FP16 may diverge, stagnate, or produce NaN.\n",
    "- **Naive BF16 often trains successfully** — this is the dramatic demonstration of range vs precision. Same \"cast everything to 16-bit\" approach, but BF16's 8-bit exponent prevents the gradient underflow that kills FP16.\n",
    "- Adding master weights to FP16 typically helps convergence.\n",
    "- Adding loss scaling to FP16 reduces the fraction of zero gradients.\n",
    "- PyTorch AMP should match or beat the manual implementations.\n",
    "\n",
    "**Zero gradient fraction:**\n",
    "- In naive FP16, many gradients may be exactly zero (underflow).\n",
    "- In naive BF16, almost no gradients are zero — the exponent range is wide enough.\n",
    "- Loss scaling shifts the FP16 distribution, rescuing underflowed gradients.\n",
    "\n",
    "**Median gradient magnitude:**\n",
    "- Shows the \"signal strength\" available to the optimizer.\n",
    "- If it drops to zero, the model stops learning.\n",
    "- Compare FP16 vs BF16: even with coarser mantissa, BF16 preserves gradient *signal*.\n",
    "\n",
    "**The key lesson from this progressive build-up:**\n",
    "Naive FP16 fails. BF16 naive often works. But both benefit from FP32 master weights and proper AMP policies. The Micikevicius paper's three-part recipe (per-op policy + loss scaling + master weights) handles FP16 correctly. BF16 simplifies the picture by removing the need for loss scaling, but master weights and per-op policies still help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build an \"autocast operator policy table\" from your local PyTorch\n",
    "\n",
    "Instead of trusting a static table from the internet, we can probe your exact PyTorch version:\n",
    "1. Run each op with autocast **disabled** → observe output dtype.\n",
    "2. Run each op with autocast **enabled** → observe output dtype.\n",
    "3. Compare to see which ops autocast changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Enhanced operator policy probe\n",
    "\n",
    "def probe_ops(dev, amp_dtype):\n",
    "    a32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    b32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x128 = torch.randn(2, 128, device=dev, dtype=torch.float32)\n",
    "    x_big = torch.randn(2, 20000, device=dev, dtype=torch.float32)\n",
    "    w32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    bias32 = torch.randn(128, device=dev, dtype=torch.float32)\n",
    "    target32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x16 = x_big.to(amp_dtype)\n",
    "\n",
    "    ops = {\n",
    "        # Matmul-like (expect lower precision)\n",
    "        \"matmul (a @ b)\": lambda: a32 @ b32,\n",
    "        \"F.linear\": lambda: F.linear(a32, w32, bias32),\n",
    "        # Numerically sensitive (expect FP32)\n",
    "        \"F.softmax\": lambda: F.softmax(x128, dim=-1),\n",
    "        \"F.layer_norm\": lambda: F.layer_norm(x128, [x128.size(-1)]),\n",
    "        \"F.cross_entropy\": lambda: F.cross_entropy(a32[:10], torch.randint(0, 128, (10,), device=dev)),\n",
    "        \"F.mse_loss\": lambda: F.mse_loss(a32, target32),\n",
    "        \"torch.exp\": lambda: torch.exp(x128),\n",
    "        \"torch.log\": lambda: torch.log(x128.abs() + 1e-6),\n",
    "        \"torch.sum\": lambda: x_big.sum(),\n",
    "        \"torch.prod\": lambda: x_big[:, :10].prod(),\n",
    "        # Pass-through (expect input dtype)\n",
    "        \"F.relu\": lambda: F.relu(x_big),\n",
    "        \"torch.max\": lambda: x_big.max(),\n",
    "        \"torch.min\": lambda: x_big.min(),\n",
    "        \"torch.mean\": lambda: x_big.mean(),\n",
    "        \"F.dropout\": lambda: F.dropout(x_big, p=0.0, training=True),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for name, fn in ops.items():\n",
    "        # Without autocast\n",
    "        y_no = fn()\n",
    "        dt_no = str(y_no.dtype) if isinstance(y_no, torch.Tensor) else \"n/a\"\n",
    "        # With autocast (FP32 inputs)\n",
    "        with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "            y_ac = fn()\n",
    "        dt_ac = str(y_ac.dtype) if isinstance(y_ac, torch.Tensor) else \"n/a\"\n",
    "\n",
    "        # With autocast (16-bit inputs, where applicable)\n",
    "        dt_ac16 = \"\"\n",
    "        try:\n",
    "            # Replace x32 temporarily\n",
    "            ops_16 = {\n",
    "                \"F.relu\": lambda: F.relu(x16),\n",
    "                \"torch.max\": lambda: x16.max(),\n",
    "                \"torch.min\": lambda: x16.min(),\n",
    "                \"torch.mean\": lambda: x16.mean(),\n",
    "                \"torch.sum\": lambda: x16.sum(),\n",
    "            }\n",
    "            if name in ops_16:\n",
    "                with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "                    y16 = ops_16[name]()\n",
    "                dt_ac16 = str(y16.dtype)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        changed = dt_no != dt_ac\n",
    "        rows.append({\n",
    "            \"op\": name,\n",
    "            \"no_autocast\": dt_no,\n",
    "            f\"autocast({amp_dtype})_fp32_input\": dt_ac,\n",
    "            \"16bit_input\": dt_ac16 if dt_ac16 else \"-\",\n",
    "            \"policy\": \"LOWER PREC\" if \"float16\" in dt_ac or \"bfloat16\" in dt_ac else (\"FP32\" if changed or dt_ac == \"torch.float32\" else \"pass-through\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "for amp_dt in [torch.float16, torch.bfloat16]:\n",
    "    if device.type == \"cuda\" and amp_dt is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n",
    "        continue\n",
    "    if device.type == \"cpu\" and amp_dt is torch.float16:\n",
    "        continue\n",
    "    if not supports_dtype_on_device(amp_dt, device):\n",
    "        continue\n",
    "    print(f\"\\n=== Autocast policy with amp_dtype={amp_dt} ===\")\n",
    "    display(probe_ops(device, amp_dt))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 The \"sum vs mean\" mystery\n",
    "\n",
    "Under autocast, `sum` produces FP32 output but `mean` stays in the input dtype. Why?\n",
    "\n",
    "**`sum`** is a reduction that accumulates values. Summing 20,000 BF16 values can easily exceed the representable range (BF16 max ~ $3.4 \\times 10^{38}$, but even FP16 max is only ~65,504). More critically, the rounding errors from adding many small values compound. PyTorch hardcodes `sum` to promote to FP32.\n",
    "\n",
    "**`mean`** inherently divides by $N$, keeping the output bounded between the min and max of the input. It cannot overflow by accumulation. PyTorch treats it as a pass-through — whatever dtype goes in, the same comes out.\n",
    "\n",
    "**`prod`** is similar to `sum` but even more extreme: multiplying many values can grow (or shrink) astronomically. Also forced to FP32.\n",
    "\n",
    "Let's verify this directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The sum vs mean mystery — direct probe\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    x16 = torch.randn(2, 20000, device=device, dtype=dtype_16bit)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "        results = {\n",
    "            \"max\": x16.max().dtype,\n",
    "            \"min\": x16.min().dtype,\n",
    "            \"sum\": x16.sum().dtype,\n",
    "            \"mean\": x16.mean().dtype,\n",
    "            \"prod\": x16.prod().dtype,\n",
    "            \"exp\": x16.exp().dtype,\n",
    "            \"log\": x16.abs().log().dtype,\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for op, dt in results.items():\n",
    "        rows.append({\n",
    "            \"op\": op,\n",
    "            \"output_dtype\": str(dt),\n",
    "            \"promoted_to_fp32\": \"YES\" if dt == torch.float32 else \"no\",\n",
    "            \"reason\": {\n",
    "                \"max\": \"element-wise selection, no accumulation\",\n",
    "                \"min\": \"element-wise selection, no accumulation\",\n",
    "                \"sum\": \"ACCUMULATION: rounding errors compound over N values\",\n",
    "                \"mean\": \"bounded output (divides by N), no overflow risk\",\n",
    "                \"prod\": \"ACCUMULATION: multiplicative explosion/collapse\",\n",
    "                \"exp\": \"can produce extreme magnitudes (overflow risk)\",\n",
    "                \"log\": \"can produce extreme magnitudes (underflow risk)\",\n",
    "            }.get(op, \"\"),\n",
    "        })\n",
    "\n",
    "    display(pd.DataFrame(rows))\n",
    "else:\n",
    "    print(\"Run on CUDA to see the sum vs mean mystery in action.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Watch dtype flow through a transformer (4 configurations)\n",
    "\n",
    "This is the \"visceral\" version of the operator policy: instead of probing individual ops, we observe dtypes flowing through a real transformer model.\n",
    "\n",
    "We'll test all four combinations from the toy example (source1):\n",
    "\n",
    "| Config | Model params | Autocast | What to observe |\n",
    "|---|---|---|---|\n",
    "| A | FP32 | OFF | Everything FP32 (baseline) |\n",
    "| B | FP16/BF16 | OFF | Everything 16-bit (no policy) |\n",
    "| C | FP16/BF16 | ON | LayerNorm→FP32, Linear→16-bit, residuals→16-bit |\n",
    "| D | FP32 | ON | Linear→16-bit even with FP32 weights! LayerNorm stays FP32, residuals→FP32 (promotion) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tiny transformer model for dtype tracing\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        att = att.masked_fill(mask, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer=2, n_embd=128, n_heads=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(n_embd, n_heads, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"TinyGPT defined: 2-layer transformer for dtype tracing and training experiments.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dtype hooks + 4-configuration trace\n",
    "\n",
    "def install_dtype_hooks(model, watch=(nn.Linear, nn.LayerNorm, nn.Embedding)):\n",
    "    hooks, records = [], []\n",
    "    def make_hook(name):\n",
    "        def hook(m, inp, out):\n",
    "            def dt(x):\n",
    "                return str(x.dtype) if isinstance(x, torch.Tensor) else type(x).__name__\n",
    "            indt = dt(inp[0]) if isinstance(inp, (tuple, list)) and inp else dt(inp)\n",
    "            oudt = dt(out) if not isinstance(out, (tuple, list)) else dt(out[0])\n",
    "            records.append({\"module\": name, \"type\": type(m).__name__, \"in_dtype\": indt, \"out_dtype\": oudt})\n",
    "        return hook\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, watch):\n",
    "            hooks.append(m.register_forward_hook(make_hook(name)))\n",
    "    return hooks, records\n",
    "\n",
    "VOCAB = 128\n",
    "BLOCK = 64\n",
    "idx = torch.randint(0, VOCAB, (2, BLOCK), device=device)\n",
    "\n",
    "dtype_16 = torch.bfloat16\n",
    "if device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    dtype_16 = torch.float16\n",
    "elif device.type == \"mps\":\n",
    "    dtype_16 = torch.float16\n",
    "\n",
    "configs = [\n",
    "    (\"A: params=FP32, autocast=OFF\", torch.float32, False),\n",
    "    (\"B: params=16bit, autocast=OFF\", dtype_16, False),\n",
    "    (\"C: params=16bit, autocast=ON\",  dtype_16, True),\n",
    "    (\"D: params=FP32, autocast=ON\",   torch.float32, True),\n",
    "]\n",
    "\n",
    "for title, param_dt, use_ac in configs:\n",
    "    model = TinyGPT(VOCAB, BLOCK).to(device).to(param_dt)\n",
    "    hooks, rec = install_dtype_hooks(model)\n",
    "    ctx = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "    with torch.inference_mode(), ctx:\n",
    "        _ = model(idx)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    df = pd.DataFrame(rec)\n",
    "    df[\"count\"] = 1\n",
    "    summary = df.groupby([\"type\", \"in_dtype\", \"out_dtype\"], as_index=False)[\"count\"].sum()\n",
    "    summary = summary.sort_values([\"type\", \"in_dtype\", \"out_dtype\"])\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    display(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 What the traces reveal\n",
    "\n",
    "**Config A (FP32, no autocast):** Everything is FP32. Baseline.\n",
    "\n",
    "**Config B (16-bit, no autocast):** Everything is 16-bit. No per-op policy. LayerNorm runs in 16-bit (risky for accumulation). Softmax runs in 16-bit (risky for overflow).\n",
    "\n",
    "**Config C (16-bit params, autocast ON):**\n",
    "- LayerNorm: input is 16-bit → **output is FP32** (autocast forces FP32 for normalization)\n",
    "- Linear after LayerNorm: **input is FP32 → output is 16-bit** (autocast casts FP32 weights/inputs to 16-bit for the matmul)\n",
    "- This is the key insight: autocast doesn't just \"use 16-bit everywhere.\" It routes different ops to different precisions.\n",
    "\n",
    "**Config D (FP32 params, autocast ON):**\n",
    "- Linear: **FP32 input → 16-bit output** (autocast temporarily casts FP32 weights to 16-bit!)\n",
    "- LayerNorm: FP32 → FP32 (stays in FP32, as it should)\n",
    "- Residual adds: 16-bit + FP32 → **FP32** (dtype promotion)\n",
    "- This is the \"standard\" AMP configuration: model stays in FP32, autocast handles per-op casting.\n",
    "\n",
    "```\n",
    "Config D data flow (FP32 params + autocast):\n",
    "\n",
    "[int64 tokens]\n",
    "       |\n",
    "  +----+---------------------+\n",
    "  |                          |\n",
    "[embed_tok] int64->fp32    [embed_pos] int64->fp32\n",
    "  |                          |\n",
    "  +-------- sum (fp32+fp32->fp32) --------+\n",
    "                                          |\n",
    "                                        [LN1] fp32->fp32\n",
    "                                          |\n",
    "                      +---------+---------+---------+\n",
    "                      |         |                   |\n",
    "                   [q_proj]  [k_proj]           [v_proj]\n",
    "                    fp32->bf16  fp32->bf16      fp32->bf16\n",
    "                      \\         |                 /\n",
    "                       \\        |                /\n",
    "                        +----[attn + softmax]----+   (bf16 compute)\n",
    "                                          |\n",
    "                                    [out_proj] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "                                          |\n",
    "                                        [LN2] fp32->fp32\n",
    "                                          |\n",
    "                                       [fc1] fp32->bf16\n",
    "                                          |\n",
    "                                       [fc2] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "```\n",
    "\n",
    "The promotion at residual connections is a key feature: it prevents precision loss from accumulating through the network's residual stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gradient underflow and why loss scaling works\n",
    "\n",
    "We'll do a controlled experiment:\n",
    "1. Create a synthetic gradient distribution spanning many orders of magnitude.\n",
    "2. Cast it to FP16 and count how many values become exactly 0.\n",
    "3. Apply a scale factor $S$, cast, then unscale.\n",
    "\n",
    "This shows the core mechanism of loss scaling without needing a full training run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient underflow + rescue via scaling\n",
    "\n",
    "N = 200_000\n",
    "log10_mag = torch.empty(N).uniform_(-12, 0)  # 1e-12 to 1\n",
    "sign = torch.randint(0, 2, (N,)) * 2 - 1\n",
    "synthetic = (10 ** log10_mag) * sign\n",
    "synthetic = synthetic.to(torch.float32)\n",
    "\n",
    "rows = []\n",
    "for S_label, S in [(\"unscaled (S=1)\", 1), (\"S=2^10 (1024)\", 2**10), (\"S=2^13 (8192)\", 2**13), (\"S=2^16 (65536)\", 2**16)]:\n",
    "    scaled = synthetic * S\n",
    "    for dt in [torch.float16, torch.bfloat16]:\n",
    "        g = scaled.to(dt)\n",
    "        zeros = float((g == 0).float().mean())\n",
    "        infs = float(torch.isinf(g).float().mean())\n",
    "        rows.append({\n",
    "            \"scaling\": S_label,\n",
    "            \"dtype\": str(dt),\n",
    "            \"zero_frac\": f\"{zeros:.3f}\",\n",
    "            \"inf_frac\": f\"{infs:.4f}\",\n",
    "            \"preserved_frac\": f\"{1 - zeros - infs:.3f}\",\n",
    "        })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize gradient distribution vs FP16 thresholds\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal = float(fi16.tiny)\n",
    "min_sub = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "vals = synthetic.abs().cpu().numpy()\n",
    "axes[0].hist(np.log10(vals + 1e-30), bins=200, alpha=0.7, color=\"steelblue\")\n",
    "axes[0].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=f\"FP16 min normal ({min_normal:.1e})\")\n",
    "axes[0].axvline(np.log10(min_sub), color=\"m\", ls=\":\", label=f\"FP16 min subnormal ({min_sub:.1e})\")\n",
    "axes[0].set_title(\"Unscaled |grad| distribution\")\n",
    "axes[0].set_xlabel(\"log10(|grad|)\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# After scaling by 2^13\n",
    "scaled_vals = (synthetic * 2**13).abs().cpu().numpy()\n",
    "axes[1].hist(np.log10(scaled_vals + 1e-30), bins=200, alpha=0.7, color=\"darkorange\")\n",
    "axes[1].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=\"FP16 min normal\")\n",
    "axes[1].axvline(np.log10(float(fi16.max)), color=\"darkred\", ls=\"-.\", label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "axes[1].set_title(\"Scaled by 2^13: distribution shifts right\")\n",
    "axes[1].set_xlabel(\"log10(|grad| * S)\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Loss scaling shifts gradients into FP16's representable range\", fontsize=11, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Underflow in a real backward pass\n",
    "\n",
    "Now we show the actual training failure mode with a tiny FP16 network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Real backward underflow demo\n",
    "\n",
    "def tiny_backward(use_scaling, scale=2**13):\n",
    "    model = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1)).to(device).half()\n",
    "    x = (torch.randn(256, 128, device=device) * 1e-3).half()\n",
    "    y = (torch.randn(256, 1, device=device) * 1e-3).half()\n",
    "    pred = model(x)\n",
    "    loss = ((pred - y)**2).mean()\n",
    "    if use_scaling:\n",
    "        (loss * scale).backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.div_(scale)\n",
    "    else:\n",
    "        loss.backward()\n",
    "    grads = torch.cat([p.grad.flatten().abs().float() for p in model.parameters() if p.grad is not None])\n",
    "    return float(loss), float((grads == 0).float().mean()), float(grads.median()), grads\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    loss0, z0, med0, g0 = tiny_backward(use_scaling=False)\n",
    "    loss1, z1, med1, g1 = tiny_backward(use_scaling=True)\n",
    "    display(pd.DataFrame([\n",
    "        {\"setting\": \"FP16, no scaling\", \"loss\": f\"{loss0:.6f}\", \"zero_grad_frac\": f\"{z0:.3f}\", \"median|grad|\": f\"{med0:.2e}\"},\n",
    "        {\"setting\": \"FP16, scaled+unscaled\", \"loss\": f\"{loss1:.6f}\", \"zero_grad_frac\": f\"{z1:.3f}\", \"median|grad|\": f\"{med1:.2e}\"},\n",
    "    ]))\n",
    "else:\n",
    "    print(\"Run on CUDA for the FP16 backward demo.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 The Micikevicius gradient histogram: where do real gradients live?\n",
    "\n",
    "The Micikevicius paper's most famous figure shows a histogram of gradient magnitudes during FP32 training, overlaid with FP16's representable range. Let's reproduce this analysis with our model.\n",
    "\n",
    "This visualization answers the question: *What fraction of the training signal would you lose by switching to FP16 or BF16?*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient histogram analysis (Micikevicius-style)\n",
    "\n",
    "def collect_gradient_histogram(model_class, device, dtype=torch.float32, steps=20):\n",
    "    # Collect all gradient values from several training steps.\n",
    "    set_seed(42)\n",
    "    model = model_class().to(device).to(dtype)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    all_grads = []\n",
    "\n",
    "    for xb, yb in get_batches(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        pred = model(xb.to(dtype))\n",
    "        loss = F.mse_loss(pred, yb.to(dtype))\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                all_grads.append(p.grad.detach().float().flatten().cpu())\n",
    "        opt.step()\n",
    "\n",
    "    return torch.cat(all_grads)\n",
    "\n",
    "grads_fp32 = collect_gradient_histogram(SimpleMLP, device, torch.float32, steps=30)\n",
    "nonzero_grads = grads_fp32[grads_fp32 != 0]\n",
    "log_abs_grads = torch.log10(nonzero_grads.abs()).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: gradient histogram with FP16 thresholds\n",
    "ax = axes[0]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal_16 = float(fi16.tiny)\n",
    "min_sub_16 = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=2, label=f\"FP16 min normal ({min_normal_16:.1e})\")\n",
    "ax.axvline(np.log10(min_sub_16), color=\"darkred\", ls=\":\", lw=1.5, label=f\"FP16 min subnormal ({min_sub_16:.1e})\")\n",
    "ax.axvline(np.log10(float(fi16.max)), color=\"orange\", ls=\"-.\", lw=1.5, label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "\n",
    "# Shade the underflow zone\n",
    "ax.axvspan(ax.get_xlim()[0], np.log10(min_sub_16), alpha=0.15, color=\"red\", label=\"FP16 underflow zone\")\n",
    "\n",
    "ax.set_title(\"FP32 gradient magnitudes vs FP16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "# Right: same but with BF16 thresholds\n",
    "ax = axes[1]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi_bf16 = torch.finfo(torch.bfloat16)\n",
    "min_normal_bf16 = float(fi_bf16.tiny)\n",
    "\n",
    "ax.axvline(np.log10(min_normal_bf16), color=\"green\", ls=\"--\", lw=2, label=f\"BF16 min normal ({min_normal_bf16:.1e})\")\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=1.5, alpha=0.5, label=f\"FP16 min normal (for comparison)\")\n",
    "\n",
    "ax.set_title(\"Same gradients vs BF16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "fig.suptitle(\"Micikevicius-style analysis: gradient magnitude distribution\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Quantify\n",
    "fp16_underflow_frac = float((nonzero_grads.abs() < min_normal_16).float().mean())\n",
    "bf16_underflow_frac = float((nonzero_grads.abs() < min_normal_bf16).float().mean())\n",
    "print(f\"\\nGradients collected: {len(nonzero_grads):,}\")\n",
    "print(f\"Fraction that would underflow in FP16: {fp16_underflow_frac:.4f} ({fp16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"Fraction that would underflow in BF16: {bf16_underflow_frac:.4f} ({bf16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"\\nThis is why FP16 needs loss scaling and BF16 usually doesn't.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Weight update stagnation (why FP32 master weights matter)\n",
    "\n",
    "Even if you avoid underflow, you can lose learning signal if weight updates are **below the ULP** of the weight's dtype.\n",
    "\n",
    "If $w \\approx 1$ in FP16, the ULP is ~$10^{-3}$. Any update $\\Delta w < 10^{-3}$ is rounded away."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weight stagnation demo\n",
    "\n",
    "def apply_updates(dtype, w0=1.0, delta=1e-5, steps=2000):\n",
    "    w = torch.tensor(w0, dtype=dtype)\n",
    "    changed = 0\n",
    "    for _ in range(steps):\n",
    "        w_new = w - torch.tensor(delta, dtype=dtype)\n",
    "        changed += int(w_new.item() != w.item())\n",
    "        w = w_new\n",
    "    return {\n",
    "        \"dtype\": str(dtype),\n",
    "        \"w0\": w0,\n",
    "        \"delta\": f\"{delta:.0e}\",\n",
    "        \"steps\": steps,\n",
    "        \"steps_where_w_changed\": changed,\n",
    "        \"final_w\": f\"{float(w):.6f}\",\n",
    "        \"expected_final\": f\"{w0 - delta * steps:.6f}\",\n",
    "        \"ulp_at_1.0\": f\"{_ulp_at_one(dtype):.2e}\",\n",
    "    }\n",
    "\n",
    "rows = [apply_updates(dt) for dt in [torch.float16, torch.bfloat16, torch.float32]]\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nFP16: delta=1e-5 is below ULP at 1.0 (~1e-3). Weight NEVER changes.\")\n",
    "print(\"BF16: delta=1e-5 is below ULP at 1.0 (~8e-3). Weight NEVER changes.\")\n",
    "print(\"FP32: delta=1e-5 is above ULP at 1.0 (~1e-7). Weight changes every step.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 The main event: train a tiny causal LM under different precision regimes\n",
    "\n",
    "We'll train TinyGPT on a character-level next-token prediction task using an in-notebook corpus.\n",
    "\n",
    "**Why character-level?** No external downloads, stable, deterministic, and still exercises the transformer mechanics that matter for autocast (attention, layernorm, softmax, embeddings).\n",
    "\n",
    "**We will compare:**\n",
    "- FP32 baseline\n",
    "- Naive FP16 (params in FP16, no autocast, no scaler) — expected to be unstable\n",
    "- AMP FP16 (autocast + GradScaler)\n",
    "- AMP BF16 (autocast, no scaler)\n",
    "\n",
    "**We will log:**\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Gradient norm\n",
    "- Step time\n",
    "- CUDA memory\n",
    "- GradScaler scale (for FP16 AMP)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tiny corpus + character-level tokenizer\n",
    "\n",
    "corpus_lines = [\n",
    "    \"Autocast is not a global cast. It is an operator policy.\",\n",
    "    \"Some ops run in lower precision for speed.\",\n",
    "    \"Some ops run in float32 for stability.\",\n",
    "    \"Loss scaling rescues fp16 gradients from underflow.\",\n",
    "    \"Bfloat16 usually has enough exponent range to avoid underflow.\",\n",
    "    \"Transformers amplify numeric issues via softmax, layernorm, and large reductions.\",\n",
    "    \"AMP exists to route the right operations to the right dtype.\",\n",
    "    \"Master weights in fp32 prevent update stagnation.\",\n",
    "    \"Optimizer state should be kept in fp32 for long-horizon accumulation.\",\n",
    "    \"The autocast operator policy is the decoder ring for debugging AMP.\",\n",
    "]\n",
    "\n",
    "corpus = \"\\n\".join(corpus_lines).strip()\n",
    "corpus = corpus * 50  # repeat for more training data\n",
    "\n",
    "chars = sorted(set(corpus))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode_tokens(ids):\n",
    "    return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "data = torch.tensor(encode(corpus), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_data):,}, Val tokens: {len(val_data):,}\")\n",
    "print(f\"Sample: {decode_tokens(train_data[:80].tolist())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Batch sampling + evaluation\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(src) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([src[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([src[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size, iters=20, use_autocast=False, amp_dtype=None):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            with amp_autocast(device, amp_dtype, enabled=use_autocast):\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(float(loss))\n",
    "        out[split] = float(np.mean(losses))\n",
    "    model.train()\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training infrastructure\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    name: str\n",
    "    steps: int = 200\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 64\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    use_autocast: bool = False\n",
    "    amp_dtype: torch.dtype = None\n",
    "    use_grad_scaler: bool = False\n",
    "    param_dtype: torch.dtype = torch.float32\n",
    "    eval_interval: int = 50\n",
    "    eval_iters: int = 10\n",
    "\n",
    "def global_grad_norm(model):\n",
    "    total_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_sq += float(p.grad.detach().float().norm())**2\n",
    "    return math.sqrt(total_sq)\n",
    "\n",
    "def global_zero_grad_frac(model):\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g = p.grad.detach()\n",
    "        zeros += int((g == 0).sum())\n",
    "        total += g.numel()\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "def train_one(cfg):\n",
    "    set_seed(42)\n",
    "    model = TinyGPT(\n",
    "        vocab_size=vocab_size, block_size=cfg.block_size,\n",
    "        n_layer=2, n_embd=128, n_heads=4, dropout=0.0,\n",
    "    ).to(device).to(cfg.param_dtype)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scaler = GradScaler(enabled=True) if (cfg.use_grad_scaler and device.type == \"cuda\") else None\n",
    "\n",
    "    logs = {\n",
    "        \"step\": [], \"train_loss\": [], \"grad_norm\": [], \"zero_grad_frac\": [],\n",
    "        \"step_time_ms\": [], \"tokens_per_s\": [], \"scale\": [],\n",
    "        \"cuda_mem_mb\": [], \"val_step\": [], \"val_loss\": [],\n",
    "    }\n",
    "    status = \"ok\"\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    tokens_per_step = cfg.batch_size * cfg.block_size\n",
    "\n",
    "    for step in range(cfg.steps):\n",
    "        t0 = time.perf_counter()\n",
    "        x, y = get_batch(\"train\", cfg.batch_size, cfg.block_size)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with amp_autocast(device, cfg.amp_dtype, enabled=cfg.use_autocast):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            status = \"non_finite_loss\"\n",
    "            break\n",
    "\n",
    "        if scaler is None:\n",
    "            loss.backward()\n",
    "            grad_norm = global_grad_norm(model)\n",
    "            zero_frac = global_zero_grad_frac(model)\n",
    "            optimizer.step()\n",
    "            scale_val = float(\"nan\")\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = global_grad_norm(model)\n",
    "            zero_frac = global_zero_grad_frac(model)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scale_val = float(scaler.get_scale())\n",
    "\n",
    "        dt = max(time.perf_counter() - t0, 1e-12)\n",
    "        logs[\"step\"].append(step)\n",
    "        logs[\"train_loss\"].append(float(loss))\n",
    "        logs[\"grad_norm\"].append(float(grad_norm))\n",
    "        logs[\"zero_grad_frac\"].append(float(zero_frac))\n",
    "        logs[\"step_time_ms\"].append(dt * 1000)\n",
    "        logs[\"tokens_per_s\"].append(tokens_per_step / dt)\n",
    "        logs[\"scale\"].append(scale_val)\n",
    "        logs[\"cuda_mem_mb\"].append(torch.cuda.max_memory_allocated() / 1024**2 if device.type == \"cuda\" else float(\"nan\"))\n",
    "\n",
    "        if cfg.eval_interval and (step % cfg.eval_interval == 0 or step == cfg.steps - 1):\n",
    "            try:\n",
    "                ev = estimate_loss(\n",
    "                    model,\n",
    "                    cfg.block_size,\n",
    "                    cfg.batch_size,\n",
    "                    cfg.eval_iters,\n",
    "                    use_autocast=cfg.use_autocast,\n",
    "                    amp_dtype=cfg.amp_dtype,\n",
    "                )\n",
    "                logs[\"val_step\"].append(step)\n",
    "                logs[\"val_loss\"].append(ev[\"val\"])\n",
    "            except Exception:\n",
    "                logs[\"val_step\"].append(step)\n",
    "                logs[\"val_loss\"].append(float(\"nan\"))\n",
    "\n",
    "    return {\"config\": cfg, \"status\": status, \"logs\": logs}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define experiment suite\n",
    "\n",
    "FAST_DEV_RUN = (device.type != \"cuda\")\n",
    "BASE_STEPS = 60 if FAST_DEV_RUN else 300\n",
    "\n",
    "suite = [\n",
    "    TrainConfig(name=\"fp32\", steps=BASE_STEPS, param_dtype=torch.float32),\n",
    "]\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"fp16_naive\", steps=BASE_STEPS,\n",
    "        param_dtype=torch.float16,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"bf16_naive\", steps=BASE_STEPS,\n",
    "            param_dtype=torch.bfloat16,\n",
    "        ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16_no_scaler\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=False, param_dtype=torch.float32,\n",
    "    ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=True, param_dtype=torch.float32,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_bf16\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "            param_dtype=torch.float32,\n",
    "        ))\n",
    "elif device.type == \"cpu\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_bf16_cpu\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "        param_dtype=torch.float32,\n",
    "    ))\n",
    "\n",
    "print(\"Planned experiments:\")\n",
    "for cfg in suite:\n",
    "    print(f\"  {cfg.name}: params={cfg.param_dtype}, autocast={cfg.use_autocast} {cfg.amp_dtype}, scaler={cfg.use_grad_scaler}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run all experiments\n",
    "\n",
    "results = []\n",
    "for cfg in suite:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running: {cfg.name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    res = train_one(cfg)\n",
    "    print(f\"  Status: {res['status']}, Steps: {len(res['logs']['step'])}\")\n",
    "    if res['logs']['train_loss']:\n",
    "        print(f\"  Final train loss: {res['logs']['train_loss'][-1]:.4f}\")\n",
    "    results.append(res)\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    cfg, logs = r[\"config\"], r[\"logs\"]\n",
    "    n = len(logs[\"step\"])\n",
    "    summary_rows.append({\n",
    "        \"name\": cfg.name,\n",
    "        \"status\": r[\"status\"],\n",
    "        \"steps\": n,\n",
    "        \"final_train_loss\": f\"{logs['train_loss'][-1]:.4f}\" if n else \"n/a\",\n",
    "        \"final_val_loss\": f\"{logs['val_loss'][-1]:.4f}\" if logs[\"val_loss\"] else \"n/a\",\n",
    "        \"mean_step_ms\": f\"{np.mean(logs['step_time_ms']):.1f}\" if n else \"n/a\",\n",
    "        \"mean_tok/s\": f\"{np.mean(logs['tokens_per_s']):.0f}\" if n else \"n/a\",\n",
    "        \"peak_cuda_MB\": f\"{np.nanmax(logs['cuda_mem_mb']):.1f}\" if device.type == \"cuda\" and n else \"n/a\",\n",
    "    })\n",
    "\n",
    "print(\"\\n\\n=== Summary ===\")\n",
    "display(pd.DataFrame(summary_rows))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Training + Validation loss curves\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    suffix = f\" ({r['status']})\" if r[\"status\"] != \"ok\" else \"\"\n",
    "    if logs[\"train_loss\"]:\n",
    "        axes[0].plot(logs[\"step\"], logs[\"train_loss\"], label=f\"{name}{suffix}\", alpha=0.8)\n",
    "    if logs[\"val_loss\"]:\n",
    "        axes[1].plot(logs[\"val_step\"], logs[\"val_loss\"], marker=\"o\", ms=4, label=f\"{name}{suffix}\", alpha=0.8)\n",
    "\n",
    "axes[0].set_title(\"Training loss vs step\")\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"cross-entropy loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"Validation loss (periodic eval)\")\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"cross-entropy loss\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Step time + throughput\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    axes[0].plot(logs[\"step\"], logs[\"step_time_ms\"], label=name, alpha=0.7)\n",
    "    axes[1].plot(logs[\"step\"], logs[\"tokens_per_s\"], label=name, alpha=0.7)\n",
    "\n",
    "axes[0].set_title(\"Step time (ms)\")\n",
    "axes[0].set_xlabel(\"step\"); axes[0].set_ylabel(\"ms\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"Throughput (tokens/s)\")\n",
    "axes[1].set_xlabel(\"step\"); axes[1].set_ylabel(\"tokens/s\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Gradient norms across precision regimes\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], logs[\"grad_norm\"], label=name, alpha=0.7)\n",
    "\n",
    "plt.title(\"Gradient L2 norm vs step (after unscaling)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"||grad||_2\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Exact-zero gradient fraction (a proxy for underflow / dead signal)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    if \"zero_grad_frac\" not in logs:\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], logs[\"zero_grad_frac\"], label=name, alpha=0.75)\n",
    "\n",
    "plt.title(\"Fraction of gradients that are exactly 0\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"zero_grad_frac\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: GradScaler dynamic scale (FP16 AMP only)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plotted = False\n",
    "for r in results:\n",
    "    if not r[\"config\"].use_grad_scaler:\n",
    "        continue\n",
    "    logs = r[\"logs\"]\n",
    "    scale = np.array(logs[\"scale\"], dtype=np.float64)\n",
    "    if len(scale) == 0 or np.all(np.isnan(scale)):\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], scale, label=r[\"config\"].name)\n",
    "    plotted = True\n",
    "\n",
    "if plotted:\n",
    "    plt.title(\"GradScaler dynamic scale factor\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"scale\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No GradScaler data to plot (did amp_fp16 run?)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: CUDA peak memory\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    for r in results:\n",
    "        logs = r[\"logs\"]\n",
    "        if not logs[\"step\"]:\n",
    "            continue\n",
    "        ax.plot(logs[\"step\"], logs[\"cuda_mem_mb\"], label=r[\"config\"].name, alpha=0.7)\n",
    "    ax.set_title(\"Peak CUDA memory allocated (MB)\")\n",
    "    ax.set_xlabel(\"step\"); ax.set_ylabel(\"MB\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"CUDA not available; skipping memory plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary bar charts: compare all experiments at a glance\n",
    "\n",
    "completed = [r for r in results if r[\"status\"] == \"ok\" and r[\"logs\"][\"train_loss\"]]\n",
    "\n",
    "if len(completed) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    names = [r[\"config\"].name for r in completed]\n",
    "    x_pos = np.arange(len(names))\n",
    "\n",
    "    # 1. Final training loss\n",
    "    final_losses = [r[\"logs\"][\"train_loss\"][-1] for r in completed]\n",
    "    colors = []\n",
    "    for r in completed:\n",
    "        if \"naive\" in r[\"config\"].name and \"bf16\" in r[\"config\"].name:\n",
    "            colors.append(\"C5\")\n",
    "        elif \"naive\" in r[\"config\"].name:\n",
    "            colors.append(\"C3\")\n",
    "        elif \"amp\" in r[\"config\"].name and \"bf16\" in r[\"config\"].name:\n",
    "            colors.append(\"C1\")\n",
    "        elif \"amp\" in r[\"config\"].name:\n",
    "            colors.append(\"C4\")\n",
    "        elif \"fp32\" in r[\"config\"].name:\n",
    "            colors.append(\"C0\")\n",
    "        else:\n",
    "            colors.append(\"C7\")\n",
    "\n",
    "    axes[0].bar(x_pos, final_losses, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[0].set_ylabel(\"Final train loss\")\n",
    "    axes[0].set_title(\"Final Training Loss (lower is better)\")\n",
    "\n",
    "    # 2. Mean step time\n",
    "    mean_times = [np.mean(r[\"logs\"][\"step_time_ms\"]) for r in completed]\n",
    "    axes[1].bar(x_pos, mean_times, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[1].set_ylabel(\"Mean step time (ms)\")\n",
    "    axes[1].set_title(\"Training Speed (lower is better)\")\n",
    "\n",
    "    # 3. Peak CUDA memory\n",
    "    if device.type == \"cuda\":\n",
    "        peak_mem = [np.nanmax(r[\"logs\"][\"cuda_mem_mb\"]) for r in completed]\n",
    "        axes[2].bar(x_pos, peak_mem, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "        axes[2].set_xticks(x_pos)\n",
    "        axes[2].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "        axes[2].set_ylabel(\"Peak memory (MB)\")\n",
    "        axes[2].set_title(\"Peak GPU Memory (lower is better)\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"CUDA memory\\nnot available\", transform=axes[2].transAxes,\n",
    "                     ha=\"center\", va=\"center\", fontsize=12, color=\"gray\")\n",
    "        axes[2].set_title(\"Peak GPU Memory\")\n",
    "\n",
    "    fig.suptitle(\"Experiment Summary: Loss, Speed, and Memory across Precision Regimes\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Need at least 2 completed experiments for summary charts.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Memory breakdown comparison\n",
    "\n",
    "Mixed precision reduces activation memory (which scales with batch size and sequence length) but still requires FP32 for optimizer state and master weights. Here's a rough breakdown for our TinyGPT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Memory breakdown analysis\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model_tmp = TinyGPT(vocab_size=vocab_size, block_size=64, n_layer=2, n_embd=128, n_heads=4)\n",
    "n_params = count_params(model_tmp)\n",
    "del model_tmp\n",
    "\n",
    "rows = []\n",
    "for name, param_bytes, grad_bytes, optstate_bytes_per_param, notes in [\n",
    "    (\"Pure FP32\",       4, 4, 8, \"Adam: m(4B) + v(4B)\"),\n",
    "    (\"Pure FP16\",       2, 2, 4, \"Adam: m(2B) + v(2B) — UNSTABLE\"),\n",
    "    (\"AMP FP16 (std)\",  4, 4, 8, \"Params+grads+opt in FP32; compute in FP16\"),\n",
    "    (\"AMP FP16 (with master wts)\", 6, 2, 8, \"FP16 params(2B) + FP32 master(4B)\"),\n",
    "    (\"AMP BF16 (std)\",  4, 4, 8, \"Params+grads+opt in FP32; compute in BF16\"),\n",
    "]:\n",
    "    param_mb = n_params * param_bytes / 1024**2\n",
    "    grad_mb = n_params * grad_bytes / 1024**2\n",
    "    opt_mb = n_params * optstate_bytes_per_param / 1024**2\n",
    "    total_mb = param_mb + grad_mb + opt_mb\n",
    "    rows.append({\n",
    "        \"config\": name,\n",
    "        \"param_memory_MB\": f\"{param_mb:.1f}\",\n",
    "        \"grad_memory_MB\": f\"{grad_mb:.1f}\",\n",
    "        \"optimizer_state_MB\": f\"{opt_mb:.1f}\",\n",
    "        \"total_fixed_MB\": f\"{total_mb:.1f}\",\n",
    "        \"notes\": notes,\n",
    "    })\n",
    "\n",
    "print(f\"TinyGPT parameters: {n_params:,}\")\n",
    "print(f\"(Real LLM savings come from activations, which scale with batch*seq_len)\\n\")\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "# Visualize memory breakdown as stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "x = np.arange(len(rows))\n",
    "w = 0.5\n",
    "names_m = [r[\"config\"] for r in rows]\n",
    "param_mb = [float(r[\"param_memory_MB\"]) for r in rows]\n",
    "grad_mb = [float(r[\"grad_memory_MB\"]) for r in rows]\n",
    "opt_mb = [float(r[\"optimizer_state_MB\"]) for r in rows]\n",
    "\n",
    "ax.bar(x, param_mb, w, label=\"Parameters\", color=\"C0\", alpha=0.8)\n",
    "ax.bar(x, grad_mb, w, bottom=param_mb, label=\"Gradients\", color=\"C1\", alpha=0.8)\n",
    "ax.bar(x, opt_mb, w, bottom=[p+g for p,g in zip(param_mb, grad_mb)], label=\"Optimizer state\", color=\"C2\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names_m, rotation=25, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Memory (MB)\")\n",
    "ax.set_title(f\"Fixed memory breakdown by precision config ({n_params:,} params)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nNote: This shows only fixed-size memory (params + grads + optimizer).\")\n",
    "print(\"Activations (which scale with batch×seq_len) are the real memory win for AMP.\")\n",
    "print(\"For large models, activation memory often exceeds parameter memory by 5-10x.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Interpreting results\n",
    "\n",
    "### Loss curves\n",
    "- **FP32**: should decrease smoothly. This is your reference.\n",
    "- **FP16 naive**: may diverge, stagnate, or produce NaN. This demonstrates why casting everything to half precision is not safe.\n",
    "- **BF16 naive**: often converges to a similar loss as FP32. This is the dramatic proof that BF16's range (8-bit exponent) matters more than FP16's precision (10-bit mantissa) for training stability. The network tolerates noisy gradient values; it cannot tolerate zero gradient values.\n",
    "- **AMP FP16 (no scaler)**: may work but may show more zero gradients than the scaled version. This shows that autocast alone helps (per-op policy keeps sensitive ops in FP32) but doesn't fully solve the backward-pass underflow problem.\n",
    "- **AMP FP16 (with scaler)**: should be stable. The GradScaler rescues gradients from underflow.\n",
    "- **AMP BF16**: typically matches FP32 quality without needing a scaler.\n",
    "\n",
    "### Step time\n",
    "- On modern GPUs, AMP often reduces step time because matmuls hit Tensor Cores.\n",
    "- If you don't see speedup: model may be too small (overhead dominates), or you're CPU-bound.\n",
    "- The smallest models sometimes see AMP *overhead* because the per-op dispatch cost exceeds the Tensor Core gains. This goes away at realistic model sizes.\n",
    "\n",
    "### Gradient norms\n",
    "- Should be comparable across regimes for a well-behaved model.\n",
    "- If FP16 naive shows erratic norms, that's the underflow/overflow instability.\n",
    "- BF16 naive norms should be close to FP32 (same gradient magnitudes, just represented with fewer mantissa bits).\n",
    "\n",
    "### Zero gradients\n",
    "- A high `zero_grad_frac` indicates underflow or dead signal (exact zeros).\n",
    "- **FP16 naive** and **AMP FP16 no scaler** may show elevated zero-gradient fractions.\n",
    "- **BF16 naive** should show low zero-gradient fractions (gradients don't underflow with 8-bit exponent).\n",
    "- The difference between FP16+scaler and FP16 without scaler directly measures the benefit of loss scaling.\n",
    "\n",
    "### GradScaler scale\n",
    "- If scale drops repeatedly, the model is hitting overflow events and GradScaler is skipping steps.\n",
    "- If scale grows steadily, training is stable and GradScaler is increasing headroom.\n",
    "- A \"spiky\" pattern (rapid drops followed by slow climbs) is normal and expected.\n",
    "\n",
    "### Memory\n",
    "- AMP typically uses ~same or slightly more memory than FP32 for parameters+optimizer (due to master weights), but saves on activations.\n",
    "- Naive 16-bit uses less total memory but trades stability.\n",
    "- The savings become dramatic at larger batch sizes and sequence lengths.\n",
    "- At LLM scale with Adam: 16 bytes/param (mixed precision) vs 16 bytes/param (FP32). The *activation* memory savings are where AMP really wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Practical checklist\n",
    "\n",
    "### Defaults that usually work\n",
    "- Prefer **BF16 autocast** if your GPU supports it (Ampere+). No GradScaler needed.\n",
    "- Otherwise use **FP16 autocast + GradScaler**.\n",
    "- Keep optimizer state in FP32 (default for most PyTorch optimizers).\n",
    "\n",
    "### When things go wrong\n",
    "1. **Loss becomes `nan`/`inf`:** Check for overflow sources (attention logits, exp/log, unstable loss). Consider lowering learning rate or adding gradient clipping.\n",
    "2. **Gradients mostly zero in FP16:** Use GradScaler with higher initial scale. Consider switching to BF16.\n",
    "3. **Training \"does nothing\":** Check for weight update stagnation — are weights actually changing? Ensure you have FP32 master weights (standard when model is FP32 + autocast).\n",
    "4. **Unexplained dtype behavior:** Use dtype hooks (Section 3.3) to confirm what's running in what dtype.\n",
    "\n",
    "### Common gotchas\n",
    "- Mixing manual `.half()` casts with autocast can lead to unexpected behavior.\n",
    "- Gradient clipping should happen **after** `scaler.unscale_(optimizer)`.\n",
    "- `autocast` should cover forward + loss, but **not** the optimizer step.\n",
    "- On Ampere+ GPUs, your \"FP32 baseline\" may use TF32 for matmuls. Check `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix — References\n",
    "\n",
    "## Papers\n",
    "\n",
    "| Paper | Year | Key contribution |\n",
    "|---|---|---|\n",
    "| Gupta et al., *Deep Learning with Limited Numerical Precision* | 2015 | Low-precision training needs deliberate rounding/scaling; stochastic rounding intuition |\n",
    "| Micikevicius et al., *Mixed Precision Training* | 2017 | FP32 master weights + loss scaling + per-op policies |\n",
    "| Kalamkar et al., *A Study of BFLOAT16 for Deep Learning Training* | 2019 | BF16 empirical validation, no loss scaling needed |\n",
    "| Rajbhandari et al., *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models* | 2020 | Memory breakdown of mixed-precision training |\n",
    "| NVIDIA (and others), *FP8 Formats for Deep Learning* | 2022 | FP8 formats (E4M3/E5M2), scaling metadata, and accumulation strategies |\n",
    "| Dettmers et al., *8-bit Optimizers via Block-wise Quantization* | 2022 | Reducing optimizer-state memory beyond AMP |\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- [PyTorch `torch.amp` docs](https://pytorch.org/docs/stable/amp.html) — autocast op reference, GradScaler API\n",
    "- [PyTorch AMP examples](https://pytorch.org/docs/stable/notes/amp_examples.html) — canonical training loop\n",
    "- [NVIDIA mixed precision blog](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n",
    "- [Google BF16 blog](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    "- [DeepSpeed config docs](https://www.deepspeed.ai/docs/config-json/) — FP16/BF16 training config\n",
    "- [PyTorch FSDP mixed precision](https://pytorch.org/docs/stable/fsdp.html)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's experiments are intentionally small; the *mechanisms* are the same at scale."
   ]
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocast / AMP in PyTorch: A Deep Practical Reference\n",
    "\n",
    "Every neural network is fundamentally a giant mathematical expression, and training is the process of tuning the constants (weights) so that expression approximates reality. We do this by computing a loss, backpropagating to get gradients, and nudging the weights a tiny bit in the right direction. Repeat billions of times.\n",
    "\n",
    "Why do we care about this *now*? Because modern deep learning training is GPU-dominated and compute-hungry. Matrix multiplications are embarrassingly parallel, so GPUs can run thousands of tiny operations at once — but as models grow, the cost of training keeps rising. Mixed precision training is one of the highest-leverage efficiency techniques we have: it uses fast low-precision hardware (Tensor Cores / specialized units) where it's safe, while preserving FP32 where training would otherwise become numerically unstable.\n",
    "\n",
    "But here's the thing: those weights and gradients are stored as **floating-point numbers**. And floating-point numbers are *not* real numbers. They are a finite approximation with a specific *resolution*. That resolution depends on the format — FP32, FP16, BF16, TF32, FP8 — and each format makes a different tradeoff between **range** (how large and small the numbers can get) and **precision** (how fine the steps between consecutive representable values are).\n",
    "\n",
    "This matters because during training, we routinely deal with numbers spanning many orders of magnitude: weights around $1$ ($10^0$), gradients as small as $10^{-8}$, attention logits that can spike to $10^3$, and accumulations over thousands of values. A format that can't represent tiny gradients kills learning (they become zero). A format that can't represent large attention logits kills stability (they become infinity). A format that rounds too aggressively corrupts the running statistics that normalization layers depend on.\n",
    "\n",
    "The fundamental question this notebook answers is: **when can we get away with lower resolution, and when does it break training?**\n",
    "\n",
    "Autocast (AMP) is PyTorch's answer: a per-operation precision policy that routes each computation to the right format. Some ops get the speed of 16-bit. Some ops get the safety of 32-bit. The combined effect is faster training with (nearly) no loss in quality. But to understand *when* and *why* AMP works — and to debug it when it doesn't — you need to understand what floating-point formats can and cannot represent.\n",
    "\n",
    "That's what this notebook is for.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is organized into **three sections**:\n",
    "\n",
    "| # | Section | What you get |\n",
    "|---|---|---|\n",
    "| **1** | **Theory** | Floating-point range vs precision, FP16/BF16/FP32/TF32 comparison tables, underflow/overflow, what AMP is really doing |\n",
    "| **2** | **What the literature says** | Paper-driven mental models (Micikevicius et al., Kalamkar et al., ZeRO, NVIDIA guidance), written explanations — *no experiments* |\n",
    "| **3** | **Practicalities** | Hands-on experiments + graphs: progressive mixed-precision implementation from scratch, operator policy probes, dtype flow through a transformer, loss/gradient/scale curves under different precision regimes |\n",
    "\n",
    "---\n",
    "\n",
    "**After completing this notebook you should be able to answer (and debug) questions like:**\n",
    "\n",
    "- Why does FP16 often need **loss scaling**, but BF16 often does not?\n",
    "- What does `autocast` *actually* do per operation (matmul vs softmax vs layernorm)?\n",
    "- Why do people talk about **FP32 master weights** and **optimizer state precision**?\n",
    "- Why does adding `1e-4` to `1.0` in FP16 produce exactly `1.0`? (with the exact bit-level explanation)\n",
    "- How can you *see* autocast happening inside a transformer forward pass?\n",
    "- What fails if you try to \"just train in half precision everywhere\"?\n",
    "- Why does naive BF16 often work where naive FP16 fails?\n",
    "- What is the \"sum vs mean\" mystery under autocast?\n",
    "- How does the gradient distribution relate to FP16's representable range?\n",
    "- How many bytes per parameter does mixed-precision Adam training actually use?\n",
    "- Why is floating-point addition NOT associative, and why does that matter for GPU reductions?\n",
    "- What is catastrophic cancellation, and why does it make LayerNorm precision-sensitive?\n",
    "- How does stochastic rounding differ from deterministic rounding, and when does it help?\n",
    "- Which layers in a transformer are most affected by autocast precision changes?\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "- Read the markdown, then run the code cells.\n",
    "- Most experiments are designed to run in a few minutes on a single GPU.\n",
    "- CPU-only runs are supported for the *conceptual* demos, but some mixed-precision behaviors (and speedups) are fundamentally GPU-driven.\n",
    "\n",
    "### If you're in a hurry (recommended run path)\n",
    "\n",
    "- **10 minutes:** run setup → Quick Reference table → **3.1 progressive mixed precision** plots\n",
    "- **30–60 minutes (GPU):** add **3.6 TinyGPT training suite** (loss/time/memory graphs)\n",
    "- **Deep dive:** run everything in order; each experiment builds on the previous mental model\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "**Section 1 — Theory**\n",
    "- Quick reference cheat sheet (the table you should memorize)\n",
    "- Floating-point: range vs precision, IEEE 754 anatomy\n",
    "- Number line visualization: where representable floats live\n",
    "- FP16 vs BF16 vs FP32 vs TF32 (tables you can trust)\n",
    "- The bit-level addition trap (why `1 + 1e-4 = 1` in FP16) — with step-by-step binary alignment\n",
    "- Underflow, overflow, accumulation error\n",
    "- **Non-associativity**: why summation order matters in low precision\n",
    "- **Catastrophic cancellation**: why LayerNorm/BatchNorm need FP32\n",
    "- **Kahan summation**: the compensated-sum fix\n",
    "- **TF32**: the format you're using without knowing it\n",
    "- **Tensor Core mechanics**: why mixed precision is fast (tile-based MMA, dimension alignment)\n",
    "- What AMP is (autocast + grad scaling)\n",
    "- Master weights, optimizer state, and accumulation\n",
    "\n",
    "**Section 2 — What the literature says**\n",
    "- Micikevicius et al. — *Mixed Precision Training*\n",
    "- Kalamkar et al. — *A Study of BFLOAT16 for Deep Learning Training*\n",
    "- NVIDIA mixed precision guidance\n",
    "- BF16 design intent\n",
    "- PyTorch AMP operator policy\n",
    "- PyTorch AMP examples: accumulation, penalties, multi-optimizer patterns\n",
    "- `torch.amp` API modernization + backend portability\n",
    "- Thread-local autocast semantics + nested precision regions\n",
    "- **Stochastic rounding**: why it helps low-precision training (Gupta et al.)\n",
    "- Rajbhandari et al. — ZeRO and optimizer state precision\n",
    "- LLM training stacks (FSDP/ZeRO) and where AMP fits\n",
    "- FP8 in practice: row-wise scaling + float8 all-gather (FSDP2)\n",
    "- Blackwell-era microscaling (MXFP8 / NVFP4)\n",
    "- FP8 and 8-bit optimizers\n",
    "\n",
    "**Section 3 — Practicalities**\n",
    "- Progressive mixed-precision implementation from scratch (FP32 → naive FP16 → **naive BF16** → master weights → loss scaling → PyTorch AMP)\n",
    "- Build an operator policy table *from your local PyTorch*\n",
    "- The \"sum vs mean\" mystery\n",
    "- **Per-layer precision toy examples**: attention, activations, FFN — why autocast treats each differently\n",
    "- **Numerical stability stress tests**: adversarial inputs for softmax, LayerNorm, cross-entropy\n",
    "- Visualize dtype flow through a transformer (4 configurations)\n",
    "- **Autocast boundary flow diagram**: visual dtype transitions per module across configs\n",
    "- **Per-layer precision sensitivity**: which parts of the transformer hurt most? (forward + backward)\n",
    "- **Per-layer gradient sensitivity**: which layers corrupt gradients the most in FP16?\n",
    "- Gradient underflow + the effect of loss scaling\n",
    "- **Micikevicius-style gradient histogram analysis**\n",
    "- **Gradient distribution evolution**: how the histogram shifts during training + loss scaling effect\n",
    "- Weight update stagnation\n",
    "- Train a tiny causal LM under different precision regimes (FP32, FP16 naive, BF16 naive, AMP FP16, AMP BF16)\n",
    "- **OPT-125M training loss curves**: real-world AMP on a 125M-parameter Hugging Face model (6 configs)\n",
    "- Plot and interpret loss/time/scale/gradient curves + summary bar charts\n",
    "\n",
    "---\n",
    "\n",
    "## Quick glossary\n",
    "\n",
    "| Term | Meaning |\n",
    "|---|---|\n",
    "| **AMP** | Automatic Mixed Precision (in PyTorch: `torch.amp`) |\n",
    "| **autocast** | Context manager that applies a per-operation dtype policy |\n",
    "| **GradScaler / loss scaling** | Rescales loss to avoid FP16 gradient underflow |\n",
    "| **master weights** | Keep weights in FP32 for updates, cast for compute |\n",
    "| **underflow** | Magnitude too small → becomes 0 (or subnormal/denormal) |\n",
    "| **overflow** | Magnitude too large → becomes `inf` |\n",
    "| **TF32** | TensorFloat-32 — an NVIDIA format with FP32 range but 10-bit mantissa, used transparently in Ampere+ matmuls |\n",
    "| **ULP** | Unit in the Last Place — the spacing between adjacent representable floats |\n",
    "| **epsilon** | Smallest number such that `1.0 + eps > 1.0` in a given format |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need:\n",
    "- Python 3.10+\n",
    "- PyTorch 2.x\n",
    "- `matplotlib`, `numpy`, `pandas`, `tqdm`\n",
    "- (optional) `transformers` + `safetensors` (only for the OPT-125M dtype-tracing section)\n",
    "\n",
    "### Install (CPU-only quick start)\n",
    "```bash\n",
    "pip install torch numpy pandas matplotlib tqdm\n",
    "```\n",
    "\n",
    "### Optional (to run the OPT-125M dtype-tracing section)\n",
    "```bash\n",
    "pip install transformers safetensors\n",
    "```\n",
    "\n",
    "### Install (CUDA)\n",
    "Install the correct PyTorch + CUDA build from the [official PyTorch instructions](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is written to *degrade gracefully*:\n",
    "- If BF16 is not supported on your GPU, BF16 experiments will be skipped.\n",
    "- If FP16 training without scaling explodes (often does), we record that as a result rather than pretending it \"worked\".\n",
    "\n",
    "> **Note:** on Apple Silicon, this notebook defaults to **CPU** for maximum compatibility. If you want to try the Apple GPU backend, set `USE_MPS_IF_AVAILABLE = True` in the first code cell. AMP/autocast behavior is best-defined on CUDA; CPU/MPS support exists but has different operator coverage and performance characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on editing:** `amp.ipynb` is generated by `_build_notebook.py`.\n",
    ">\n",
    "> If you want to change the content, edit `_build_notebook.py` and regenerate:\n",
    ">\n",
    "> ```bash\n",
    "> python3 _build_notebook.py\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core imports + environment report\n",
    "import os, math, time, random, struct, platform\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: numpy. Install with: pip install numpy\") from e\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: pandas. Install with: pip install pandas\") from e\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.ticker as ticker\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: matplotlib. Install with: pip install matplotlib\") from e\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: tqdm. Install with: pip install tqdm\") from e\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 100,\n",
    "})\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"PyTorch is required. CPU-only: pip install torch\"\n",
    "    ) from e\n",
    "\n",
    "# Prefer torch.amp (newer API)\n",
    "if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\"):\n",
    "    autocast = torch.amp.autocast\n",
    "    GradScaler = torch.amp.GradScaler\n",
    "else:\n",
    "    autocast = torch.cuda.amp.autocast\n",
    "    GradScaler = torch.cuda.amp.GradScaler\n",
    "\n",
    "def amp_autocast(dev: torch.device, dtype: torch.dtype | None, enabled: bool = True, cache_enabled: bool = True):\n",
    "    # Best-effort autocast context manager that degrades gracefully.\n",
    "    if (not enabled) or (dtype is None):\n",
    "        return nullcontext()\n",
    "    try:\n",
    "        return autocast(device_type=dev.type, dtype=dtype, enabled=True, cache_enabled=cache_enabled)\n",
    "    except TypeError:\n",
    "        # Older signatures may not support cache_enabled.\n",
    "        try:\n",
    "            return autocast(device_type=dev.type, dtype=dtype, enabled=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "            return nullcontext()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "        return nullcontext()\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# ---- User knobs -------------------------------------------------------------\n",
    "# Leave as-is for \"works everywhere\" defaults; tweak if you have specific HW.\n",
    "PREFERRED_DEVICE = None   # one of: \"cuda\", \"mps\", \"cpu\" (or None for auto)\n",
    "USE_MPS_IF_AVAILABLE = False  # Apple Silicon: set True to try MPS when no CUDA\n",
    "\n",
    "def choose_device():\n",
    "    if PREFERRED_DEVICE is not None:\n",
    "        pref = str(PREFERRED_DEVICE).lower()\n",
    "        if pref == \"cuda\" and torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        if pref == \"mps\" and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        if pref == \"cpu\":\n",
    "            return torch.device(\"cpu\")\n",
    "        print(f\"[warn] Requested device '{PREFERRED_DEVICE}' not available; falling back to auto.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if USE_MPS_IF_AVAILABLE and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = choose_device()\n",
    "\n",
    "def supports_dtype_on_device(dtype: torch.dtype, dev: torch.device) -> bool:\n",
    "    try:\n",
    "        torch.tensor([0.0], device=dev, dtype=dtype)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"Python  {platform.python_version()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA:   {torch.version.cuda}\")\n",
    "    print(f\"GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"BF16:   {'supported' if torch.cuda.is_bf16_supported() else 'NOT supported'}\")\n",
    "elif device.type == \"mps\":\n",
    "    print(\"MPS:    available (Apple Silicon)\")\n",
    "    print(f\"FP16:   {'supported' if supports_dtype_on_device(torch.float16, device) else 'NOT supported'}\")\n",
    "    print(f\"BF16:   {'supported' if supports_dtype_on_device(torch.bfloat16, device) else 'NOT supported'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Floating-Point Formats for Deep Learning\n",
    "\n",
    "This is the table you should memorize. Every design decision in AMP traces back to these numbers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate the cheat sheet from your local PyTorch (so the numbers are trustworthy)\n",
    "\n",
    "def _ulp_at_one(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "format_specs = [\n",
    "    # FP8 rows are included even if float8 dtypes are unavailable at runtime.\n",
    "    {\n",
    "        \"name\": \"FP8 (E4M3)\",\n",
    "        \"dtype\": getattr(torch, \"float8_e4m3fn\", None),\n",
    "        \"exp_bits\": 4,\n",
    "        \"mant_bits\": 3,\n",
    "        \"fallback\": {\n",
    "            \"bits\": 8,\n",
    "            \"eps\": \"1.25e-01\",\n",
    "            \"ulp1\": \"1.25e-01\",\n",
    "            \"tiny\": \"1.56e-02\",\n",
    "            \"sub\": \"1.95e-03\",\n",
    "            \"max\": \"4.48e+02\",\n",
    "            \"exp_range\": \"[-6, 7]\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FP8 (E5M2)\",\n",
    "        \"dtype\": getattr(torch, \"float8_e5m2\", None),\n",
    "        \"exp_bits\": 5,\n",
    "        \"mant_bits\": 2,\n",
    "        \"fallback\": {\n",
    "            \"bits\": 8,\n",
    "            \"eps\": \"2.50e-01\",\n",
    "            \"ulp1\": \"2.50e-01\",\n",
    "            \"tiny\": \"6.10e-05\",\n",
    "            \"sub\": \"1.53e-05\",\n",
    "            \"max\": \"5.73e+04\",\n",
    "            \"exp_range\": \"[-14, 15]\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FP16 (IEEE half)\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"exp_bits\": 5,\n",
    "        \"mant_bits\": 10,\n",
    "        \"fallback\": None,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BF16 (brain float)\",\n",
    "        \"dtype\": torch.bfloat16,\n",
    "        \"exp_bits\": 8,\n",
    "        \"mant_bits\": 7,\n",
    "        \"fallback\": None,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FP32 (single)\",\n",
    "        \"dtype\": torch.float32,\n",
    "        \"exp_bits\": 8,\n",
    "        \"mant_bits\": 23,\n",
    "        \"fallback\": None,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TF32 (tensor float)\",\n",
    "        \"dtype\": None,\n",
    "        \"exp_bits\": 8,\n",
    "        \"mant_bits\": 10,\n",
    "        \"fallback\": {\n",
    "            \"bits\": \"19*\",\n",
    "            \"eps\": \"9.77e-04\",\n",
    "            \"ulp1\": \"9.77e-04\",\n",
    "            \"tiny\": \"1.18e-38\",\n",
    "            \"sub\": \"n/a (internal)\",\n",
    "            \"max\": \"3.40e+38\",\n",
    "            \"exp_range\": \"[-126, 127]\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FP64 (double)\",\n",
    "        \"dtype\": torch.float64,\n",
    "        \"exp_bits\": 11,\n",
    "        \"mant_bits\": 52,\n",
    "        \"fallback\": None,\n",
    "    },\n",
    "]\n",
    "\n",
    "for spec in format_specs:\n",
    "    name = spec[\"name\"]\n",
    "    dt = spec[\"dtype\"]\n",
    "    exp_b = spec[\"exp_bits\"]\n",
    "    mant_b = spec[\"mant_bits\"]\n",
    "    fb = spec[\"fallback\"]\n",
    "\n",
    "    runtime_available = False\n",
    "    if name.startswith(\"TF32\"):\n",
    "        runtime_available = (device.type == \"cuda\")\n",
    "\n",
    "    fi = None\n",
    "    if dt is not None:\n",
    "        try:\n",
    "            fi = torch.finfo(dt)\n",
    "            runtime_available = True\n",
    "        except Exception:\n",
    "            fi = None\n",
    "\n",
    "    if fi is not None:\n",
    "        ulp1 = _ulp_at_one(dt)\n",
    "        sub = _smallest_subnormal(dt)\n",
    "        bits = fi.bits\n",
    "        exp_range = f\"[{-2**(exp_b-1)+2}, {2**(exp_b-1)-1}]\"\n",
    "        eps_s = f\"{fi.eps:.2e}\"\n",
    "        ulp_s = f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\"\n",
    "        tiny_s = f\"{fi.tiny:.2e}\"\n",
    "        sub_s = f\"{sub:.2e}\" if sub is not None else \"n/a\"\n",
    "        max_s = f\"{fi.max:.2e}\"\n",
    "    else:\n",
    "        bits = fb[\"bits\"] if fb is not None else \"n/a\"\n",
    "        exp_range = fb[\"exp_range\"] if fb is not None else \"n/a\"\n",
    "        eps_s = fb[\"eps\"] if fb is not None else \"n/a\"\n",
    "        ulp_s = fb[\"ulp1\"] if fb is not None else \"n/a\"\n",
    "        tiny_s = fb[\"tiny\"] if fb is not None else \"n/a\"\n",
    "        sub_s = fb[\"sub\"] if fb is not None else \"n/a\"\n",
    "        max_s = fb[\"max\"] if fb is not None else \"n/a\"\n",
    "\n",
    "    rows.append({\n",
    "        \"Format\": name,\n",
    "        \"Available in this runtime\": \"yes\" if runtime_available else \"no\",\n",
    "        \"Total bits\": bits,\n",
    "        \"Exponent bits\": exp_b,\n",
    "        \"Mantissa bits\": mant_b,\n",
    "        \"Precision bits (incl hidden 1)\": mant_b + 1,\n",
    "        \"Approx decimal digits\": round((mant_b + 1) * math.log10(2), 1),\n",
    "        \"Exponent range\": exp_range,\n",
    "        \"epsilon (ULP at 1.0)\": eps_s,\n",
    "        \"ULP at 1.0\": ulp_s,\n",
    "        \"Min normal\": tiny_s,\n",
    "        \"Min subnormal\": sub_s,\n",
    "        \"Max finite\": max_s,\n",
    "    })\n",
    "\n",
    "df_cheat = pd.DataFrame(rows).set_index(\"Format\")\n",
    "display(df_cheat)\n",
    "\n",
    "print()\n",
    "print(\"Rows marked 'no' still show reference values so the table stays complete.\")\n",
    "print(\"* TF32 is not a storage format. It is used internally by Tensor Cores on\")\n",
    "print(\"  Ampere+ GPUs for FP32 matmuls: FP32 range, but only 10 mantissa bits.\")\n",
    "print(\"  Your 'FP32 baseline' on Ampere+ may secretly be TF32 precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read this table\n",
    "\n",
    "Think of a floating-point format as a **measuring tape**:\n",
    "- **Exponent bits** determine the **length** of the tape (range: how big/small magnitudes you can reach).\n",
    "- **Mantissa bits** determine how **fine the tick marks** are (precision: how many significant digits you keep).\n",
    "\n",
    "| Format | Range | Precision | Training implication |\n",
    "|---|---|---|---|\n",
    "| **FP8** | Very narrow | Very low | Needs careful scaling (per-tensor/per-channel), mostly used with specialized kernels/hardware |\n",
    "| **FP16** | Narrow (5-bit exp) | Moderate (10-bit mantissa) | Underflow risk for gradients, overflow risk for activations → needs **loss scaling** |\n",
    "| **BF16** | Wide (8-bit exp, same as FP32) | Low (7-bit mantissa) | Rarely underflows → usually **no loss scaling** needed, but coarse rounding in reductions |\n",
    "| **FP32** | Wide | High | Stable baseline; slower and more memory |\n",
    "| **TF32** | Wide | Moderate (10-bit mantissa) | Compute-only on Ampere+ GPUs: FP32 matmuls may silently use TF32 for speed |\n",
    "| **FP64** | Very wide | Very high | Mostly for numeric reference/debugging (too slow for large training) |\n",
    "\n",
    "**Two immediate consequences:**\n",
    "1. FP16 has more mantissa bits than BF16 → **better precision per value**.\n",
    "2. BF16 has the same exponent width as FP32 → **dramatically better range** than FP16.\n",
    "\n",
    "So: FP16 fails first due to **range** (underflow/overflow). BF16 fails first due to **precision** (rounding/accumulation error). FP8 fails due to both unless extra care is taken.\n",
    "\n",
    "Autocast exists to route computations so that you get the performance of 16-bit compute without the worst numeric failure modes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visual bit layout: sign / exponent / mantissa for each format\n",
    "# This is the single most referenced diagram in floating-point explanations.\n",
    "\n",
    "formats_vis = [\n",
    "    (\"FP64 (double)\",     1, 11, 52, 64),\n",
    "    (\"FP32 (single)\",     1,  8, 23, 32),\n",
    "    (\"TF32 (tensor)*\",    1,  8, 10, 19),\n",
    "    (\"BF16 (brain float)\",1,  8,  7, 16),\n",
    "    (\"FP16 (IEEE half)\",  1,  5, 10, 16),\n",
    "    (\"FP8 (E5M2)\",        1,  5,  2,  8),\n",
    "    (\"FP8 (E4M3)\",        1,  4,  3,  8),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "y_positions = list(range(len(formats_vis)))[::-1]\n",
    "bar_height = 0.6\n",
    "max_bits = max(f[4] for f in formats_vis)\n",
    "\n",
    "for i, (name, s_bits, e_bits, m_bits, total) in enumerate(formats_vis):\n",
    "    y = y_positions[i]\n",
    "    # Scale bar width proportional to bit count (relative to max)\n",
    "    scale = 0.85  # fraction of plot width for the widest format\n",
    "    bit_width = scale / max_bits\n",
    "\n",
    "    # Draw sign bits\n",
    "    ax.barh(y, s_bits * bit_width, height=bar_height, left=0,\n",
    "            color=\"#e74c3c\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "    # Draw exponent bits\n",
    "    ax.barh(y, e_bits * bit_width, height=bar_height, left=s_bits * bit_width,\n",
    "            color=\"#3498db\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "    # Draw mantissa bits\n",
    "    ax.barh(y, m_bits * bit_width, height=bar_height, left=(s_bits + e_bits) * bit_width,\n",
    "            color=\"#2ecc71\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "\n",
    "    # Labels inside bars\n",
    "    mid_s = s_bits * bit_width / 2\n",
    "    mid_e = (s_bits + e_bits / 2) * bit_width\n",
    "    mid_m = (s_bits + e_bits + m_bits / 2) * bit_width\n",
    "\n",
    "    fontsize = 8 if total >= 16 else 7\n",
    "    if s_bits >= 1:\n",
    "        ax.text(mid_s, y, f\"S\\n{s_bits}\", ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "    ax.text(mid_e, y, f\"Exp\\n{e_bits}\", ha=\"center\", va=\"center\", fontsize=fontsize, fontweight=\"bold\", color=\"white\")\n",
    "    if m_bits >= 2:\n",
    "        ax.text(mid_m, y, f\"Mantissa\\n{m_bits}\", ha=\"center\", va=\"center\", fontsize=fontsize, fontweight=\"bold\", color=\"white\")\n",
    "    else:\n",
    "        ax.text(mid_m, y, f\"M{m_bits}\", ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "\n",
    "    # Format name and total bits on the left\n",
    "    ax.text(-0.02, y, f\"{name}  [{total} bits]\", ha=\"right\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlim(-0.55, max_bits * scale / max_bits + 0.05)\n",
    "ax.set_ylim(-0.5, len(formats_vis) - 0.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.set_title(\"Floating-Point Bit Layouts: where the bits go\", fontsize=13, fontweight=\"bold\", pad=15)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#e74c3c\", edgecolor=\"white\", label=\"Sign (1 bit)\"),\n",
    "    Patch(facecolor=\"#3498db\", edgecolor=\"white\", label=\"Exponent (range)\"),\n",
    "    Patch(facecolor=\"#2ecc71\", edgecolor=\"white\", label=\"Mantissa (precision)\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"  - BF16 has the SAME exponent width as FP32 (8 bits) → same range → no underflow issues\")\n",
    "print(\"  - FP16 has a NARROWER exponent (5 bits) but MORE mantissa than BF16 → better precision, worse range\")\n",
    "print(\"  - TF32 combines FP32's exponent with FP16's mantissa width (10 bits) → internal to Tensor Cores\")\n",
    "print(\"  - FP8 formats have tiny mantissa → need per-tensor scaling to be usable\")\n",
    "print()\n",
    "print(\"* TF32 is 19 bits internally but is NOT a storage format. Tensor Cores use it transparently for FP32 matmuls.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision / Autocast Regimes (PyTorch) — Cheat Sheet\n",
    "\n",
    "This is the \"optimizer table\" equivalent for AMP: a small set of regimes that cover most real training setups.\n",
    "\n",
    "**Rule of thumb (training):**\n",
    "- If you have CUDA + BF16 support → use **BF16 autocast** (usually no GradScaler).\n",
    "- Else if you have CUDA → use **FP16 autocast + GradScaler**.\n",
    "- Avoid `model.half()` / \"everything FP16\" for training unless you're deliberately doing it (it removes autocast's safety policy and makes optimizer math low precision).\n",
    "\n",
    "| Regime (common name) | What you set | What runs in 16-bit | What stays FP32 (by policy) | Loss scaling | Typical outcome |\n",
    "|---|---|---|---|---|---|\n",
    "| **FP32 baseline** | model params FP32, autocast OFF | nothing | everything | no | stable, slower |\n",
    "| **FP32 + TF32 matmuls** | Ampere+ default unless disabled | matmuls use **TF32** internally | everything else FP32 | no | stable + faster, but matmul precision is ~FP16 mantissa |\n",
    "| **AMP BF16 (recommended if supported)** | model params FP32, `autocast(dtype=bf16)` | matmuls / linears / convs | softmax / layernorm / losses / big reductions | no | stable + fast (range like FP32) |\n",
    "| **AMP FP16 + GradScaler** | model params FP32, `autocast(dtype=fp16)` + `GradScaler` | matmuls / linears / convs | softmax / layernorm / losses / big reductions | **yes** | stable + fast (but FP16 gradients need scaling) |\n",
    "| **Naive BF16** | model params BF16, autocast OFF | everything | almost nothing | no | often \"works\", but sensitive ops can drift (reductions/normalization) |\n",
    "| **Naive FP16** | model params FP16, autocast OFF | everything | almost nothing | maybe (manual) | frequently unstable (underflow/overflow + update stagnation) |\n",
    "\n",
    "**Key idea:** autocast is useful even if you *could* run everything in BF16/FP16 — it keeps the numerically sensitive operations in FP32."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Suggest an AMP recipe for THIS machine\n",
    "\n",
    "def recommend_amp_recipe(dev: torch.device):\n",
    "    rec = {\"device\": dev.type}\n",
    "    if dev.type == \"cuda\":\n",
    "        bf16_ok = torch.cuda.is_bf16_supported()\n",
    "        rec[\"bf16_supported\"] = bool(bf16_ok)\n",
    "        if bf16_ok:\n",
    "            rec[\"recommended_autocast_dtype\"] = \"bfloat16\"\n",
    "            rec[\"use_grad_scaler\"] = False\n",
    "            rec[\"why\"] = \"BF16 has FP32-like exponent range → underflow is rare.\"\n",
    "        else:\n",
    "            rec[\"recommended_autocast_dtype\"] = \"float16\"\n",
    "            rec[\"use_grad_scaler\"] = True\n",
    "            rec[\"why\"] = \"FP16 has narrow exponent range → GradScaler rescues gradients from underflow.\"\n",
    "        rec[\"note\"] = \"Keep model parameters in FP32; let autocast choose per-op dtypes.\"\n",
    "    elif dev.type == \"cpu\":\n",
    "        rec[\"recommended_autocast_dtype\"] = \"bfloat16\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"CPU autocast supports BF16; speedups vary by CPU/kernel support.\"\n",
    "        rec[\"note\"] = \"CPU demos here are mostly about numerics, not performance.\"\n",
    "    elif dev.type == \"mps\":\n",
    "        rec[\"recommended_autocast_dtype\"] = \"float16\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"MPS typically uses FP16 for reduced precision.\"\n",
    "        rec[\"note\"] = \"Operator coverage differs from CUDA; verify dtype behavior with the probes in Section 3.\"\n",
    "    else:\n",
    "        rec[\"recommended_autocast_dtype\"] = \"float32\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"Unknown device type.\"\n",
    "        rec[\"note\"] = \"\"\n",
    "    return rec\n",
    "\n",
    "display(pd.DataFrame([recommend_amp_recipe(device)]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 — Theory\n",
    "\n",
    "The core trick of autocast is simple to state:\n",
    "\n",
    "> **Run the *right* operations in lower precision for speed/memory, while keeping *numerically sensitive* operations in FP32.**\n",
    "\n",
    "But to understand *why* this works (and when it doesn't), we need to understand what floating-point formats can and cannot represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Where floating-point lives during training (and why autocast exists)\n",
    "\n",
    "A single training step can be decomposed into:\n",
    "\n",
    "1. **Forward**: parameters + activations → logits\n",
    "2. **Loss**: logits + targets → scalar loss\n",
    "3. **Backward**: loss → gradients for parameters\n",
    "4. **Optimizer update**: parameters + gradients (+ optimizer state) → new parameters\n",
    "\n",
    "Different tensors have different numeric requirements:\n",
    "\n",
    "| Tensor | Typical AMP dtype | Why |\n",
    "|---|---|---|\n",
    "| Activations / matmul results | FP16/BF16 (where safe) | Saves memory + uses Tensor Cores |\n",
    "| Softmax / LayerNorm stats / reductions | FP32 | Protects against overflow + rounding accumulation |\n",
    "| Gradients | Often FP32 *storage* (even if compute is mixed) | Stable updates + compatibility with optimizers |\n",
    "| Parameters (\"master weights\") | FP32 | Prevents update stagnation |\n",
    "| Optimizer state (Adam moments) | FP32 | Long-horizon accumulation is precision-sensitive |\n",
    "\n",
    "**Autocast's job** is mostly about **(1) and (2)**: choose per-op dtypes during the forward pass.\n",
    "\n",
    "**GradScaler's job** is mostly about **(3)** when FP16 is involved: keep gradients from underflowing to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Floating-point is \"range + precision\", not just \"more bits = better\"\n",
    "\n",
    "A binary floating-point number is roughly:\n",
    "\n",
    "$$(-1)^{\\text{sign}} \\times (1.\\text{mantissa}) \\times 2^{\\text{exponent}}$$\n",
    "\n",
    "The bit budget is split across:\n",
    "\n",
    "- **Exponent bits** → *range* (how large/small magnitudes you can represent)\n",
    "- **Mantissa (fraction) bits** → *precision* (how many significant bits you keep)\n",
    "\n",
    "For deep learning training, the key question is not \"can I store 3.14159?\" but:\n",
    "\n",
    "- Can I represent **tiny gradients** without them becoming 0 (underflow)?\n",
    "- Can I represent **large activations** without them becoming `inf` (overflow)?\n",
    "- Can I sum many numbers without destroying meaning via rounding?\n",
    "\n",
    "These failure modes show up differently in FP16 and BF16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 IEEE 754 anatomy (sign, exponent bias, hidden bit)\n",
    "\n",
    "A *normalized* binary floating-point value is encoded as:\n",
    "- **sign bit** $s$ (0 = positive, 1 = negative)\n",
    "- **exponent field** $E$ (stored with a **bias** so we can represent negative exponents)\n",
    "- **mantissa / fraction field** $m$\n",
    "\n",
    "For normalized numbers:\n",
    "\n",
    "$$\\text{value} = (-1)^s \\times (1 + m) \\times 2^{(E - \\text{bias})}$$\n",
    "\n",
    "Key details:\n",
    "- The leading `1.` is **implicit** (the \"hidden bit\"), giving you 1 extra bit of effective precision for free.\n",
    "- Exponent all-zeros and all-ones are **reserved**: `E=0` → subnormals / zero; `E=all ones` → `inf` / `nan`.\n",
    "- The **bias** is $2^{(\\text{exp\\_bits} - 1)} - 1$. For FP32: $127$. For FP16: $15$. For BF16: $127$.\n",
    "\n",
    "Let's decode $\\pi$ in all three formats to see this concretely."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bit-level decoding of pi across FP32, FP16, BF16\n",
    "\n",
    "def bits_f32(x: float) -> str:\n",
    "    (u32,) = struct.unpack(\">I\", struct.pack(\">f\", float(x)))\n",
    "    return f\"{u32:032b}\"\n",
    "\n",
    "def bits_f16(x: float) -> str:\n",
    "    u16 = np.frombuffer(np.float16(x).tobytes(), dtype=np.uint16)[0]\n",
    "    return f\"{int(u16):016b}\"\n",
    "\n",
    "def bits_bf16(x: float) -> str:\n",
    "    t = torch.tensor(float(x), dtype=torch.bfloat16)\n",
    "    i16 = int(t.view(torch.int16).item()) & 0xFFFF\n",
    "    return f\"{i16:016b}\"\n",
    "\n",
    "def decode_float(bits: str, exp_bits: int, mant_bits: int, bias: int):\n",
    "    s = int(bits[0], 2)\n",
    "    E = int(bits[1:1+exp_bits], 2)\n",
    "    M_bits = bits[1+exp_bits:]\n",
    "    assert len(M_bits) == mant_bits\n",
    "\n",
    "    if E == 0:\n",
    "        exp = 1 - bias\n",
    "        mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "        val = ((-1)**s) * mant * (2**exp)\n",
    "        return \"subnormal/zero\", s, E, exp, mant, val\n",
    "\n",
    "    if E == (2**exp_bits - 1):\n",
    "        return \"inf/nan\", s, E, None, None, None\n",
    "\n",
    "    exp = E - bias\n",
    "    mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "    val = ((-1)**s) * (1.0 + mant) * (2**exp)\n",
    "    return \"normal\", s, E, exp, mant, val\n",
    "\n",
    "x = math.pi\n",
    "rows = []\n",
    "\n",
    "for name, get_bits, eb, mb, bias in [\n",
    "    (\"float32\", bits_f32, 8, 23, 127),\n",
    "    (\"float16\", bits_f16, 5, 10, 15),\n",
    "    (\"bfloat16\", bits_bf16, 8, 7, 127),\n",
    "]:\n",
    "    b = get_bits(x)\n",
    "    kind, s, E, exp, mant, val = decode_float(b, eb, mb, bias)\n",
    "    rows.append({\n",
    "        \"dtype\": name,\n",
    "        \"bits\": f\"{b[:1]}|{b[1:1+eb]}|{b[1+eb:]}\",\n",
    "        \"sign\": s, \"E(stored)\": E, \"exponent\": exp,\n",
    "        \"1+mantissa\": round(1 + mant, 8) if mant is not None else None,\n",
    "        \"decoded\": round(val, 10) if val is not None else None,\n",
    "        \"error vs pi\": f\"{abs(val - math.pi):.2e}\" if val is not None else None,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(f\"\\nTrue pi = {math.pi}\")\n",
    "print(\"Notice: BF16 and FP16 both decode to 3.140625, but via different bit patterns.\")\n",
    "print(\"FP16 has more mantissa bits (10) giving finer precision; BF16 has fewer (7) but same exponent range as FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Why FP32 → BF16 conversion is trivial (but FP32 → FP16 is not)\n",
    "\n",
    "Since BF16 shares the same 8-bit exponent as FP32, converting FP32 to BF16 is just **truncating** (or rounding) the bottom 16 mantissa bits. The exponent field doesn't change, so no value can overflow or underflow during conversion.\n",
    "\n",
    "Converting FP32 to FP16, on the other hand, requires **narrowing the exponent** from 8 bits to 5 bits. This means:\n",
    "- FP32 values with exponents outside FP16's range (below $2^{-14}$ or above $2^{15}$) become 0 or `inf` during conversion.\n",
    "- The conversion itself can destroy values even before you do any computation.\n",
    "\n",
    "This is one reason BF16 is considered a \"drop-in\" replacement for FP32 in many training scenarios, while FP16 requires extra infrastructure (loss scaling, careful range management)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FP32 → BF16 vs FP32 → FP16 conversion: what survives?\n",
    "\n",
    "test_values = [3.14159, 1e-6, 1e-10, 1e-30, 1e-38, 1e30, 65504.0, 65536.0, 1e38]\n",
    "\n",
    "rows = []\n",
    "for v in test_values:\n",
    "    fp32 = torch.tensor(v, dtype=torch.float32)\n",
    "    bf16 = fp32.to(torch.bfloat16)\n",
    "    fp16 = fp32.to(torch.float16)\n",
    "    rows.append({\n",
    "        \"FP32 value\": f\"{v:.2e}\",\n",
    "        \"→ BF16\": f\"{float(bf16):.4e}\",\n",
    "        \"BF16 survived?\": \"inf/0\" if not torch.isfinite(bf16) or float(bf16) == 0 and v != 0 else \"YES\",\n",
    "        \"BF16 rel error\": f\"{abs(float(bf16) - v) / (abs(v) + 1e-45):.2e}\" if torch.isfinite(bf16) and float(bf16) != 0 else \"-\",\n",
    "        \"→ FP16\": f\"{float(fp16):.4e}\",\n",
    "        \"FP16 survived?\": \"inf\" if torch.isinf(fp16) else (\"0 (underflow)\" if float(fp16) == 0 and v != 0 else \"YES\"),\n",
    "        \"FP16 rel error\": f\"{abs(float(fp16) - v) / (abs(v) + 1e-45):.2e}\" if torch.isfinite(fp16) and float(fp16) != 0 else \"-\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey takeaway:\")\n",
    "print(\"  BF16 preserves ALL magnitudes (same exponent range as FP32) — just loses some decimal precision.\")\n",
    "print(\"  FP16 DESTROYS values outside its narrow range: 1e-10 underflows to 0, 65536 overflows to inf.\")\n",
    "print(\"  This is why BF16 conversion is 'just truncate the mantissa' — safe and trivial.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Normal vs subnormal numbers (and \"flush-to-zero\")\n",
    "\n",
    "**Subnormals** (also called denormals) extend the representable range closer to 0 by giving up the implicit leading `1.`:\n",
    "\n",
    "$$\\text{subnormal value} = (-1)^s \\times (0.\\text{mantissa}) \\times 2^{(1 - \\text{bias})}$$\n",
    "\n",
    "They matter because **gradients can be very small**. But subnormals can be slow on some hardware, so many compute paths enable **FTZ/DAZ** (\"flush-to-zero\" / \"denormals-are-zero\"), which means extremely small values become exactly 0.\n",
    "\n",
    "**Practical lesson:** It is not enough to know the *spec* of a dtype. You also need to know what your hardware/kernel path does with subnormals.\n",
    "\n",
    "Let's probe whether the smallest subnormal survives on your device."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Subnormal survival probe\n",
    "\n",
    "def subnormal_survives(dtype, dev):\n",
    "    z = torch.tensor(0.0, dtype=dtype, device=dev)\n",
    "    o = torch.tensor(1.0, dtype=dtype, device=dev)\n",
    "    sub = torch.nextafter(z, o)\n",
    "    return {\n",
    "        \"dtype\": str(dtype), \"device\": dev.type,\n",
    "        \"nextafter(0,1)\": f\"{float(sub):.6e}\",\n",
    "        \"is_zero\": bool((sub == 0).item()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    try:\n",
    "        rows.append(subnormal_survives(dt, device))\n",
    "    except Exception as e:\n",
    "        rows.append({\"dtype\": str(dt), \"device\": device.type, \"error\": type(e).__name__})\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 The measuring tape analogy\n",
    "\n",
    "A good way to think about floating-point formats is as a **measuring tape**:\n",
    "- The **length** of the tape is determined by the exponent bits (range: how big/small you can measure).\n",
    "- The **fineness of the tick marks** is determined by the mantissa bits (precision: how closely you can read off a value).\n",
    "\n",
    "FP32 is a long tape with fine tick marks. BF16 is equally long but with coarser tick marks. FP16 is a much shorter tape with tick marks finer than BF16 but coarser than FP32.\n",
    "\n",
    "For training, **tape length (range) matters more than tick mark fineness (precision)** — because a gradient that falls *off the tape entirely* (underflow to zero) provides zero learning signal, while a gradient that lands *between tick marks* (rounding) still provides a useful approximate signal. SGD is inherently noisy; it tolerates imprecise gradients, but it cannot learn from absent ones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The measuring tape: visualize range and precision as tape length vs tick density\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 5), gridspec_kw={\"hspace\": 0.6})\n",
    "\n",
    "tape_configs = [\n",
    "    (\"FP32 (8-bit exp, 23-bit mantissa)\", torch.float32, \"#2ecc71\", -126, 127, 23),\n",
    "    (\"BF16 (8-bit exp, 7-bit mantissa)\",  torch.bfloat16, \"#3498db\", -126, 127, 7),\n",
    "    (\"FP16 (5-bit exp, 10-bit mantissa)\", torch.float16, \"#e74c3c\", -14, 15, 10),\n",
    "]\n",
    "\n",
    "for ax, (label, dt, color, exp_min, exp_max, mant_bits) in zip(axes, tape_configs):\n",
    "    # Draw the tape as a colored bar representing the exponent range\n",
    "    tape_left = exp_min\n",
    "    tape_right = exp_max\n",
    "    full_range = 127 - (-126)  # FP32 range for normalization\n",
    "\n",
    "    # Normalize to common axis\n",
    "    ax.barh(0, tape_right - tape_left, left=tape_left, height=0.4,\n",
    "            color=color, alpha=0.3, edgecolor=color, linewidth=2)\n",
    "\n",
    "    # Draw tick marks proportional to mantissa precision\n",
    "    # More mantissa bits = more ticks (denser)\n",
    "    n_ticks = min(2 ** mant_bits, 200)  # cap for visualization\n",
    "    tick_positions = np.linspace(tape_left, tape_right, n_ticks)\n",
    "    for tp in tick_positions:\n",
    "        ax.plot([tp, tp], [-0.15, 0.15], color=color, linewidth=0.3, alpha=0.6)\n",
    "\n",
    "    # Labels\n",
    "    ax.text(tape_left - 1, 0, label, ha=\"right\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "    ax.text((tape_left + tape_right) / 2, -0.35,\n",
    "            f\"Range: 2^{exp_min} to 2^{exp_max}  |  Precision: {2**mant_bits} levels per power-of-2\",\n",
    "            ha=\"center\", va=\"top\", fontsize=8, color=\"gray\")\n",
    "\n",
    "    ax.set_xlim(-140, 135)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.set_xlabel(\"exponent (log2 scale)\" if ax == axes[-1] else \"\")\n",
    "\n",
    "fig.suptitle(\"The Measuring Tape: Range (tape length) vs Precision (tick density)\", fontsize=12, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  FP32 and BF16 have the SAME tape length (range) — they can measure the same extremes.\")\n",
    "print(\"  FP16 has a MUCH SHORTER tape — values beyond 2^15 ≈ 65504 overflow to infinity,\")\n",
    "print(\"  and values below 2^-14 ≈ 6e-5 underflow to zero.\")\n",
    "print()\n",
    "print(\"  But BF16's tick marks are 8x coarser than FP16's (128 vs 1024 levels per interval).\")\n",
    "print(\"  For training, this tradeoff overwhelmingly favors BF16: coarse ticks = noise, short tape = death.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 ULP: spacing grows with magnitude\n",
    "\n",
    "A float format has *roughly constant relative precision* but *variable absolute precision*.\n",
    "\n",
    "- Near 1.0, FP16 spacing is ~$10^{-3}$.\n",
    "- Near 1024, FP16 spacing is ~$1$.\n",
    "\n",
    "This is the concrete reason \"tiny updates disappear\" when weights are stored in low precision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ULP vs magnitude for each dtype\n",
    "\n",
    "def ulp(x: torch.Tensor, dtype: torch.dtype):\n",
    "    x = x.to(dtype)\n",
    "    return (torch.nextafter(x, x * 2) - x).abs().to(torch.float32)\n",
    "\n",
    "ks = torch.arange(-10, 21, device=device)\n",
    "x = (2.0 ** ks).to(torch.float32)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, color in [(torch.float16, \"C0\"), (torch.bfloat16, \"C1\"), (torch.float32, \"C2\")]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    u = ulp(x, dt).cpu().numpy()\n",
    "    plt.plot(ks.cpu().numpy(), np.log2(u + 1e-45), marker=\"o\", markersize=4, label=str(dt), color=color)\n",
    "\n",
    "plt.title(\"ULP (spacing between adjacent floats) vs magnitude\")\n",
    "plt.xlabel(\"log2(|x|)\")\n",
    "plt.ylabel(\"log2(ULP(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 Number line: where representable floats actually live\n",
    "\n",
    "The spacing between representable numbers is *not uniform* — it depends on the magnitude. Near zero, floats are dense; as magnitude grows, they spread apart. And crucially, **different formats have different densities at every scale**.\n",
    "\n",
    "This visualization plots the actual representable numbers in each format within a small interval. Think of it as zooming into the \"measuring tape\" to see the tick marks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number line: representable floats in [1.0, 2.0) for each dtype\n",
    "# This interval is illuminating because ULP is constant within a power-of-2 interval\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 6), sharex=True)\n",
    "\n",
    "for ax, (dt, label, color) in zip(axes, [\n",
    "    (torch.float32, \"FP32 (23-bit mantissa: 8,388,608 values in [1,2))\", \"C2\"),\n",
    "    (torch.bfloat16, \"BF16 (7-bit mantissa: 128 values in [1,2))\", \"C1\"),\n",
    "    (torch.float16, \"FP16 (10-bit mantissa: 1,024 values in [1,2))\", \"C0\"),\n",
    "    (None, \"Comparison overlay\", \"k\"),\n",
    "]):\n",
    "    if dt is not None:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        two = torch.tensor(2.0, dtype=dt)\n",
    "        vals = [float(one)]\n",
    "        cur = one\n",
    "        while True:\n",
    "            cur = torch.nextafter(cur, two)\n",
    "            if float(cur) >= 2.0:\n",
    "                break\n",
    "            vals.append(float(cur))\n",
    "            if len(vals) > 2000:\n",
    "                break\n",
    "        vals = np.array(vals)\n",
    "        ax.eventplot([vals], lineoffsets=0, linelengths=0.6, colors=color, linewidths=0.5)\n",
    "        ax.set_ylabel(label, fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.text(1.0, 0.35, f\"{len(vals)} representable values\", fontsize=8, color=color)\n",
    "    else:\n",
    "        # Overlay: show a narrow window [1.0, 1.02] with all three\n",
    "        for dt2, c2, yoff in [(torch.float16, \"C0\", 0.3), (torch.bfloat16, \"C1\", 0.0), (torch.float32, \"C2\", -0.3)]:\n",
    "            one2 = torch.tensor(1.0, dtype=dt2)\n",
    "            limit = torch.tensor(1.02, dtype=dt2)\n",
    "            vs = [float(one2)]\n",
    "            cur2 = one2\n",
    "            for _ in range(200):\n",
    "                cur2 = torch.nextafter(cur2, limit)\n",
    "                if float(cur2) >= 1.02:\n",
    "                    break\n",
    "                vs.append(float(cur2))\n",
    "            vs = np.array(vs)\n",
    "            ax.eventplot([vs], lineoffsets=yoff, linelengths=0.25, colors=c2, linewidths=1.0)\n",
    "        ax.set_ylabel(\"Zoomed [1.0, 1.02]\", fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim(1.0, 1.02)\n",
    "        ax.legend([\"FP16\", \"BF16\", \"FP32\"], fontsize=7, loc=\"upper right\")\n",
    "\n",
    "axes[0].set_xlim(1.0, 2.0)\n",
    "for ax in axes[:3]:\n",
    "    ax.set_xlim(1.0, 2.0)\n",
    "axes[-1].set_xlim(1.0, 1.02)\n",
    "axes[-1].set_xlabel(\"value\")\n",
    "fig.suptitle(\"Representable floats in [1.0, 2.0): density depends on mantissa bits\", fontsize=11, y=1.01)\n",
    "plt.tight_layout();\n",
    "print(\"Key insight: BF16 has ~8x fewer representable values than FP16 in [1,2),\")\n",
    "print(\"but FP16 has ~8,192x fewer than FP32. This is the precision-range tradeoff made visible.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 The bit-level addition trap (why `1 + 1e-4 = 1` in FP16)\n",
    "\n",
    "This is the single most important numeric fact for understanding **weight update stagnation**.\n",
    "\n",
    "When adding two floating-point numbers, the hardware must **align exponents** by shifting the smaller number's mantissa to the right. If the shift pushes all significant bits past the mantissa width, the smaller number is effectively lost.\n",
    "\n",
    "**Concrete example (from the FP16 bit-level):**\n",
    "\n",
    "- `1.0` in FP16: exponent = $2^0$, mantissa = all zeros.\n",
    "- `1e-4` in FP16: exponent = $2^{-14}$, mantissa encodes ~1.639.\n",
    "- To add them, we must shift `1e-4`'s mantissa by **14 positions** to align with `1.0`'s exponent.\n",
    "- FP16 has only **10 mantissa bits**. After shifting 14 positions right, *all* significant bits fall off the edge.\n",
    "- Result: `1.0 + 1e-4 = 1.0` exactly.\n",
    "\n",
    "This is exactly what happens during training: if `learning_rate * gradient` is smaller than the ULP at the weight's magnitude, the weight **never changes**.\n",
    "\n",
    "FP16's epsilon is ~$9.77 \\times 10^{-4}$. Any update smaller than this relative to the weight magnitude is silently dropped."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate the addition trap across dtypes\n",
    "\n",
    "print(\"Does 1.0 + delta produce a value > 1.0?\\n\")\n",
    "\n",
    "deltas = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "rows = []\n",
    "for delta in deltas:\n",
    "    row = {\"delta\": f\"{delta:.0e}\"}\n",
    "    for name, dt in [(\"FP16\", torch.float16), (\"BF16\", torch.bfloat16), (\"FP32\", torch.float32)]:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        d = torch.tensor(delta, dtype=dt)\n",
    "        result = one + d\n",
    "        changed = float(result) != float(one)\n",
    "        row[name] = \"YES\" if changed else \"no (lost!)\"\n",
    "    rows.append(row)\n",
    "\n",
    "df_add = pd.DataFrame(rows)\n",
    "display(df_add)\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"- FP16 loses updates smaller than ~1e-3 relative to weight magnitude.\")\n",
    "print(\"- BF16 loses updates smaller than ~8e-3 (even coarser!).\")\n",
    "print(\"- FP32 loses updates smaller than ~1e-7.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights: typical lr*grad products\")\n",
    "print(\"are often 1e-5 to 1e-7, which FP16 and BF16 both silently discard.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bit-level mechanics: *why* `1.0 + 1e-4 = 1.0` in FP16\n",
    "\n",
    "The table above shows *that* small updates get lost. Let's see *why* at the bit level.\n",
    "\n",
    "When hardware adds two floats, it must **align their exponents** by right-shifting the smaller number's mantissa. If the shift exceeds the mantissa width, the smaller number's bits fall off completely.\n",
    "\n",
    "This is the exact same mechanism that causes **weight update stagnation** during training: `weight += lr * gradient` produces the same weight if `lr * gradient` is too small relative to the weight's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step-by-step bit alignment: why 1.0 + 1e-4 = 1.0 in FP16\n",
    "\n",
    "print(\"=== Bit-level addition in FP16: 1.0 + 1e-4 ===\\n\")\n",
    "\n",
    "# 1.0 in FP16\n",
    "one_f16 = np.float16(1.0)\n",
    "one_bits = f\"{int(np.frombuffer(one_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "print(f\"1.0 in FP16:   {one_bits[0]}|{one_bits[1:6]}|{one_bits[6:]}\")\n",
    "print(f\"               s| exp  | mantissa\")\n",
    "print(f\"               = (-1)^0 × 1.0000000000 × 2^(15 - 15) = 1.0\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 1e-4 in FP16\n",
    "small_f16 = np.float16(1e-4)\n",
    "small_bits = f\"{int(np.frombuffer(small_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "E_small = int(small_bits[1:6], 2)\n",
    "print(f\"1e-4 in FP16:  {small_bits[0]}|{small_bits[1:6]}|{small_bits[6:]}\")\n",
    "print(f\"               = (-1)^0 × 1.{small_bits[6:]} × 2^({E_small} - 15) = 2^{{{E_small - 15}}}\")\n",
    "print(f\"               ≈ {float(small_f16):.6e}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Addition step: align exponents ---\")\n",
    "print()\n",
    "shift = 15 - E_small\n",
    "print(f\"To add these, we align the smaller exponent ({E_small - 15}) to the larger (0).\")\n",
    "print(f\"This means shifting 1e-4's mantissa RIGHT by {shift} positions.\")\n",
    "print()\n",
    "print(f\"  1.0:     1.{'0' * 10}         (exponent = 0)\")\n",
    "print(f\"+ 1e-4:    0.{'0' * (shift - 1)}1{'?' * max(0, 10 - shift)}   (shifted {shift} positions right)\")\n",
    "print()\n",
    "print(f\"FP16 mantissa is only 10 bits wide.\")\n",
    "print(f\"After shifting right by {shift}, ALL significant bits of 1e-4 are\")\n",
    "print(f\"beyond the 10-bit mantissa boundary → they are discarded.\")\n",
    "print()\n",
    "print(f\"Result: 1.0 + 1e-4 = 1.0  (the small value vanished completely)\")\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "result = np.float16(1.0) + np.float16(1e-4)\n",
    "eps_f16 = np.finfo(np.float16).eps\n",
    "print(f\"Verification:  np.float16(1.0) + np.float16(1e-4) = {result}\")\n",
    "print(f\"FP16 epsilon:  {eps_f16:.4e}\")\n",
    "print(f\"1e-4 < eps?    {1e-4 < eps_f16}  → update is below FP16's resolution at magnitude 1.0\")\n",
    "print()\n",
    "print(\"Training implication: if weight ≈ 1.0 and lr × grad ≈ 1e-4,\")\n",
    "print(\"the weight NEVER changes in FP16. This is weight update stagnation.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 Epsilon meets training: where do real weight updates fall?\n",
    "\n",
    "The addition trap is not theoretical — it directly determines whether training succeeds. Let's connect the abstract epsilon values to concrete training scenarios.\n",
    "\n",
    "For a weight $w$ stored in a given dtype, the **minimum detectable update** is approximately $|w| \\times \\epsilon$. If $\\text{lr} \\times |\\text{grad}|$ is smaller than this, the weight never changes.\n",
    "\n",
    "Typical training setups use learning rates of $10^{-3}$ to $10^{-5}$, and gradient magnitudes often range from $10^{-2}$ to $10^{-6}$. Their product ($\\text{lr} \\times |\\text{grad}|$) is what the format must be able to represent *relative to the weight magnitude*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Where do typical lr × grad products fall relative to epsilon boundaries?\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Epsilon values (minimum detectable relative update)\n",
    "epsilons = {\n",
    "    \"FP16\": float(torch.finfo(torch.float16).eps),\n",
    "    \"BF16\": float(torch.finfo(torch.bfloat16).eps),\n",
    "    \"FP32\": float(torch.finfo(torch.float32).eps),\n",
    "}\n",
    "\n",
    "# Typical lr × |grad| magnitudes in real training\n",
    "typical_updates = {\n",
    "    \"lr=1e-3 × |g|=1e-1\\n(early training, large grads)\": 1e-4,\n",
    "    \"lr=1e-3 × |g|=1e-3\\n(mid training)\": 1e-6,\n",
    "    \"lr=1e-4 × |g|=1e-3\\n(fine-tuning)\": 1e-7,\n",
    "    \"lr=1e-4 × |g|=1e-5\\n(late training, small grads)\": 1e-9,\n",
    "    \"lr=1e-5 × |g|=1e-5\\n(LLM fine-tune)\": 1e-10,\n",
    "}\n",
    "\n",
    "y_pos = 0\n",
    "colors_eps = {\"FP16\": \"#e74c3c\", \"BF16\": \"#3498db\", \"FP32\": \"#2ecc71\"}\n",
    "\n",
    "# Draw epsilon boundaries as vertical lines\n",
    "for name, eps in epsilons.items():\n",
    "    ax.axvline(np.log10(eps), color=colors_eps[name], linewidth=3, alpha=0.7,\n",
    "               label=f\"{name} epsilon = {eps:.1e}\")\n",
    "\n",
    "# Shade the \"safe update\" zone (above all epsilons)\n",
    "ax.axvspan(np.log10(max(epsilons.values())), 0, alpha=0.05, color=\"green\")\n",
    "\n",
    "# Draw typical update magnitudes as horizontal markers\n",
    "y_updates = np.linspace(0.9, 0.1, len(typical_updates))\n",
    "for (label, mag), y in zip(typical_updates.items(), y_updates):\n",
    "    marker_color = \"green\"\n",
    "    if mag < epsilons[\"FP16\"]:\n",
    "        marker_color = \"red\"\n",
    "    elif mag < epsilons[\"BF16\"]:\n",
    "        marker_color = \"orange\"\n",
    "    ax.plot(np.log10(mag), y, \"D\", color=marker_color, markersize=10, zorder=5)\n",
    "    ax.text(np.log10(mag) + 0.15, y, label, fontsize=7, va=\"center\", color=marker_color)\n",
    "\n",
    "# Annotations\n",
    "ax.text(np.log10(epsilons[\"FP16\"]) - 0.1, 0.95,\n",
    "        \"← Updates here are LOST\\n    in FP16 (weight never changes)\",\n",
    "        fontsize=8, color=\"#e74c3c\", ha=\"right\", va=\"top\", fontstyle=\"italic\")\n",
    "ax.text(np.log10(epsilons[\"BF16\"]) - 0.1, 0.5,\n",
    "        \"← Also lost in BF16\",\n",
    "        fontsize=8, color=\"#3498db\", ha=\"right\", va=\"center\", fontstyle=\"italic\")\n",
    "\n",
    "ax.set_xlim(-12, 0.5)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "ax.set_xlabel(\"log10(lr × |gradient|)  — relative to weight magnitude of ~1.0\", fontsize=10)\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Where do real weight updates fall relative to format epsilon?\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Green diamonds: update is large enough for ALL formats\")\n",
    "print(\"  - Orange diamonds: update works in FP32 and FP16, but NOT in BF16\")\n",
    "print(\"  - Red diamonds: update only works in FP32\")\n",
    "print()\n",
    "print(\"This is why FP32 master weights are essential: the optimizer applies updates in FP32\")\n",
    "print(\"(where they're captured), then casts back to 16-bit for the next forward pass.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9 TF32: the format you're using without knowing it\n",
    "\n",
    "**TF32 (TensorFloat-32)** is an NVIDIA format introduced with the Ampere architecture (A100, 2020). It is *not* a storage format — you never create a TF32 tensor. Instead, Tensor Cores silently use TF32 arithmetic when you run FP32 matmuls on Ampere+ GPUs.\n",
    "\n",
    "**Bit layout (19 bits total):**\n",
    "\n",
    "| Component | Bits |\n",
    "|---|---|\n",
    "| Sign | 1 |\n",
    "| Exponent | 8 (same as FP32) |\n",
    "| Mantissa | 10 (same as FP16) |\n",
    "\n",
    "This gives TF32 the **range of FP32** (exponent $[-126, 127]$, max $\\sim 3.4 \\times 10^{38}$) with the **precision of FP16** (10 mantissa bits, $\\epsilon \\approx 9.77 \\times 10^{-4}$).\n",
    "\n",
    "**Why this matters:**\n",
    "- On Ampere+ GPUs, `torch.backends.cuda.matmul.allow_tf32` defaults to `True`.\n",
    "- Your \"FP32 baseline\" is **secretly TF32** for all matmuls (Linear layers, attention projections, etc.).\n",
    "- Non-matmul ops (LayerNorm, softmax, element-wise) still use full FP32 precision.\n",
    "- This is actually a reasonable default: matmuls with FP32 accumulation tolerate reduced input precision well, and TF32 is ~8× faster than strict FP32 on Tensor Cores.\n",
    "\n",
    "**The practical implication:** When you compare \"FP32 vs AMP\" on Ampere+ GPUs, you're really comparing \"TF32 matmuls + FP32 everything else\" vs \"FP16/BF16 matmuls + FP32 sensitive ops.\" The gap is smaller than you might expect."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TF32 demo: measure the precision impact of TF32 on matmul\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    M = 1024\n",
    "    a = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "    b = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Reference: strict FP32 (TF32 disabled)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    ref = a @ b\n",
    "\n",
    "    # TF32 enabled\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    out_tf32 = a @ b\n",
    "\n",
    "    # FP64 reference for absolute ground truth\n",
    "    ref64 = (a.double() @ b.double()).float()\n",
    "\n",
    "    def _rel_err(x, ref):\n",
    "        return float((x - ref).abs().mean() / (ref.abs().mean() + 1e-12))\n",
    "\n",
    "    err_strict = _rel_err(ref, ref64)\n",
    "    err_tf32 = _rel_err(out_tf32, ref64)\n",
    "\n",
    "    print(\"Matmul precision comparison (1024x1024, vs FP64 reference):\")\n",
    "    print(f\"  Strict FP32:  mean relative error = {err_strict:.2e}\")\n",
    "    print(f\"  TF32 enabled: mean relative error = {err_tf32:.2e}\")\n",
    "    print(f\"  TF32 / FP32 error ratio: {err_tf32 / max(err_strict, 1e-20):.1f}x\")\n",
    "    print()\n",
    "    print(f\"  TF32 matmul default: torch.backends.cuda.matmul.allow_tf32 = {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "    print()\n",
    "    print(\"Key insight: TF32 introduces ~1000x more error than strict FP32, but this is still\")\n",
    "    print(\"small enough (~1e-4 relative) that training is unaffected. The 8x Tensor Core speedup\")\n",
    "    print(\"makes this an excellent default tradeoff.\")\n",
    "    print()\n",
    "    print(\"To disable TF32 for strict FP32 comparisons:\")\n",
    "    print(\"  torch.backends.cuda.matmul.allow_tf32 = False\")\n",
    "    print(\"  torch.backends.cudnn.allow_tf32 = False\")\n",
    "else:\n",
    "    print(\"TF32 is an NVIDIA Ampere+ GPU feature. Not applicable on this device.\")\n",
    "    print(\"On CUDA Ampere+ GPUs, FP32 matmuls silently use TF32 (10-bit mantissa)\")\n",
    "    print(\"by default, giving ~8x speedup with ~FP16-level precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.10 Tensor Core mechanics: why mixed precision is fast\n",
    "\n",
    "Modern GPU speedups from mixed precision come from **Tensor Cores** — specialized hardware units that perform tile-based matrix-multiply-accumulate (MMA) operations.\n",
    "\n",
    "**How Tensor Cores work:**\n",
    "\n",
    "1. **Tile-based:** Tensor Cores operate on small tiles (e.g., 16×16×16 for FP16 on Volta/Turing, 16×8×16 on Ampere). The GPU's warp scheduler feeds tiles from the input matrices to Tensor Cores.\n",
    "2. **Mixed-precision accumulation:** Inputs are FP16/BF16 (or TF32/FP8), but the **accumulation** (partial sums) happens in **FP32**. This is why matmuls under autocast are both fast *and* numerically reasonable.\n",
    "3. **Result:** $D = A \\times B + C$ where $A$, $B$ are low-precision and $C$, $D$ are FP32 accumulators.\n",
    "\n",
    "**Dimension alignment matters:**\n",
    "\n",
    "Tensor Cores achieve maximum throughput when matrix dimensions are **multiples of 8** (for FP16/BF16) or **multiples of 16** (for FP8/INT8). Misaligned dimensions force the GPU to pad or fall back to slower CUDA cores.\n",
    "\n",
    "This is why you'll see guidance like:\n",
    "- Embedding dimensions: use multiples of 64 or 128\n",
    "- Vocabulary size: pad to a multiple of 8\n",
    "- Batch size: prefer multiples of 8\n",
    "\n",
    "The code below demonstrates the speedup difference between aligned and misaligned dimensions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tensor Core alignment benchmark: aligned vs misaligned dimensions\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    import torch.utils.benchmark as benchmark\n",
    "\n",
    "    def bench_matmul(M, K, N, dtype, label):\n",
    "        a = torch.randn(M, K, device=device, dtype=dtype)\n",
    "        b = torch.randn(K, N, device=device, dtype=dtype)\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = a @ b\n",
    "        torch.cuda.synchronize()\n",
    "        t = benchmark.Timer(\n",
    "            stmt=\"a @ b\",\n",
    "            globals={\"a\": a, \"b\": b},\n",
    "        )\n",
    "        m = t.blocked_autorange(min_run_time=0.5)\n",
    "        return m.median * 1e6  # microseconds\n",
    "\n",
    "    rows = []\n",
    "    for label, M, K, N in [\n",
    "        (\"Aligned (512×512×512)\", 512, 512, 512),\n",
    "        (\"Misaligned (511×513×509)\", 511, 513, 509),\n",
    "        (\"Aligned (1024×1024×1024)\", 1024, 1024, 1024),\n",
    "        (\"Misaligned (1023×1025×1021)\", 1023, 1025, 1021),\n",
    "    ]:\n",
    "        for dtype, dname in [(torch.float32, \"FP32\"), (torch.float16, \"FP16\"), (torch.bfloat16, \"BF16\")]:\n",
    "            if not supports_dtype_on_device(dtype, device):\n",
    "                continue\n",
    "            us = bench_matmul(M, K, N, dtype, label)\n",
    "            rows.append({\"shape\": label, \"dtype\": dname, \"time_us\": f\"{us:.0f}\"})\n",
    "\n",
    "    df_tc = pd.DataFrame(rows)\n",
    "    # Pivot for readability\n",
    "    if len(df_tc) > 0:\n",
    "        pivot = df_tc.pivot(index=\"shape\", columns=\"dtype\", values=\"time_us\")\n",
    "        print(\"Matmul time (microseconds) — aligned vs misaligned dimensions:\")\n",
    "        display(pivot)\n",
    "        print()\n",
    "        print(\"Aligned dimensions (multiples of 8) allow Tensor Cores to operate at full throughput.\")\n",
    "        print(\"Misaligned dimensions may cause padding overhead or fallback to slower CUDA cores.\")\n",
    "        print()\n",
    "        print(\"Practical rule: make embedding dim, hidden dim, vocab size, and batch size multiples of 8.\")\n",
    "else:\n",
    "    print(\"Tensor Core benchmarks require CUDA. Skipping on this device.\")\n",
    "    print()\n",
    "    print(\"Key takeaways for Tensor Cores:\")\n",
    "    print(\"  1. They perform tile-based MMA (matrix-multiply-accumulate) on small blocks (e.g. 16x16)\")\n",
    "    print(\"  2. Inputs are FP16/BF16, accumulation is FP32 — this is why autocast matmuls are accurate\")\n",
    "    print(\"  3. Matrix dimensions should be multiples of 8 for optimal Tensor Core utilization\")\n",
    "    print(\"  4. Speedup is typically 2-8x over standard FP32 CUDA cores\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 FP16 vs BF16 vs FP32: the complete numeric comparison\n",
    "\n",
    "We generated the cheat sheet above. Here we dig deeper into what those numbers mean for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed format facts table\n",
    "\n",
    "def _ulp_at_one_v2(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal_v2(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dtype_row(name, dtype, exp_bits, mant_bits, exp_min, exp_max):\n",
    "    fi = torch.finfo(dtype)\n",
    "    ulp1 = _ulp_at_one_v2(dtype)\n",
    "    sub = _smallest_subnormal_v2(dtype)\n",
    "    return {\n",
    "        \"dtype\": name,\n",
    "        \"bits\": fi.bits,\n",
    "        \"exp_bits\": exp_bits,\n",
    "        \"mant_bits\": mant_bits,\n",
    "        \"precision_bits\": mant_bits + 1,\n",
    "        \"decimal_digits\": round((mant_bits + 1) * math.log10(2), 2),\n",
    "        \"exp_range\": f\"[{exp_min}, {exp_max}]\",\n",
    "        \"epsilon\": f\"{fi.eps:.2e}\",\n",
    "        \"ulp(1.0)\": f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\",\n",
    "        \"min_normal\": f\"{fi.tiny:.2e}\",\n",
    "        \"min_subnormal\": f\"{sub:.2e}\" if sub is not None else \"n/a\",\n",
    "        \"max_finite\": f\"{fi.max:.2e}\",\n",
    "    }\n",
    "\n",
    "dtype_info = pd.DataFrame([\n",
    "    *( [dtype_row(\"float8_e4m3fn\", torch.float8_e4m3fn, 4, 3, -6, 7)] if hasattr(torch, \"float8_e4m3fn\") else [] ),\n",
    "    *( [dtype_row(\"float8_e5m2\", torch.float8_e5m2, 5, 2, -14, 15)] if hasattr(torch, \"float8_e5m2\") else [] ),\n",
    "    dtype_row(\"float16\", torch.float16, 5, 10, -14, 15),\n",
    "    dtype_row(\"bfloat16\", torch.bfloat16, 8, 7, -126, 127),\n",
    "    dtype_row(\"float32\", torch.float32, 8, 23, -126, 127),\n",
    "    dtype_row(\"float64\", torch.float64, 11, 52, -1022, 1023),\n",
    "]).set_index(\"dtype\")\n",
    "\n",
    "display(dtype_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the table\n",
    "\n",
    "**Precision** (how fine the tick marks are):\n",
    "- FP64: ULP at 1.0 is ~2e-16. This is essentially \"reference precision\" for most deep learning numerics.\n",
    "- FP32: ULP at 1.0 is ~$1.2 \\times 10^{-7}$. Updates as small as $10^{-7}$ are captured.\n",
    "- FP16: ULP at 1.0 is ~$9.8 \\times 10^{-4}$. Updates smaller than $10^{-3}$ are lost.\n",
    "- BF16: ULP at 1.0 is ~$7.8 \\times 10^{-3}$. Updates smaller than $10^{-2}$ are lost. Even coarser than FP16!\n",
    "- FP8: ULP at 1.0 is huge. FP8 is *not* a drop-in training dtype without extra scaling strategies and specialized kernels.\n",
    "\n",
    "**Range** (how long the measuring tape is):\n",
    "- FP16: smallest normal is ~$6 \\times 10^{-5}$. Gradients below this become zero.\n",
    "- BF16: smallest normal is ~$1.2 \\times 10^{-38}$, same as FP32. Gradients essentially never underflow.\n",
    "- FP8: range depends on format (E4M3 vs E5M2), but is far smaller than FP16/BF16/FP32.\n",
    "- This is why **FP16 needs loss scaling** but **BF16 usually does not**.\n",
    "\n",
    "### 1.2.1 TF32 — the hidden precision mode\n",
    "\n",
    "*(See §1.1.9 above for a detailed TF32 deep dive with code demo.)*\n",
    "\n",
    "On NVIDIA Ampere+ GPUs, FP32 matmuls can automatically use **TF32** internally:\n",
    "- Same 8-bit exponent as FP32 (full range)\n",
    "- But only 10 bits of mantissa (same as FP16 precision)\n",
    "- Transparent: your code says `float32`, but Tensor Cores use TF32 for speed\n",
    "\n",
    "This means your \"FP32 baseline\" on modern GPUs may actually be **TF32 precision** for matmuls. When comparing FP32 vs AMP, be aware of this hidden variable. You can control it with `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Why autocast targets matmul/linear first: FP32 accumulation\n",
    "\n",
    "Deep learning is dominated by large matrix multiplications (GEMMs): `Linear`, attention projections, and MLPs.\n",
    "\n",
    "On modern accelerators, these kernels typically:\n",
    "- **multiply** in FP16/BF16 (or TF32/FP8, depending on mode)\n",
    "- **accumulate** partial sums in **FP32**\n",
    "\n",
    "This is a sweet spot:\n",
    "- massive speedup (Tensor Cores / specialized units)\n",
    "- much better numeric behavior than \"pure FP16 accumulation\"\n",
    "\n",
    "Autocast heavily leans on this: it prefers to cast matmul-like ops down because they are both *fast* and comparatively *stable* (relative to softmax, layernorm, exp/log, large reductions)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Matmul accuracy across dtypes (vs FP64 reference)\n",
    "\n",
    "M = 256 if device.type != \"cuda\" else 512\n",
    "a = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "b = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "\n",
    "ref = (a.double() @ b.double()).float()\n",
    "\n",
    "def _err_stats(c, ref):\n",
    "    c = c.float()\n",
    "    abs_err = (c - ref).abs()\n",
    "    rel_err = abs_err / (ref.abs() + 1e-6)\n",
    "    return abs_err, rel_err\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    try:\n",
    "        c_dt = a.to(dt) @ b.to(dt)\n",
    "        abs_err, rel_err = _err_stats(c_dt, ref)\n",
    "        rows.append({\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": str(c_dt.dtype).replace(\"torch.\", \"\"),\n",
    "            \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "            \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "            \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "            \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "            \"note\": \"\",\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": \"-\",\n",
    "            \"max_abs_err\": \"-\",\n",
    "            \"mean_abs_err\": \"-\",\n",
    "            \"max_rel_err\": \"-\",\n",
    "            \"mean_rel_err\": \"-\",\n",
    "            \"note\": f\"matmul failed ({type(e).__name__})\",\n",
    "        })\n",
    "\n",
    "# FP32: on CUDA this may be strict FP32 or TF32 depending on allow_tf32.\n",
    "if supports_dtype_on_device(torch.float32, device):\n",
    "    if device.type == \"cuda\":\n",
    "        orig_tf32 = torch.backends.cuda.matmul.allow_tf32\n",
    "        try:\n",
    "            for allow in [False, True]:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = allow\n",
    "                c = a @ b\n",
    "                abs_err, rel_err = _err_stats(c, ref)\n",
    "                rows.append({\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"matmul_mode\": \"strict_fp32\" if not allow else \"tf32_allowed\",\n",
    "                    \"matmul_output_dtype\": str(c.dtype).replace(\"torch.\", \"\"),\n",
    "                    \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "                    \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "                    \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "                    \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "                    \"note\": \"\",\n",
    "                })\n",
    "        finally:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = orig_tf32\n",
    "    else:\n",
    "        c = a @ b\n",
    "        abs_err, rel_err = _err_stats(c, ref)\n",
    "        rows.append({\n",
    "            \"dtype\": \"float32\",\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": str(c.dtype).replace(\"torch.\", \"\"),\n",
    "            \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "            \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "            \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "            \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "            \"note\": \"\",\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- Errors come from (1) input rounding to dt and (2) output rounding back to dt.\")\n",
    "print(\"- On CUDA, FP16/BF16 matmuls usually accumulate in FP32 internally, which helps stability.\")\n",
    "print(\"- On Ampere+ CUDA GPUs, float32 matmuls may run in TF32 mode when allow_tf32=True (10 mantissa bits).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 The three numeric disasters that show up during training\n",
    "\n",
    "### (A) Underflow — values become 0\n",
    "\n",
    "- Common in **gradients**, especially late in training or in deep nets with tiny signals.\n",
    "- Most harmful in **FP16** due to narrow exponent range (5 bits → min normal $\\approx 6 \\times 10^{-5}$).\n",
    "- BF16 has the same exponent range as FP32, so underflow is rare.\n",
    "\n",
    "### (B) Overflow — values become `inf`\n",
    "\n",
    "- Common in **activations** (exponentials, attention logits) or badly-initialized models.\n",
    "- FP16 max is only ~65,504. Easy to exceed.\n",
    "- BF16 max is ~$3.4 \\times 10^{38}$, same as FP32.\n",
    "\n",
    "### (C) Accumulation / cancellation error\n",
    "\n",
    "Even when values are in range, precision limits corrupt sums and products:\n",
    "- Adding many small numbers to a large accumulator can lose the small contributions (same mechanism as the `1 + 1e-4` trap).\n",
    "- For reductions (layernorm statistics, softmax normalization, large sums), frameworks often keep accumulation in FP32.\n",
    "\n",
    "**Autocast** is partly about preventing A/B (range disasters), and partly about routing sensitive reductions so C doesn't destroy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0 Accumulation error, made concrete\n",
    "\n",
    "**Accumulation error** is just \"rounding happens at every add/multiply, and it compounds\".\n",
    "\n",
    "The simplest microscope is:\n",
    "\n",
    "> start at a value (like 1.0), add a tiny increment many times, and see when the increment stops \"counting\".\n",
    "\n",
    "This is the same mechanism behind:\n",
    "- weight-update stagnation (updates below ULP get rounded away)\n",
    "- instability in reductions (sums/means/variances done in low precision)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Accumulation microscope: 1.0 + N * delta, computed sequentially in different dtypes\n",
    "\n",
    "N = 20000\n",
    "base = 1.0\n",
    "delta = 1e-3\n",
    "\n",
    "steps_i = torch.arange(N + 1, device=device, dtype=torch.int64)\n",
    "expected64 = base + delta * steps_i.double()\n",
    "\n",
    "rows = []\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "every = max(1, (N + 1) // 800)\n",
    "\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32, torch.float64]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    deltas = torch.full((N,), delta, device=device, dtype=dt)\n",
    "    start = torch.tensor([base], device=device, dtype=dt)\n",
    "    cs = torch.cat([start, deltas]).cumsum(0)\n",
    "    err = (cs.double() - expected64).cpu().numpy()\n",
    "\n",
    "    label = str(dt).replace(\"torch.\", \"\")\n",
    "    plt.plot(steps_i.cpu().numpy()[::every], err[::every], label=label, alpha=0.85)\n",
    "\n",
    "    final = float(cs[-1].cpu())\n",
    "    rows.append({\n",
    "        \"dtype\": label,\n",
    "        \"final\": f\"{final:.6f}\",\n",
    "        \"expected\": f\"{float(expected64[-1].cpu()):.6f}\",\n",
    "        \"abs_err\": f\"{abs(final - float(expected64[-1].cpu())):.3e}\",\n",
    "    })\n",
    "\n",
    "plt.axhline(0.0, color=\"k\", lw=1, alpha=0.3)\n",
    "plt.title(\"Accumulation error: sequentially adding delta many times\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cumsum(dtype) - reference (float64 arithmetic)\")\n",
    "plt.legend()\n",
    "plt.tight_layout();\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- If the error curve flattens, your increments stopped affecting the accumulator.\")\n",
    "print(\"- This is why AMP promotes certain reductions (like sum/prod and norm stats) to FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LayerNorm-like statistics are especially sensitive:\n",
    "# mean/var of (large offset + small noise) is a classic cancellation problem.\n",
    "\n",
    "M = 200_000\n",
    "x = (torch.randn(M, device=device, dtype=torch.float32) * 0.1) + 1000.0\n",
    "\n",
    "ref_mu = float(x.double().mean())\n",
    "ref_var = float(((x.double() - ref_mu) ** 2).mean())\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    xd = x.to(dt)\n",
    "    mu = xd.mean()\n",
    "    var = ((xd - mu) ** 2).mean()\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"mean\": f\"{float(mu):.6f}\",\n",
    "        \"var\": f\"{float(var):.6e}\",\n",
    "        \"mean_abs_err\": f\"{abs(float(mu) - ref_mu):.3e}\",\n",
    "        \"var_rel_err\": f\"{abs(float(var) - ref_var) / (ref_var + 1e-30):.3e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nThis is a simplified version of why LayerNorm/Softmax reductions are often forced to FP32 under autocast.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0a Why floating-point addition is NOT associative\n",
    "\n",
    "In real-number math, addition is **associative**: $(a + b) + c = a + (b + c)$. In floating-point, this identity breaks because **rounding happens after every operation**.\n",
    "\n",
    "This has a direct practical consequence: **the order in which you sum values affects the result**. When autocast forces reductions to FP32, one reason is this: the same sum computed in FP16 vs FP32 gives different answers because rounding error accumulates differently with fewer mantissa bits.\n",
    "\n",
    "This also means **nondeterministic reduction order** (common in parallel GPU kernels) can cause run-to-run variance in low precision. Knowing this helps debug \"my loss is slightly different every run\" issues."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Non-associativity of floating-point addition\n",
    "\n",
    "# Choose three values where grouping matters\n",
    "a_val, b_val, c_val = 1.0, 1e-4, -1.0\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32, torch.float64]:\n",
    "    a = torch.tensor(a_val, dtype=dt)\n",
    "    b = torch.tensor(b_val, dtype=dt)\n",
    "    c = torch.tensor(c_val, dtype=dt)\n",
    "\n",
    "    lhs = (a + b) + c      # (1 + 1e-4) + (-1)\n",
    "    rhs = a + (b + c)       # 1 + (1e-4 + (-1))\n",
    "\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"(a+b)+c\": f\"{float(lhs):.8e}\",\n",
    "        \"a+(b+c)\": f\"{float(rhs):.8e}\",\n",
    "        \"equal?\": \"YES\" if float(lhs) == float(rhs) else \"NO\",\n",
    "        \"abs_diff\": f\"{abs(float(lhs) - float(rhs)):.2e}\",\n",
    "        \"explanation\": \"\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"  (a+b)+c: first computes 1.0 + 1e-4. In FP16, this is just 1.0 (addition trap).\")\n",
    "print(\"           Then 1.0 + (-1.0) = 0.0. The 1e-4 is completely lost.\")\n",
    "print(\"  a+(b+c): first computes 1e-4 + (-1.0) = -0.9999. No precision loss here.\")\n",
    "print(\"           Then 1.0 + (-0.9999) = 1e-4 (approximately). The value survives!\")\n",
    "print()\n",
    "print(\"Practical lesson: summation order matters in low precision.\")\n",
    "print(\"This is one reason parallel GPU reductions can give different results than sequential ones,\")\n",
    "print(\"and why autocast promotes large reductions to FP32 where the effect is negligible.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0b Catastrophic cancellation: when subtraction destroys information\n",
    "\n",
    "**Catastrophic cancellation** occurs when you subtract two nearly-equal numbers. The leading significant digits cancel, leaving only the noisy trailing digits. In low precision, there are fewer trailing digits to begin with, so the result can be almost entirely noise.\n",
    "\n",
    "This is exactly what happens in **variance computation**: $\\text{Var}(X) = E[X^2] - (E[X])^2$. If the mean is large relative to the standard deviation, both $E[X^2]$ and $(E[X])^2$ are large and nearly equal. The subtraction amplifies the rounding error in each term.\n",
    "\n",
    "**Why and how the amplification works.** Every floating-point format stores a number with a fixed number of significant digits (the mantissa). FP32 gets ~7 decimal digits of precision, BF16 ~2.4, FP16 ~3.3. Anything beyond that is rounded away *before the subtraction ever happens*.\n",
    "\n",
    "Consider our test case: data with mean $\\mu = 10{,}000$ and $\\sigma = 0.1$, so the true variance is $0.01$.\n",
    "\n",
    "The naive formula needs to compute:\n",
    "\n",
    "$$E[X^2] - (E[X])^2 \\;\\approx\\; \\underbrace{100{,}000{,}000.01}_{9 \\text{ significant digits}} - \\underbrace{100{,}000{,}000.00}_{9 \\text{ significant digits}} = 0.01$$\n",
    "\n",
    "Both terms are $\\sim\\!10^8$, but their difference is $\\sim\\!10^{-2}$ — a factor of $10^{10}$ smaller. To recover the answer you'd need **10** significant decimal digits. But FP32 only has ~7, and BF16 only has ~2.4. So by the time the subtraction happens, both operands have already been rounded *identically* in their leading digits, and the answer is built from whatever noise the rounding left in the trailing digits.\n",
    "\n",
    "Concretely in **FP32** (~7 significant digits):\n",
    "- $E[X^2]$ is stored as something like $1.000000_{\\;?} \\times 10^8$ — the $0.01$ part falls below the precision floor\n",
    "- $(E[X])^2$ is stored as $1.000000_{\\;?} \\times 10^8$ — with its own, different rounding error\n",
    "- Subtraction cancels all 7 leading digits, leaving a result that's entirely the *difference of two rounding errors*\n",
    "- In the demo this gives $-8.0$ instead of $0.01$ — a **negative variance**, which is mathematically impossible. That minus sign is the hallmark of catastrophic cancellation.\n",
    "\n",
    "In **BF16** (~2.4 significant digits): both terms round to *exactly the same value*, so the subtraction returns $0$.\n",
    "\n",
    "In **FP16**: $10{,}000^2 = 10^8$ exceeds the FP16 max of $65{,}504$, so $E[X^2]$ overflows to `inf` and the result is `nan`.\n",
    "\n",
    "This is the mechanical reason **LayerNorm and BatchNorm statistics need FP32** under autocast.\n",
    "\n",
    "The \"safe\" formula sidesteps the problem by subtracting the mean *first*: $\\text{Var}(X) = E[(X - \\mu)^2]$. The centered values $(x_i - \\mu)$ are $\\sim\\!0.1$, so their squares are $\\sim\\!0.01$ — numbers that comfortably fit in any format's precision. No large-minus-large subtraction ever occurs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Catastrophic cancellation in variance computation\n",
    "\n",
    "def variance_naive(x):\n",
    "    # Var(X) = E[X^2] - E[X]^2 -- catastrophic cancellation when E[X] >> std(X)\n",
    "    return (x ** 2).mean() - x.mean() ** 2\n",
    "\n",
    "def variance_safe(x):\n",
    "    # Var(X) = E[(X - E[X])^2] -- avoids large-number subtraction\n",
    "    return ((x - x.mean()) ** 2).mean()\n",
    "\n",
    "# Test with offset data: mean ≈ 10000, std ≈ 0.1\n",
    "set_seed(0)\n",
    "x_fp32 = (torch.randn(50_000, device=device) * 0.1 + 10_000.0).float()\n",
    "ref_var = float(variance_safe(x_fp32.double()))\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    x = x_fp32.to(dt)\n",
    "    v_naive = float(variance_naive(x))\n",
    "    v_safe = float(variance_safe(x))\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"naive_var (E[X²]-E[X]²)\": f\"{v_naive:.6e}\",\n",
    "        \"safe_var (E[(X-μ)²])\": f\"{v_safe:.6e}\",\n",
    "        \"reference (FP64)\": f\"{ref_var:.6e}\",\n",
    "        \"naive_rel_err\": f\"{abs(v_naive - ref_var) / (ref_var + 1e-30):.2e}\",\n",
    "        \"safe_rel_err\": f\"{abs(v_safe - ref_var) / (ref_var + 1e-30):.2e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"  The naive formula (E[X²] - E[X]²) catastrophically cancels in FP16/BF16 because\")\n",
    "print(\"  E[X²] ≈ 1e8 and E[X]² ≈ 1e8, but their difference is only ~0.01.\")\n",
    "print(\"  Subtracting two huge nearly-equal numbers amplifies the rounding error.\")\n",
    "print()\n",
    "print(\"  The safe formula (E[(X-μ)²]) subtracts the mean FIRST, so the squared values\")\n",
    "print(\"  are small (~0.01) and there's no cancellation.\")\n",
    "print()\n",
    "print(\"  PyTorch's LayerNorm/BatchNorm use the safe formula internally — but even so,\")\n",
    "print(\"  autocast runs them in FP32 because the intermediate accumulations still benefit\")\n",
    "print(\"  from higher precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0c Kahan summation: the compensated-sum fix\n",
    "\n",
    "If naive accumulation loses precision, can we do better without just using FP32?\n",
    "\n",
    "**Kahan summation** (compensated summation) tracks a separate \"error compensation\" variable that captures the rounding error from each addition and feeds it back into the next step. It's like an accountant who keeps a running tally of all the pennies that got rounded off.\n",
    "\n",
    "```\n",
    "running_sum = 0.0\n",
    "compensation = 0.0        # tracks accumulated rounding error\n",
    "\n",
    "for x in values:\n",
    "    y = x - compensation   # add back what was lost last time\n",
    "    t = running_sum + y    # this addition might round\n",
    "    compensation = (t - running_sum) - y   # what got lost this time\n",
    "    running_sum = t\n",
    "```\n",
    "\n",
    "Some high-performance kernels internally use compensated summation when operating in low precision. Understanding it helps you appreciate what \"accumulate in FP32\" really means: it's the brute-force version of the same idea (just use more bits instead of being clever about rounding)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Kahan summation vs naive summation in low precision\n",
    "\n",
    "def kahan_sum(values, dtype):\n",
    "    # Compensated summation in the given dtype.\n",
    "    s = torch.tensor(0.0, dtype=dtype)\n",
    "    c = torch.tensor(0.0, dtype=dtype)   # compensation for lost low-order bits\n",
    "    for v in values:\n",
    "        y = v.to(dtype) - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y     # captures the rounding error\n",
    "        s = t\n",
    "    return float(s)\n",
    "\n",
    "def naive_sum(values, dtype):\n",
    "    # Simple sequential sum in the given dtype.\n",
    "    s = torch.tensor(0.0, dtype=dtype)\n",
    "    for v in values:\n",
    "        s = s + v.to(dtype)\n",
    "    return float(s)\n",
    "\n",
    "# Sum 10,000 small values: 1e-3 each → expected total = 10.0\n",
    "N = 10_000\n",
    "values = [torch.tensor(1e-3)] * N\n",
    "expected = 10.0\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    n_sum = naive_sum(values, dt)\n",
    "    k_sum = kahan_sum(values, dt)\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"naive_sum\": f\"{n_sum:.6f}\",\n",
    "        \"kahan_sum\": f\"{k_sum:.6f}\",\n",
    "        \"expected\": f\"{expected:.6f}\",\n",
    "        \"naive_err\": f\"{abs(n_sum - expected):.4e}\",\n",
    "        \"kahan_err\": f\"{abs(k_sum - expected):.4e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKahan summation recovers much of the precision lost by naive accumulation.\")\n",
    "print(\"In practice, 'accumulate in FP32' (what Tensor Cores and autocast do) achieves\")\n",
    "print(\"the same goal with less complexity: you just use a wider accumulator register.\")\n",
    "print(\"But Kahan summation shows that the problem IS solvable in low precision —\")\n",
    "print(\"it's just not worth the extra operations when FP32 accumulators are available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Where does exp() overflow by dtype?\n",
    "\n",
    "x = torch.linspace(-20, 20, 400, device=device)\n",
    "\n",
    "def safe_exp(x, dtype):\n",
    "    return torch.exp(x.to(dtype)).to(torch.float32).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, label in [(torch.float32, \"FP32\"), (torch.bfloat16, \"BF16\")]:\n",
    "    y = safe_exp(x, dt)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y, 1e-30, 1e30)), label=label)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    y16 = safe_exp(x, torch.float16)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y16, 1e-30, 1e30)), label=\"FP16\")\n",
    "\n",
    "plt.axhline(np.log10(65504), color=\"r\", linestyle=\"--\", alpha=0.5, label=\"FP16 max (65504)\")\n",
    "plt.title(\"log10(exp(x)) computed in different dtypes — FP16 overflows early\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(exp(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Loss functions are \"log-sum-exp machines\" (and dtype matters)\n",
    "\n",
    "Many deep learning losses contain exponentials and logs. A classic example:\n",
    "\n",
    "$$\\log(1 + e^x) \\quad \\text{(softplus)}$$\n",
    "\n",
    "- The naive formula overflows quickly in FP16 (for $x > 11$, $e^x > 65504$).\n",
    "- Stable implementations (e.g., `F.softplus`) avoid overflow by rewriting the expression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softplus across dtypes\n",
    "\n",
    "def naive_softplus(x):\n",
    "    return torch.log1p(torch.exp(x))\n",
    "\n",
    "x = torch.linspace(-80, 80, 2000, device=device)\n",
    "ref = F.softplus(x.double()).float()  # high-precision reference\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    y_naive = naive_softplus(x.to(dt)).float()\n",
    "    y_stable = F.softplus(x.to(dt)).float()\n",
    "    err_naive = (y_naive - ref).abs().cpu().numpy()\n",
    "    err_stable = (y_stable - ref).abs().cpu().numpy()\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_naive + 1e-12), label=f\"naive {dt}\")\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_stable + 1e-12), ls=\"--\", label=f\"stable {dt}\")\n",
    "\n",
    "plt.title(\"Softplus error: naive vs stable implementation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(|error|) vs FP64 reference\")\n",
    "plt.legend(ncols=2)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Softmax: the overflow trap and the stability rewrite\n",
    "\n",
    "Softmax is everywhere in transformers (attention). Naive softmax:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "This can overflow in low precision because $e^x$ explodes quickly. The stable rewrite subtracts the max:\n",
    "\n",
    "$$\\text{softmax}(x) = \\text{softmax}(x - \\max(x))$$\n",
    "\n",
    "PyTorch's `F.softmax` uses a stable implementation. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softmax\n",
    "\n",
    "def naive_softmax(x, dim=-1):\n",
    "    ex = torch.exp(x)\n",
    "    return ex / ex.sum(dim=dim, keepdim=True)\n",
    "\n",
    "logits = torch.tensor([0.0, 20.0, 40.0, 80.0], device=device)\n",
    "ref = F.softmax(logits.double(), dim=0).float()\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    x = logits.to(dt)\n",
    "    try:\n",
    "        naive = naive_softmax(x, dim=0).float()\n",
    "        naive_ok = bool(torch.isfinite(naive).all().item())\n",
    "    except Exception:\n",
    "        naive = torch.full_like(ref, float(\"nan\"))\n",
    "        naive_ok = False\n",
    "    stable = F.softmax(x, dim=0).float()\n",
    "    stable_ok = bool(torch.isfinite(stable).all().item())\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt),\n",
    "        \"naive_finite\": naive_ok,\n",
    "        \"stable_finite\": stable_ok,\n",
    "        \"max_abs_err(stable vs ref)\": f\"{float((stable - ref).abs().max()):.2e}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Cross-entropy: the loss function that motivates half of autocast's policy\n",
    "\n",
    "Cross-entropy is the workhorse loss for classification and language modeling. Its internals combine the two most dangerous operations we've seen:\n",
    "\n",
    "$$\\text{CE}(x, y) = -\\log\\left(\\frac{e^{x_y}}{\\sum_j e^{x_j}}\\right) = -x_y + \\log\\sum_j e^{x_j}$$\n",
    "\n",
    "The $\\log\\sum\\exp$ term involves:\n",
    "- **Exponentiation** (overflow risk: $e^{11} > 65504$, the FP16 max)\n",
    "- **Summation over a large vocabulary** (accumulation risk: summing 50,000+ terms in FP16)\n",
    "- **Logarithm** (underflow risk: $\\log(0)$ if softmax probabilities flush to zero)\n",
    "\n",
    "PyTorch's `F.cross_entropy` uses the **log-sum-exp trick** (subtract max before exponentiating), making it numerically stable. Autocast forces it to FP32 because even the stable version benefits from FP32 accumulation over large vocabularies.\n",
    "\n",
    "Let's see what happens at different logit scales — simulating early training (small logits) vs. late training or badly-initialized models (large logits)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cross-entropy loss: naive vs stable implementation across dtypes and logit scales\n",
    "\n",
    "set_seed(0)\n",
    "seq_len_ce = 128\n",
    "vocab_ce = 1000\n",
    "target_ce = torch.randint(0, vocab_ce, (seq_len_ce,), device=device)\n",
    "\n",
    "logit_scales = [1.0, 5.0, 10.0, 20.0, 50.0]\n",
    "\n",
    "rows = []\n",
    "for scale in logit_scales:\n",
    "    logits_base = torch.randn(seq_len_ce, vocab_ce, device=device) * scale\n",
    "\n",
    "    for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "        if not supports_dtype_on_device(dt, device):\n",
    "            continue\n",
    "        logits_dt = logits_base.to(dt)\n",
    "\n",
    "        # Naive: manual log(softmax(x)) — no max subtraction\n",
    "        try:\n",
    "            ex = torch.exp(logits_dt)\n",
    "            sm = ex / ex.sum(dim=-1, keepdim=True)\n",
    "            naive_loss = float(-torch.log(sm[torch.arange(seq_len_ce, device=device), target_ce] + 1e-12).mean())\n",
    "            naive_ok = math.isfinite(naive_loss)\n",
    "        except Exception:\n",
    "            naive_loss = float(\"inf\")\n",
    "            naive_ok = False\n",
    "\n",
    "        # Stable: F.cross_entropy (always runs computation in >= the input dtype)\n",
    "        ref_loss = float(F.cross_entropy(logits_base.float(), target_ce))\n",
    "        try:\n",
    "            stable_loss = float(F.cross_entropy(logits_dt.float(), target_ce))\n",
    "        except Exception:\n",
    "            stable_loss = float(\"nan\")\n",
    "\n",
    "        rows.append({\n",
    "            \"logit_scale\": scale,\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"naive_CE\": f\"{naive_loss:.4f}\" if naive_ok else \"inf/nan\",\n",
    "            \"stable_CE (F.cross_entropy)\": f\"{stable_loss:.4f}\",\n",
    "            \"naive_finite?\": \"YES\" if naive_ok else \"NO\",\n",
    "            \"stable_error_vs_fp32\": f\"{abs(stable_loss - ref_loss):.2e}\",\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"  - At scale>=10, naive cross-entropy in FP16 produces inf (exp overflows)\")\n",
    "print(\"  - F.cross_entropy uses log-sum-exp with max subtraction → always finite\")\n",
    "print(\"  - Autocast forces cross_entropy to FP32 because the log-sum-exp reduction\")\n",
    "print(\"    over large vocabularies (50k+ for LLMs) benefits from FP32 accumulation\")\n",
    "print(\"  - This is why you should NEVER compute loss outside the autocast context\")\n",
    "print(\"    and then manually cast — let autocast handle the precision routing\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 What AMP actually is\n",
    "\n",
    "In PyTorch, AMP is two complementary tools:\n",
    "\n",
    "1. **`autocast`** (forward + loss)\n",
    "   - A context manager that applies a **per-operation dtype policy**.\n",
    "   - It *temporarily* casts inputs/weights for each operation. It does **not** permanently change model parameters.\n",
    "   - Matmuls/linear → lower precision. Softmax/layernorm/losses → FP32.\n",
    "\n",
    "2. **`GradScaler`** (backward + optimizer step)\n",
    "   - Primarily for **FP16 training** (BF16 usually doesn't need it).\n",
    "   - Multiplies loss by a scale factor $S$ before backward → gradients are $S\\times$ larger → fewer underflow to zero.\n",
    "   - Before optimizer step, divides gradients by $S$. If overflow is detected (`inf`/`nan`), skips the step and reduces $S$.\n",
    "\n",
    "**Clean mental model:**\n",
    "- `autocast` protects you from **bad forward dtypes**.\n",
    "- `GradScaler` protects you from **bad backward magnitudes** (FP16 gradient underflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The canonical AMP training loop (four conceptual changes)\n",
    "\n",
    "Start with FP32 training:\n",
    "```python\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "logits = model(x)\n",
    "loss = loss_fn(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "AMP adds four things:\n",
    "```python\n",
    "scaler = GradScaler()\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "with autocast(device_type=\"cuda\", dtype=torch.float16):   # 1. wrap forward + loss\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "scaler.scale(loss).backward()    # 2. scale loss before backward\n",
    "scaler.step(optimizer)           # 3. unscale + check for inf/nan + step\n",
    "scaler.update()                  # 4. adjust scale factor\n",
    "```\n",
    "\n",
    "That's the \"small code change\" people talk about. But the *reason* it works is the theory above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Master weights and optimizer state (why \"just casting the model\" is not the same)\n",
    "\n",
    "There are **three** numeric objects in training that matter:\n",
    "\n",
    "1. **Parameters** (weights) — used in forward/backward\n",
    "2. **Gradients** — produced by backward\n",
    "3. **Optimizer state** (e.g., Adam's first and second moment estimates $m_t, v_t$) — long-lived accumulators\n",
    "\n",
    "A classic mixed precision recipe:\n",
    "- Keep a **master copy of weights in FP32**.\n",
    "- Do forward/backward in FP16/BF16 where safe.\n",
    "- Maintain optimizer state (Adam moments) in FP32.\n",
    "\n",
    "**Why FP32 master weights?**\n",
    "\n",
    "Because 16-bit formats have coarse spacing. A small update $\\Delta w = \\text{lr} \\times \\text{grad}$ can be *below the ULP* at the magnitude of $w$, so the weight never changes. We showed this in the `1 + 1e-4` demo above.\n",
    "\n",
    "Over many steps, these tiny updates *accumulate* in FP32 and eventually become large enough to appear in the 16-bit copy. This is the key insight from the Micikevicius et al. (2017) paper.\n",
    "\n",
    "**Why FP32 optimizer state?**\n",
    "\n",
    "Adam's moments are exponential moving averages. They accumulate information over the entire training run. Even small rounding errors compound over thousands of steps. Keeping moments in FP32 prevents this drift.\n",
    "\n",
    "**Memory implication:**\n",
    "\n",
    "Mixed precision doesn't eliminate FP32 — it just limits FP32 to parameters + optimizer state (which is fixed-size), while saving on activations (which scale with batch size and sequence length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Autocast is an operator policy (not a global cast)\n",
    "\n",
    "Autocast does **not** \"turn the whole model into FP16\". Instead it applies a per-operation policy:\n",
    "\n",
    "| Policy | Operations | Rationale |\n",
    "|---|---|---|\n",
    "| **Lower precision** (FP16/BF16) | `linear`, `matmul`, `mm`, `bmm`, convolutions | Compute-bound → Tensor Core speedup |\n",
    "| **Force FP32** | `softmax`, `layer_norm`, `log_softmax`, `mse_loss`, `cross_entropy`, `sum`, `prod`, `exp`, `log`, ... | Numerically sensitive: overflow/underflow/accumulation risk |\n",
    "| **Promote to widest** | Binary ops when inputs differ | If one input is FP32, the op runs in FP32 |\n",
    "| **Pass-through** | `relu`, `dropout`, `max`, `min`, `mean`, ... | Element-wise, no numeric risk; output matches input dtype |\n",
    "\n",
    "The exact policy is PyTorch-version-dependent. In Section 3 we will probe it empirically on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 summary\n",
    "\n",
    "| Concept | Key takeaway |\n",
    "|---|---|\n",
    "| **FP16** | Better precision than BF16, but narrow range → needs loss scaling and careful op policies |\n",
    "| **BF16** | FP32-like range → often trains without scaling, but coarse precision → reductions need care |\n",
    "| **TF32** | Your \"FP32 baseline\" on Ampere+ GPUs may secretly be TF32 (10-bit mantissa) for matmuls |\n",
    "| **AMP** | `autocast` (forward op policy) + `GradScaler` (backward magnitude control) |\n",
    "| **Master weights** | Keep FP32 copy for updates; prevents stagnation from ULP rounding |\n",
    "| **Optimizer state** | Keep in FP32; long-horizon accumulation is precision-sensitive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 — What the literature says\n",
    "\n",
    "This section is intentionally *written*: the point is to build a paper-and-doc-driven mental model that you can carry into real training code.\n",
    "\n",
    "No experiments here — only explanations. Think of this section as: \"what each source contributes, and how it maps to PyTorch AMP.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 A reading map\n",
    "\n",
    "| Source | What it gives you | Maps to |\n",
    "|---|---|---|\n",
    "| Gupta et al. (2015) | Why limited precision can still train; stochastic rounding + scaling intuition | Section 1.3 (rounding/accumulation), Section 3 debugging |\n",
    "| Micikevicius et al. (2017) | The canonical mixed precision recipe (master weights + loss scaling) | Sections 1.4–1.6, scaling experiments |\n",
    "| Kalamkar et al. (2019) | BF16 bit-level analysis, proof that BF16 avoids FP16's underflow | Section 1.2, BF16 training runs |\n",
    "| NVIDIA mixed precision guidance | Engineering intuition + failure modes | Underflow/overflow + \"sensitive ops in FP32\" |\n",
    "| PyTorch `torch.amp` docs | The actual API + gotchas | `autocast` + `GradScaler` loops in Section 3 |\n",
    "| PyTorch autocast op reference | *The* per-op policy | Probed empirically in Section 3.2 |\n",
    "| PyTorch AMP examples page | Exact operational order for edge cases (accumulation, penalty, multi-optimizer) | Section 2.9.1, Section 3.10 |\n",
    "| Rajbhandari et al. (2020) — ZeRO | Memory breakdown of mixed-precision training at scale | Optimizer state precision, Section 2.6 |\n",
    "| Distributed training docs (FSDP/ZeRO/DeepSpeed) | Where dtypes live in large systems | Section 2.7 |\n",
    "| FP8 literature (e.g., NVIDIA) | Why FP8 needs scaling (E4M3 vs E5M2) + where it fits vs AMP | Section 1 tables, Section 3 practical guidance |\n",
    "| PyTorch float8 + FSDP2 blogs/tutorials | What actually closes the convergence gap at scale (rowwise scaling, float8 all-gather) | Section 2.11.1 |\n",
    "| NVIDIA Transformer Engine primer | Blackwell-era microscaling formats (MXFP8/NVFP4) and stochastic rounding direction | Section 2.11.2 |\n",
    "| Dettmers et al. (8-bit optimizers) | Optimizer-state precision as the next bottleneck after AMP | Section 2.6 memory discussion |\n",
    "\n",
    "If you only read one thing: read the Micikevicius paper, then read the PyTorch autocast op reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Micikevicius et al. (2017): *Mixed Precision Training*\n",
    "\n",
    "> arXiv:1710.03740 — ICLR 2018\n",
    "\n",
    "This is the foundational paper for everything in this notebook. It introduced the three-part recipe that all modern AMP implementations are based on.\n",
    "\n",
    "**The problem:** FP16 arithmetic is fast (2–8x throughput on Tensor Cores) and memory-efficient (half the bytes), but naively training in FP16 breaks. Models either diverge, produce NaN losses, or silently stagnate. The paper identifies three distinct failure modes:\n",
    "\n",
    "1. **Weight update stagnation.** When a weight $w$ is stored in FP16, the update $\\Delta w = \\eta \\cdot g$ can be smaller than the ULP (unit in the last place) at $|w|$. The update is rounded away and the weight never changes. The paper's solution: maintain an FP32 \"master copy\" of all parameters. Forward and backward compute use the FP16 cast, but the actual parameter update happens in FP32 where the precision is sufficient to register small changes. The updated FP32 value is then cast back to FP16 for the next forward pass.\n",
    "\n",
    "2. **Gradient underflow.** Even with FP32 master weights, gradients themselves are computed in FP16 during the backward pass. FP16's smallest normalized number is approximately $6 \\times 10^{-5}$. The paper's analysis of gradient histograms across several production models (image classifiers, speech models, generative models) shows that a significant fraction of gradient values fall below this threshold. Once they underflow to zero, the update signal is lost. The solution: **loss scaling**. Multiply the scalar loss by a large constant $S$ before calling `backward()`. Since backpropagation is a chain of multiplications, every gradient in the network is scaled up by $S$. After backward, divide gradients by $S$ before the optimizer step. If $S$ is chosen well, gradients that would have underflowed are now safely within FP16's representable range, and the division by $S$ restores correct magnitudes. The paper demonstrates that a fixed $S$ works for many models, but notes that $S$ too large can cause gradient *overflow*. This motivates **dynamic loss scaling**: start with a large $S$, and if `inf`/`nan` gradients are detected, skip the update and reduce $S$. If training is stable, periodically increase $S$ to maximize headroom.\n",
    "\n",
    "3. **Accumulation error in reductions.** Large dot products and reductions (e.g., summing thousands of values) accumulate rounding errors that compound in FP16. The paper recommends performing these accumulations in FP32, even when the operands are FP16. Modern Tensor Cores implement this: they accept FP16 inputs but accumulate in FP32.\n",
    "\n",
    "**What to remember:** AMP is not a single trick. It is the *combination* of (a) per-op precision policies, (b) loss scaling, and (c) FP32 master weights/optimizer state. Removing any one of these can cause training to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kalamkar et al. (2019): *A Study of BFLOAT16 for Deep Learning Training*\n",
    "\n",
    "> arXiv:1905.12322\n",
    "\n",
    "This paper provides the empirical and theoretical justification for BF16 as a training format.\n",
    "\n",
    "**The key insight** is architectural: BF16 uses **8 exponent bits** (same as FP32), giving it the same dynamic range — roughly $10^{-38}$ to $10^{38}$. This means BF16 can represent the same extreme magnitudes as FP32. The gradient underflow problem that plagues FP16 (5-bit exponent → range only $10^{-5}$ to $6.5 \\times 10^4$) essentially does not exist for BF16.\n",
    "\n",
    "**The tradeoff:** BF16 sacrifices mantissa bits (7 vs FP16's 10 vs FP32's 23). This means worse *precision* per individual value. But the paper demonstrates empirically that deep neural networks are remarkably robust to precision loss — they are much more sensitive to *range* loss. Training with BF16 across a variety of models (ResNets, Transformers, LSTMs) converges to the same final accuracy as FP32, often without any loss scaling at all.\n",
    "\n",
    "**Conversion simplicity:** Since BF16 and FP32 share the same exponent field, conversion is trivial: truncate (or round) the bottom 16 mantissa bits. FP32 → FP16 conversion is harder because the exponent field must also be narrowed, risking overflow or underflow in the conversion itself.\n",
    "\n",
    "**Practical implication for AMP:** BF16 autocast is often a \"drop-in\" replacement that doesn't require a `GradScaler`. This is explicitly confirmed by modern frameworks: DeepSpeed documentation states, \"Training with bfloat16 does not require loss scaling.\"\n",
    "\n",
    "**When BF16 can still struggle:** Precision-sensitive accumulations (layernorm statistics, attention score normalization, large batch reductions) can still degrade with BF16's 7-bit mantissa. This is why autocast routes these operations to FP32 even in BF16 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NVIDIA mixed precision guidance (the engineering perspective)\n",
    "\n",
    "NVIDIA's developer blog and documentation provide the practitioner's view of mixed precision. The key engineering decomposition:\n",
    "\n",
    "1. **Tensor Core compute** wants FP16/BF16 inputs. Modern GPUs (Volta+) have dedicated matrix-multiply-accumulate units that operate on 16-bit inputs with FP32 accumulation. These deliver 2–8$\\times$ the throughput of FP32 CUDA cores for matmuls, which dominate deep learning compute.\n",
    "\n",
    "2. **Dimension alignment matters.** Tensor Cores process tiles of fixed size (typically 8 or 16 elements). If your tensor dimensions aren't multiples of 8, you either waste hardware or fall back to slower code paths. This is a practical detail that affects whether you actually see the theoretical speedup.\n",
    "\n",
    "3. **Memory bandwidth matters as much as compute.** 16-bit formats halve the bytes transferred between GPU memory (HBM) and compute cores (SMs). For memory-bandwidth-bound models (which many transformer models are at inference time), this alone can yield significant speedup even without Tensor Core benefits.\n",
    "\n",
    "4. **Some ops are numerically sensitive.** NVIDIA's guidance identifies the same ops that PyTorch's autocast routes to FP32: exponentials, logs, softmax, normalization statistics, and large reductions. The rationale is the same: overflow/underflow risk and accumulation error.\n",
    "\n",
    "The practical outcome is the \"few lines of code\" AMP recipe. But the engineering reason it works is the numeric analysis in the theory section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BF16: \"FP32 range, fewer bits of precision\"\n",
    "\n",
    "BF16 was originally designed for Google's TPUs. The design goal was explicit: make it possible to train neural networks in 16-bit formats *without* the underflow problems of FP16.\n",
    "\n",
    "**The design decision:** Given 16 bits, how should you split them between exponent and mantissa?\n",
    "\n",
    "- FP16 (IEEE): 5 exponent + 10 mantissa → good precision, but range only $[6 \\times 10^{-5}, 6.5 \\times 10^4]$.\n",
    "- BF16 (Google/Intel): 8 exponent + 7 mantissa → FP32-like range $[10^{-38}, 3.4 \\times 10^{38}]$, coarser precision.\n",
    "\n",
    "For training, range wins. Gradients span many orders of magnitude, and losing any of them to underflow is worse than representing them imprecisely. Networks are fundamentally robust to noise (they're trained with stochastic gradient descent after all), but they cannot learn from zero gradients.\n",
    "\n",
    "**In practice, for many transformer trainings on modern GPUs:**\n",
    "- BF16 autocast is \"drop-in\" without loss scaling.\n",
    "- FP16 autocast typically needs a `GradScaler`.\n",
    "- Both still benefit from FP32 master weights and FP32 optimizer state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 PyTorch AMP docs: the operator policy is the decoder ring\n",
    "\n",
    "A common mistake is to treat autocast like a global switch (\"my model is now FP16\"). But autocast is really a **per-operation dispatch table**:\n",
    "\n",
    "- Some ops are eligible for lower precision (FP16/BF16). These are the compute-heavy ops where Tensor Cores give speedup.\n",
    "- Some ops are forced to FP32. These are the numerically sensitive ops where lower precision would cause training failures.\n",
    "- Some ops promote to the widest input type. These are binary operations where mixed-dtype inputs would be ambiguous.\n",
    "- Unlisted ops run in whatever dtype they receive (pass-through).\n",
    "\n",
    "**The key documentation page** is the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). It lists every CUDA op and its autocast behavior. This is the authoritative answer to \"why did this op run in FP32?\" — faster and more reliable than any blog post.\n",
    "\n",
    "**A common debugging pattern:** when AMP training fails, the first thing to check is which ops are running in which dtype. The dtype hooks we build in Section 3 make this visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Rajbhandari et al. (2020): ZeRO and optimizer state precision\n",
    "\n",
    "> arXiv:1910.02054 — *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models*\n",
    "\n",
    "While ZeRO is primarily about distributed training, its memory analysis (Section 2) is essential for understanding where mixed precision fits.\n",
    "\n",
    "**The memory breakdown for a model with $\\Psi$ parameters trained with Adam in mixed precision:**\n",
    "\n",
    "| Component | Dtype | Bytes per parameter |\n",
    "|---|---|---|\n",
    "| FP16 parameters (forward/backward) | FP16 | 2 |\n",
    "| FP16 gradients | FP16 | 2 |\n",
    "| FP32 master weights | FP32 | 4 |\n",
    "| FP32 Adam first moment ($m$) | FP32 | 4 |\n",
    "| FP32 Adam second moment ($v$) | FP32 | 4 |\n",
    "| **Total** | | **16 bytes/param** |\n",
    "\n",
    "For a 7B parameter model: $7 \\times 10^9 \\times 16 = 112$ GB just for parameters + optimizer state — before any activations.\n",
    "\n",
    "**Key insight:** Mixed precision doesn't eliminate FP32 from the system. It moves FP32 to where it's needed (optimizer state, master weights) and uses 16-bit where it's safe (activations, compute). The memory savings come primarily from activations (which scale with batch size and sequence length), not from the fixed-size parameter/optimizer memory.\n",
    "\n",
    "**Practical takeaway:** \"AMP\" in a distributed stack is not only about autocast during forward/backward. It is about **where each tensor lives and in what dtype** across the entire training loop: parameters, gradients, optimizer state, communication buffers, and activation checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 LLM training stacks (FSDP / ZeRO / DeepSpeed): where precision choices multiply\n",
    "\n",
    "At LLM scale, training systems shard parameters, gradients, and optimizer state across multiple GPUs. Mixed precision gets more complicated because dtype decisions affect:\n",
    "\n",
    "- **Parameter storage:** \"working\" params in BF16/FP16 for forward/backward; FP32 master copy for updates.\n",
    "- **Gradient communication:** All-reduce operations can accumulate rounding errors. Some stacks reduce in FP32 for stability, others use BF16 + gradient compression.\n",
    "- **Optimizer state:** Typically FP32 (Adam moments need high precision over long training). Some frameworks offer FP16 optimizer states as a memory optimization, but this trades stability for memory.\n",
    "- **Activation checkpointing:** Recomputed activations use the same autocast policy as the original forward pass, but the checkpointing mechanism itself must preserve dtypes correctly.\n",
    "\n",
    "**FSDP mixed precision** (PyTorch): lets you separately control `param_dtype` (for sharded parameters), `reduce_dtype` (for gradient all-reduce), and `buffer_dtype`. FP16 + scaler or BF16 without scaler.\n",
    "\n",
    "**DeepSpeed ZeRO:** explicit config knobs. `fp16.loss_scale = 0` enables dynamic loss scaling; `bf16.enabled = true` with no loss scaling.\n",
    "\n",
    "**The practical lesson:** When debugging AMP issues in distributed training, the dtype of every tensor transfer (parameter broadcast, gradient reduce, optimizer state scatter/gather) is a potential source of numeric problems. This is well beyond the scope of a single `autocast` context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 How to think about the autocast policy (conceptual categories)\n",
    "\n",
    "Autocast decisions fall into a few conceptual buckets. These aren't arbitrary — each traces back to a specific numeric risk:\n",
    "\n",
    "**1. Lower precision eligible (matmul-like ops)**\n",
    "\n",
    "These are the ops where Tensor Cores provide speedup and numeric risk is low. The key property: these operations multiply pairs of values and accumulate in FP32 (on Tensor Cores), so the inputs can be 16-bit without precision loss in the output.\n",
    "\n",
    "**2. Force FP32 (numerically sensitive ops)**\n",
    "\n",
    "Two sub-categories:\n",
    "- **Overflow/underflow risk:** `exp`, `log`, `softmax`, `log_softmax`. These can produce extreme magnitudes.\n",
    "- **Accumulation risk:** `sum`, `prod`, `layer_norm`, `batch_norm`, `cross_entropy`, `mse_loss`. These reduce many values and small rounding errors compound.\n",
    "\n",
    "**3. Promote to widest (binary ops with mixed inputs)**\n",
    "\n",
    "If you add a BF16 tensor to an FP32 tensor, the result is FP32. This prevents silent precision loss when autocast and non-autocast regions interact.\n",
    "\n",
    "**4. Pass-through (element-wise, no numeric risk)**\n",
    "\n",
    "`relu`, `sigmoid`, `tanh`, `dropout`, `max`, `min`, `mean`. These just transform individual values without extreme magnitudes or accumulation. Whatever goes in comes out.\n",
    "\n",
    "**Practitioner's two rules:**\n",
    "- If an op creates very large/small magnitudes ($e^x$, $\\log x$, softmax), it needs FP32.\n",
    "- If an op reduces many values ($\\sum$, variance, cross-entropy), it needs FP32 accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Dynamic loss scaling: what `GradScaler` is doing\n",
    "\n",
    "Loss scaling multiplies the scalar loss by a factor $S$ before calling `backward()`.\n",
    "\n",
    "Since backpropagation is linear (gradients are proportional to the loss), every gradient in the network is multiplied by $S$. This shifts the entire gradient distribution toward larger magnitudes, rescuing values that would have underflowed to zero in FP16.\n",
    "\n",
    "**The protocol:**\n",
    "\n",
    "1. Compute loss normally under autocast.\n",
    "2. Multiply loss by $S$ and call `backward()`.\n",
    "3. Before the optimizer step, **unscale** all gradients by dividing by $S$.\n",
    "4. Check for `inf`/`nan` in gradients:\n",
    "   - If found: **skip** the optimizer step, reduce $S$ (usually halve it).\n",
    "   - If not found: proceed with the step. Periodically increase $S$ (e.g., double it every $N$ successful steps) to maximize headroom.\n",
    "\n",
    "**Why it's safe:** scaling and unscaling cancel out exactly if no overflow occurs. The optimizer sees the same gradients it would have without scaling.\n",
    "\n",
    "**Critical detail:** gradient clipping must happen **after** unscaling. PyTorch's `scaler.unscale_(optimizer)` does the unscale explicitly; `scaler.step(optimizer)` then checks for inf/nan before calling `optimizer.step()`.\n",
    "\n",
    "**Why BF16 doesn't need this:** BF16 has the same exponent range as FP32. Gradients that would be $10^{-10}$ in FP32 are also representable in BF16 (though with less precision). No underflow → no need for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9.1 What the official AMP examples add (beyond the canonical loop)\n",
    "\n",
    "PyTorch's [AMP examples page](https://docs.pytorch.org/docs/stable/notes/amp_examples.html) is the best source for tricky loop mechanics that are easy to get subtly wrong.\n",
    "\n",
    "**1) Gradient accumulation is \"effective-batch scoped\".**\n",
    "\n",
    "During microbatch accumulation, grads should stay **scaled** and the scale factor should remain constant until the effective batch is complete. The correct order is:\n",
    "- scale/backward on each microbatch\n",
    "- unscale once (optional, right before clipping/inspection)\n",
    "- step once\n",
    "- update once\n",
    "\n",
    "If you unscale too early (mid-accumulation), later backwards add scaled grads to already-unscaled grads, and you lose mathematical equivalence.\n",
    "\n",
    "**2) `unscale_` is one-shot per optimizer per step.**\n",
    "\n",
    "The examples explicitly note that calling `scaler.unscale_(optimizer)` twice between optimizer steps raises a runtime error. This matters in larger codebases where clipping/logging utilities may each try to unscale.\n",
    "\n",
    "**3) Multi-loss / multi-optimizer logic is per-loss scaling, per-optimizer stepping, single update.**\n",
    "\n",
    "For multiple losses, call `scaler.scale(loss_i)` on each relevant loss. For multiple optimizers, call `scaler.step(optimizer_i)` for each optimizer, then `scaler.update()` once at the end of the iteration.\n",
    "\n",
    "**4) Gradient penalties with `autograd.grad` require manual inverse scaling.**\n",
    "\n",
    "When using `torch.autograd.grad` to build a penalty term, scale the outputs passed into `autograd.grad`, then manually divide the returned gradients by `scaler.get_scale()` before composing the penalty. Those gradients are not tied to an optimizer yet, so `unscale_` cannot do it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9.2 API modernization and backend portability (`torch.amp`)\n",
    "\n",
    "The modern PyTorch API is device-agnostic:\n",
    "- `torch.amp.autocast(device_type=..., dtype=...)`\n",
    "- `torch.amp.GradScaler(device_type, ...)`\n",
    "\n",
    "The older namespace-specific APIs (`torch.cuda.amp.*`, `torch.cpu.amp.*`) are being deprecated in favor of this unified interface.\n",
    "\n",
    "**Autocast availability is now explicit:** `torch.amp.autocast_mode.is_autocast_available(device_type)` lets you guard backend-specific paths. The docs list support surfaces beyond CUDA/CPU, including backends such as XPU/HPU/MTIA/MAIA.\n",
    "\n",
    "**Autocast state is thread-local.** This is easy to miss in multi-GPU code:\n",
    "- each thread/process needs its own autocast context\n",
    "- in `DataParallel`/multi-GPU-per-process `DistributedDataParallel`, this can affect where autocast is enabled\n",
    "\n",
    "**Nested precision regions are a first-class pattern.** You can use `autocast(enabled=False)` subregions for numerically sensitive code, but tensors produced in outer low-precision regions may need explicit casts when entering the disabled block.\n",
    "\n",
    "**JIT caveat:** the AMP docs still document disabling the JIT autocast pass in some tracing/freeze workflows due to known issues.\n",
    "They also note that dtype mismatch errors inside an autocast-enabled region should be treated as bugs and reported upstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Gupta et al. (2015): *Deep Learning with Limited Numerical Precision*\n",
    "\n",
    "This earlier line of work is worth knowing because it frames low-precision training as a **numerical analysis problem**, not a hardware trick.\n",
    "\n",
    "**The core idea:** training can succeed even when weights/activations/gradients are represented in low precision, *if you control rounding and scaling*.\n",
    "\n",
    "Key takeaways often cited from this family of results:\n",
    "- **Rounding is the enemy.** Deterministic rounding can systematically bias updates. Stochastic rounding can remove that bias at the cost of noise.\n",
    "- **Scaling matters.** Keeping values in a representable dynamic range is the difference between \"noisy training\" and \"dead training\" (underflow to zeros).\n",
    "- **Noise tolerance is real.** SGD is already noisy; in many regimes, extra quantization noise is tolerable *as long as the signal is not destroyed*.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- Autocast is a modern, practical version of \"do low precision where it's safe\".\n",
    "- Loss scaling is a specialized scaling strategy focused on preserving *gradient* signal in FP16.\n",
    "- The accumulation demos in Section 1 (tiny increments and LayerNorm stats) are the same failure modes this literature is trying to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1 Stochastic rounding: why it helps low-precision training\n",
    "\n",
    "One key insight from the Gupta line of work is that **rounding mode matters**.\n",
    "\n",
    "Standard IEEE 754 rounding is **deterministic** (round-to-nearest, ties-to-even). This creates a systematic problem: if a value consistently falls just below a representable grid point, it *always* rounds down. Over many training steps, this introduces a persistent bias — the value drifts away from where it \"should\" be.\n",
    "\n",
    "**Stochastic rounding** instead rounds probabilistically:\n",
    "- If a value $x$ falls between grid points $x_\\text{lo}$ and $x_\\text{hi}$:\n",
    "  - Round to $x_\\text{hi}$ with probability $(x - x_\\text{lo}) / (x_\\text{hi} - x_\\text{lo})$\n",
    "  - Round to $x_\\text{lo}$ otherwise\n",
    "\n",
    "The expected value of the rounded result is $x$ itself — it's an **unbiased** estimator. Over many steps, small updates that would be systematically rounded away under deterministic rounding instead *accumulate on average*.\n",
    "\n",
    "**Connection to AMP:** Modern autocast doesn't use stochastic rounding (it uses standard IEEE rounding). Instead, it achieves a similar effect by keeping accumulations and updates in FP32, where the deterministic rounding error is small enough not to matter. But in FP8 and other extreme low-precision formats, stochastic rounding is actively used in some training frameworks.\n",
    "\n",
    "To keep this entire section purely literature and prose, the runnable stochastic-rounding experiment appears in **Section 3.5.1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 FP8 for deep learning: why it exists and why it's not just \"AMP but smaller\"\n",
    "\n",
    "FP8 is attractive because it can further reduce memory bandwidth and increase tensor-core throughput compared to FP16/BF16. NVIDIA's H100 (Hopper architecture, 2022) and H200 GPUs include dedicated FP8 Tensor Cores that deliver ~2× the throughput of FP16 Tensor Cores.\n",
    "\n",
    "But FP8 is fundamentally different from FP16/BF16:\n",
    "- the representable grid is *much* coarser\n",
    "- range depends strongly on the FP8 variant (commonly summarized as **E4M3** vs **E5M2**)\n",
    "- most practical FP8 training systems rely on **explicit scaling** (per-tensor/per-channel) and carefully chosen accumulation dtypes\n",
    "\n",
    "### E4M3 vs E5M2: two formats for two jobs\n",
    "\n",
    "| Property | **E4M3** (float8_e4m3fn) | **E5M2** (float8_e5m2) |\n",
    "|---|---|---|\n",
    "| Exponent bits | 4 | 5 |\n",
    "| Mantissa bits | 3 | 2 |\n",
    "| Max finite value | 448 | 57,344 |\n",
    "| Smallest normal | ~0.015625 | ~6.1e-5 |\n",
    "| Precision (ULP at 1.0) | 0.125 | 0.25 |\n",
    "| Primary use case | **Forward pass** (more precision) | **Backward pass** (more range for gradients) |\n",
    "\n",
    "The key insight: **E4M3 prioritizes precision** (3 mantissa bits give finer resolution for activations and weights in the forward pass), while **E5M2 prioritizes range** (5 exponent bits give wider dynamic range for gradients in the backward pass, which can span many orders of magnitude).\n",
    "\n",
    "A typical FP8 training recipe uses **E4M3 for forward-pass tensors** and **E5M2 for gradient tensors**.\n",
    "\n",
    "### Per-tensor scaling: why FP8 is fundamentally different from FP16/BF16 AMP\n",
    "\n",
    "With FP16/BF16 AMP, autocast just changes the dtype and the hardware handles the rest. With FP8, the representable range is so narrow that you need an **explicit scale factor per tensor** (or per channel/block) to map your values into the format's sweet spot.\n",
    "\n",
    "This means:\n",
    "1. Before casting a tensor to FP8, compute its `amax` (absolute maximum value).\n",
    "2. Choose a scale factor $S$ such that $\\text{tensor} / S$ fits within the FP8 representable range.\n",
    "3. Store and propagate $S$ alongside the FP8 tensor.\n",
    "4. Undo the scaling when you need the real values back.\n",
    "\n",
    "This makes FP8 closer to **learned quantization with dynamic scaling** than to the drop-in nature of BF16 autocast. Current FP8 training typically uses frameworks like NVIDIA's Transformer Engine or specialized `torch.float8` APIs that manage this scaling metadata automatically.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- AMP (`torch.amp.autocast`) is primarily about FP16/BF16 (and TF32) policies.\n",
    "- FP8 usually requires a specialized kernel stack (often beyond the default autocast) that manages scaling metadata alongside tensors.\n",
    "- Libraries like NVIDIA Transformer Engine integrate FP8 into the training loop with per-tensor delayed scaling (using amax history from previous iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11.1 What changed in 2024: row-wise scaling and float8 + FSDP2\n",
    "\n",
    "Recent PyTorch production writeups added an important correction to early FP8 intuition: **scaling granularity is often the deciding factor for convergence.**\n",
    "\n",
    "From PyTorch's float8/FSDP2 blogs and tutorials:\n",
    "- naive tensor-wise scaling can underperform BF16 at large scale (outlier sensitivity)\n",
    "- **row-wise scaling** substantially reduces quantization error and was reported to recover convergence while improving throughput (roughly 34–43% in reported large-model runs)\n",
    "- pairing float8 compute with **float8 all-gather** in FSDP2 reduces communication bandwidth pressure\n",
    "- stack-level tuning (`torch.compile`, distributed overlap, checkpointing choices) is required for end-to-end gains\n",
    "\n",
    "The practical takeaway is that \"FP8 success\" is not just a dtype flag. It's a systems recipe: scaling strategy + communication dtype + compiler/runtime integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11.2 Beyond Hopper FP8: MXFP8 and NVFP4 (Blackwell direction)\n",
    "\n",
    "NVIDIA's Transformer Engine primer describes where low precision is heading on Blackwell-class systems:\n",
    "\n",
    "- **MXFP8 (microscaling FP8):** block-level scaling metadata instead of a single scale per tensor, reducing sensitivity to outliers and making E4M3 usable in more paths.\n",
    "- **NVFP4:** a 4-bit floating format with additional scaling structure (including fine-grained blocks and higher-precision fallback scales) aimed at pushing training/inference efficiency further.\n",
    "- **Stochastic rounding** becomes more important again at these precisions to avoid systematic update bias.\n",
    "\n",
    "Conceptually, this extends AMP's core idea (\"precision should follow numerical risk\") into a regime where **scaling metadata and rounding mode** are first-class parts of the training graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 8-bit optimizer-state work: the next bottleneck after AMP\n",
    "\n",
    "Classic AMP reduces activation memory and speeds up matmuls, but for large models the **optimizer state** becomes a dominant fixed cost (ZeRO's 16 bytes/parameter for Adam mixed precision — 8 of those bytes are just the two FP32 moment buffers).\n",
    "\n",
    "This motivates a separate line of work: compressing the **optimizer state** (and sometimes gradients) to 8-bit representations while preserving training quality.\n",
    "\n",
    "### Dettmers et al. (2022): Block-wise Quantization\n",
    "\n",
    "The key technique from *8-bit Optimizers via Block-wise Quantization*:\n",
    "\n",
    "1. **Block-wise quantization:** Split each optimizer state tensor (e.g., Adam's $m$ and $v$) into contiguous blocks of **2048 elements**. Each block gets its own normalization constant (the block's absolute maximum), so outliers in one region don't destroy precision for the rest.\n",
    "\n",
    "2. **Dynamic tree quantization:** Instead of uniform quantization (which wastes bins on empty ranges), use a **non-uniform mapping** designed for the actual distribution of optimizer states. Optimizer moments tend to follow heavy-tailed distributions, so more bins are allocated near zero and fewer at the extremes. The mapping is computed via a binary tree structure that adapts to the data distribution.\n",
    "\n",
    "3. **Stable embedding layer:** The embedding layer's optimizer state is particularly sensitive to quantization (sparse updates, high variance). Dettmers et al. propose keeping embedding optimizer states in higher precision or using a stabilized update rule.\n",
    "\n",
    "**Practical result:** ~75% memory reduction for optimizer state (from 8 bytes/param down to ~2 bytes/param for the moment buffers) with negligible accuracy loss across a wide range of tasks and model sizes.\n",
    "\n",
    "### Integration with bitsandbytes\n",
    "\n",
    "The `bitsandbytes` library makes this a near-drop-in replacement:\n",
    "\n",
    "```python\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Drop-in replacement for torch.optim.AdamW\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "The optimizer internally quantizes $m$ and $v$ to 8-bit after each update and dequantizes before the next step. The model parameters and gradients remain in their original precision (FP32 or whatever AMP provides).\n",
    "\n",
    "**How it relates to autocast:**\n",
    "- Autocast is about *compute dtype choice per op* during forward/loss (and indirectly backward).\n",
    "- 8-bit optimizers are about *storage precision* for long-lived optimizer tensors.\n",
    "\n",
    "They are complementary: you can use AMP for activations/compute and 8-bit optimizers to reduce the memory footprint of optimizer state. Combined, they can bring the per-parameter memory cost from 16 bytes (standard Adam) down to ~10 bytes (8-bit optimizer + FP32 params/grads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 summary\n",
    "\n",
    "| Paper / Source | Key contribution |\n",
    "|---|---|\n",
    "| **Gupta et al.** | Limited precision can work when rounding/scaling are designed, not ignored |\n",
    "| **Micikevicius et al.** | The three-part recipe: per-op policies + loss scaling + FP32 master weights |\n",
    "| **Kalamkar et al.** | BF16's 8-bit exponent matches FP32 → no underflow → no loss scaling needed |\n",
    "| **NVIDIA guidance** | Engineering view: Tensor Cores, dimension alignment, bandwidth savings |\n",
    "| **PyTorch AMP docs** | The per-op dispatch table (the \"decoder ring\" for debugging) |\n",
    "| **PyTorch AMP examples** | Correct ordering for accumulation, penalties, and multi-optimizer loops |\n",
    "| **`torch.amp` unified API** | Device-agnostic AMP + backend availability checks + thread-local caveats |\n",
    "| **ZeRO (Rajbhandari et al.)** | Memory breakdown: 16 bytes/param with Adam mixed precision |\n",
    "| **Distributed stacks** | Precision applies to params, grads, optimizer state, and communication separately |\n",
    "| **FP8 literature** | FP8 needs explicit scaling + specialized kernels; not a drop-in autocast dtype |\n",
    "| **PyTorch float8 + FSDP2 work** | Row-wise scaling + float8 collectives are key for practical FP8 convergence/perf |\n",
    "| **Blackwell TE direction** | Microscaling formats (MXFP8/NVFP4) push scaling metadata into the core loop |\n",
    "| **8-bit optimizers** | Optimizer-state precision is the next memory target after AMP |\n",
    "\n",
    "**The single most useful reference for debugging:** the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). Bookmark it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 — Practicalities (experiments + graphs)\n",
    "\n",
    "This section is where we turn everything into measurements.\n",
    "\n",
    "**Principles:**\n",
    "- Prefer experiments that are **small, fast, and explain a single idea**.\n",
    "- Log everything you might need for debugging (loss, grad norms, scaler scale, step time, NaN/inf).\n",
    "- Make results comparable across dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Controlling confounders (TF32, randomness, and fair comparisons)\n",
    "\n",
    "When comparing FP32 vs AMP, you can accidentally compare the wrong thing:\n",
    "\n",
    "1. **TF32 on Ampere+:** Many FP32 matmuls use TF32 internally (10-bit mantissa). Your \"FP32 baseline\" may not be strict FP32 precision.\n",
    "2. **Randomness:** Dropout, data sampling, and nondeterministic kernels introduce run-to-run variance.\n",
    "\n",
    "In the TinyGPT training suite (Section 3.6), CUDA runs optionally include both `fp32` (TF32 allowed) and `fp32_strict` (TF32 disabled) so you can see how much TF32 changes your \"FP32 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if device.type == \"cuda\":\n",
    "    print(\"TF32 matmul:\", torch.backends.cuda.matmul.allow_tf32)\n",
    "    print(\"TF32 cuDNN:\", torch.backends.cudnn.allow_tf32)\n",
    "\n",
    "    # Uncomment to disable TF32 for strict FP32 comparisons:\n",
    "    # torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    # torch.backends.cudnn.allow_tf32 = False\n",
    "else:\n",
    "    print(\"CUDA not available; TF32 not applicable\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building mixed precision from scratch (the progressive implementation)\n",
    "\n",
    "This is the most important experiment in the notebook. Instead of jumping straight to `torch.amp`, we will:\n",
    "\n",
    "1. Train a simple model in **FP32** (baseline).\n",
    "2. Try **naive FP16** (just cast everything to half) — watch it fail or stagnate.\n",
    "3. Add **FP32 master weights** — fix the stagnation.\n",
    "4. Add **manual loss scaling** — fix gradient underflow.\n",
    "5. Replace everything with **PyTorch AMP** — the clean two-line version.\n",
    "\n",
    "By building each piece manually, the \"magic\" of AMP becomes completely transparent.\n",
    "\n",
    "We'll use a small MLP on a synthetic regression task to keep things fast and focused. The model is intentionally simple — the numeric effects are the same at any scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Progressive mixed-precision: shared setup\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Synthetic dataset: regression\n",
    "N_SAMPLES = 4096\n",
    "INPUT_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "X_data = torch.randn(N_SAMPLES, INPUT_DIM, device=device)\n",
    "# Target: a noisy linear function (so the model CAN learn it)\n",
    "true_w = torch.randn(INPUT_DIM, OUTPUT_DIM, device=device) * 0.01\n",
    "Y_data = X_data @ true_w + torch.randn(N_SAMPLES, OUTPUT_DIM, device=device) * 0.01\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "PROG_STEPS = 300\n",
    "\n",
    "def get_batches(steps):\n",
    "    for _ in range(steps):\n",
    "        idx = torch.randint(0, N_SAMPLES, (BATCH_SIZE,))\n",
    "        yield X_data[idx], Y_data[idx]\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def grad_stats(model):\n",
    "    # Return fraction of zero gradients and median |grad|.\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().float().flatten())\n",
    "    if not grads:\n",
    "        return 0.0, 0.0\n",
    "    g = torch.cat(grads)\n",
    "    zero_frac = float((g == 0).float().mean())\n",
    "    median_abs = float(g.abs().median())\n",
    "    return zero_frac, median_abs\n",
    "\n",
    "print(f\"Dataset: {N_SAMPLES} samples, input_dim={INPUT_DIM}\")\n",
    "print(f\"Model: MLP {INPUT_DIM} -> {HIDDEN_DIM} -> {HIDDEN_DIM} -> {OUTPUT_DIM}\")\n",
    "print(f\"Training: {PROG_STEPS} steps, batch_size={BATCH_SIZE}, lr={LR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Step 0: FP32 baseline\n",
    "\n",
    "Everything in FP32. This is our reference for correct training behavior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 0: FP32 baseline\n",
    "set_seed(42)\n",
    "model_fp32 = SimpleMLP().to(device).float()\n",
    "opt_fp32 = torch.optim.Adam(model_fp32.parameters(), lr=LR)\n",
    "\n",
    "log_fp32 = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": []}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_fp32.zero_grad(set_to_none=True)\n",
    "    pred = model_fp32(xb)\n",
    "    loss = F.mse_loss(pred, yb)\n",
    "    loss.backward()\n",
    "    zf, mg = grad_stats(model_fp32)\n",
    "    opt_fp32.step()\n",
    "    log_fp32[\"loss\"].append(float(loss))\n",
    "    log_fp32[\"zero_grad_frac\"].append(zf)\n",
    "    log_fp32[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"FP32 baseline: final loss = {log_fp32['loss'][-1]:.6f}\")\n",
    "print(f\"  Zero-grad fraction: {log_fp32['zero_grad_frac'][-1]:.4f}\")\n",
    "print(f\"  Median |grad|: {log_fp32['median_grad'][-1]:.2e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Step 1: Naive FP16 (just cast everything to half)\n",
    "\n",
    "The simplest approach: `model.half()` and cast inputs to FP16. No master weights, no scaling, no autocast.\n",
    "\n",
    "This is what the Micikevicius paper warns against. Expect problems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1: Naive FP16\n",
    "log_fp16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_fp16_naive[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping naive FP16 demo: CPU/MPS fp16 matmuls are often unsupported or unrepresentative.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_fp16_naive = SimpleMLP().to(device).half()\n",
    "    opt_fp16_naive = torch.optim.Adam(model_fp16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_fp16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_fp16_naive(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_fp16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_fp16_naive)\n",
    "        opt_fp16_naive.step()\n",
    "        log_fp16_naive[\"loss\"].append(float(loss))\n",
    "        log_fp16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_fp16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive FP16: status = {log_fp16_naive['status']}\")\n",
    "    if log_fp16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_fp16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_fp16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_fp16_naive['median_grad'][-1]:.2e}\")\n",
    "    else:\n",
    "        print(\"  Training failed immediately.\")\n",
    "\n",
    "    print()\n",
    "    if log_fp16_naive[\"status\"] != \"ok\" or (log_fp16_naive[\"loss\"] and log_fp16_naive[\"loss\"][-1] > log_fp32[\"loss\"][-1] * 5):\n",
    "        print(\"As expected, naive FP16 is problematic. The combination of:\")\n",
    "        print(\"  1. Weight update stagnation (updates below FP16 ULP)\")\n",
    "        print(\"  2. Gradient underflow (small gradients become zero)\")\n",
    "        print(\"makes training unstable or ineffective.\")\n",
    "    else:\n",
    "        print(\"Naive FP16 happened to work for this model (it sometimes does for simple/small models).\")\n",
    "        print(\"This does NOT mean it's safe in general.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2b Step 1b: Naive BF16 (the same cast-everything approach, but with BF16)\n",
    "\n",
    "Now do the *exact same thing* but with BF16 instead of FP16. This is the key comparison that shows **why BF16's range matters more than FP16's precision** for training stability.\n",
    "\n",
    "Same code, same model, same learning rate — just a different 16-bit format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1b: Naive BF16\n",
    "log_bf16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if not supports_dtype_on_device(torch.bfloat16, device):\n",
    "    log_bf16_naive[\"status\"] = \"skipped_no_bf16\"\n",
    "    print(\"Skipping naive BF16 demo: BF16 not supported on this device.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_bf16_naive = SimpleMLP().to(device).to(torch.bfloat16)\n",
    "    opt_bf16_naive = torch.optim.Adam(model_bf16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_bf16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_bf16_naive(xb.to(torch.bfloat16))\n",
    "        loss = F.mse_loss(pred, yb.to(torch.bfloat16))\n",
    "        if not torch.isfinite(loss):\n",
    "            log_bf16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_bf16_naive)\n",
    "        opt_bf16_naive.step()\n",
    "        log_bf16_naive[\"loss\"].append(float(loss))\n",
    "        log_bf16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_bf16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive BF16: status = {log_bf16_naive['status']}\")\n",
    "    if log_bf16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_bf16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_bf16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_bf16_naive['median_grad'][-1]:.2e}\")\n",
    "\n",
    "    print()\n",
    "    if log_bf16_naive[\"status\"] == \"ok\" and log_bf16_naive[\"loss\"]:\n",
    "        if log_bf16_naive[\"loss\"][-1] < log_fp32[\"loss\"][-1] * 2:\n",
    "            print(\"BF16 naive training WORKS! This is the key insight:\")\n",
    "            print(\"  - BF16 has the same exponent range as FP32 (8-bit exponent)\")\n",
    "            print(\"  - Gradients don't underflow, so training progresses even without loss scaling\")\n",
    "            print(\"  - The coarser precision (7-bit mantissa) introduces noise, but SGD tolerates noise\")\n",
    "            print(\"  - Compare this to FP16 naive above: same approach, different outcome, entirely due to range.\")\n",
    "        else:\n",
    "            print(\"BF16 naive converged but to a higher loss — precision may have limited final accuracy.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Step 2: Add FP32 master weights\n",
    "\n",
    "The first fix from the Micikevicius paper: keep an FP32 copy of parameters for the optimizer update.\n",
    "\n",
    "**The flow:**\n",
    "1. Forward pass uses FP16 parameters (for speed/memory).\n",
    "2. Backward produces FP16 gradients.\n",
    "3. Copy FP16 gradients to FP32 master parameters.\n",
    "4. Optimizer updates FP32 master weights.\n",
    "5. Copy updated FP32 weights back to FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 2: FP16 + FP32 master weights\n",
    "log_master = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_master[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 master-weights demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_master = SimpleMLP().to(device).half()  # FP16 for forward/backward\n",
    "\n",
    "    # Create FP32 master copy\n",
    "    master_params = [p.detach().clone().float().requires_grad_(True) for p in model_master.parameters()]\n",
    "    opt_master = torch.optim.Adam(master_params, lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_master.zero_grad(set_to_none=True)\n",
    "        model_master.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward in FP16\n",
    "        pred = model_master(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_master[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master params\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float()\n",
    "\n",
    "        zf, mg = grad_stats(model_master)\n",
    "\n",
    "        # Update FP32 master weights\n",
    "        opt_master.step()\n",
    "\n",
    "        # Copy FP32 master -> FP16 model\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_master[\"loss\"].append(float(loss))\n",
    "        log_master[\"zero_grad_frac\"].append(zf)\n",
    "        log_master[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights: status = {log_master['status']}\")\n",
    "    if log_master[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_master['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_master['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Step 3: Add loss scaling\n",
    "\n",
    "The second fix: multiply the loss by a scale factor $S$ before backward, then unscale gradients before the optimizer step. This prevents small gradients from underflowing to zero in FP16."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: FP16 + FP32 master weights + loss scaling\n",
    "log_scaled = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_scaled[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 loss-scaling demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_scaled = SimpleMLP().to(device).half()\n",
    "    master_params_s = [p.detach().clone().float().requires_grad_(True) for p in model_scaled.parameters()]\n",
    "    opt_scaled = torch.optim.Adam(master_params_s, lr=LR)\n",
    "\n",
    "    LOSS_SCALE = 2**13  # 8192 — a common starting point\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_scaled.zero_grad(set_to_none=True)\n",
    "        model_scaled.zero_grad(set_to_none=True)\n",
    "\n",
    "        pred = model_scaled(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_scaled[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "\n",
    "        # Scale loss before backward\n",
    "        (loss * LOSS_SCALE).backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master, then UNSCALE\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float() / LOSS_SCALE\n",
    "\n",
    "        zf, mg = grad_stats(model_scaled)\n",
    "\n",
    "        opt_scaled.step()\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_scaled[\"loss\"].append(float(loss))\n",
    "        log_scaled[\"zero_grad_frac\"].append(zf)\n",
    "        log_scaled[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights + scaling: status = {log_scaled['status']}\")\n",
    "    if log_scaled[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_scaled['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_scaled['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Step 4: PyTorch AMP (the clean version)\n",
    "\n",
    "Now replace all the manual work with PyTorch's `autocast` + `GradScaler`. Two extra lines of code.\n",
    "\n",
    "Note: autocast handles per-op dtype policies automatically. The model stays in FP32, and autocast temporarily casts operations during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: PyTorch AMP\n",
    "set_seed(42)\n",
    "model_amp = SimpleMLP().to(device).float()  # FP32 — autocast handles casting\n",
    "opt_amp = torch.optim.Adam(model_amp.parameters(), lr=LR)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "elif device.type == \"cpu\":\n",
    "    amp_dtype = torch.bfloat16\n",
    "    use_scaler = False\n",
    "else:  # mps (experimental AMP coverage)\n",
    "    amp_dtype = torch.float16\n",
    "    use_scaler = False\n",
    "\n",
    "scaler = GradScaler(enabled=(use_scaler and device.type == \"cuda\")) if use_scaler else None\n",
    "\n",
    "log_amp = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\", \"dtype\": str(amp_dtype)}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_amp.zero_grad(set_to_none=True)\n",
    "\n",
    "    with amp_autocast(device, amp_dtype, enabled=True):\n",
    "        pred = model_amp(xb)\n",
    "        loss = F.mse_loss(pred, yb)\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "        log_amp[\"status\"] = \"non_finite_loss\"\n",
    "        break\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt_amp)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        opt_amp.step()\n",
    "\n",
    "    zf, mg = grad_stats(model_amp)\n",
    "    log_amp[\"loss\"].append(float(loss))\n",
    "    log_amp[\"zero_grad_frac\"].append(zf)\n",
    "    log_amp[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"PyTorch AMP ({log_amp['dtype']}): status = {log_amp['status']}\")\n",
    "if log_amp[\"loss\"]:\n",
    "    print(f\"  Final loss = {log_amp['loss'][-1]:.6f}\")\n",
    "    print(f\"  Zero-grad fraction: {log_amp['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare all progressive approaches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "configs = [\n",
    "    (\"FP32 baseline\", log_fp32, \"C0\"),\n",
    "    (\"Naive FP16\", log_fp16_naive, \"C3\"),\n",
    "    (\"Naive BF16\", log_bf16_naive, \"C5\"),\n",
    "    (\"FP16 + master wts\", log_master, \"C1\"),\n",
    "    (\"FP16 + master + scaling\", log_scaled, \"C4\"),\n",
    "    (f\"PyTorch AMP ({log_amp['dtype']})\", log_amp, \"C2\"),\n",
    "]\n",
    "\n",
    "# Loss curves\n",
    "for name, log, color in configs:\n",
    "    if log[\"loss\"]:\n",
    "        axes[0].plot(log[\"loss\"], label=name, color=color, alpha=0.8)\n",
    "axes[0].set_title(\"Training loss\")\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"MSE loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "# Zero gradient fraction\n",
    "for name, log, color in configs:\n",
    "    if log[\"zero_grad_frac\"]:\n",
    "        axes[1].plot(log[\"zero_grad_frac\"], label=name, color=color, alpha=0.8)\n",
    "axes[1].set_title(\"Fraction of zero gradients\")\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"fraction\")\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# Median gradient magnitude\n",
    "for name, log, color in configs:\n",
    "    if log[\"median_grad\"]:\n",
    "        axes[2].plot(log[\"median_grad\"], label=name, color=color, alpha=0.8)\n",
    "axes[2].set_title(\"Median |gradient|\")\n",
    "axes[2].set_xlabel(\"step\")\n",
    "axes[2].set_ylabel(\"|grad|\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Progressive Mixed Precision: from naive FP16 to PyTorch AMP\", fontsize=12, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 What to observe\n",
    "\n",
    "**Loss curves:**\n",
    "- FP32 baseline should decrease smoothly.\n",
    "- Naive FP16 may diverge, stagnate, or produce NaN.\n",
    "- **Naive BF16 often trains successfully** — this is the dramatic demonstration of range vs precision. Same \"cast everything to 16-bit\" approach, but BF16's 8-bit exponent prevents the gradient underflow that kills FP16.\n",
    "- Adding master weights to FP16 typically helps convergence.\n",
    "- Adding loss scaling to FP16 reduces the fraction of zero gradients.\n",
    "- PyTorch AMP should match or beat the manual implementations.\n",
    "\n",
    "**Zero gradient fraction:**\n",
    "- In naive FP16, many gradients may be exactly zero (underflow).\n",
    "- In naive BF16, almost no gradients are zero — the exponent range is wide enough.\n",
    "- Loss scaling shifts the FP16 distribution, rescuing underflowed gradients.\n",
    "\n",
    "**Median gradient magnitude:**\n",
    "- Shows the \"signal strength\" available to the optimizer.\n",
    "- If it drops to zero, the model stops learning.\n",
    "- Compare FP16 vs BF16: even with coarser mantissa, BF16 preserves gradient *signal*.\n",
    "\n",
    "**The key lesson from this progressive build-up:**\n",
    "Naive FP16 fails. BF16 naive often works. But both benefit from FP32 master weights and proper AMP policies. The Micikevicius paper's three-part recipe (per-op policy + loss scaling + master weights) handles FP16 correctly. BF16 simplifies the picture by removing the need for loss scaling, but master weights and per-op policies still help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build an \"autocast operator policy table\" from your local PyTorch\n",
    "\n",
    "Instead of trusting a static table from the internet, we can probe your exact PyTorch version:\n",
    "1. Run each op with autocast **disabled** → observe output dtype.\n",
    "2. Run each op with autocast **enabled** → observe output dtype.\n",
    "3. Compare to see which ops autocast changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Enhanced operator policy probe\n",
    "\n",
    "def probe_ops(dev, amp_dtype):\n",
    "    a32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    b32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x128 = torch.randn(2, 128, device=dev, dtype=torch.float32)\n",
    "    x_big = torch.randn(2, 20000, device=dev, dtype=torch.float32)\n",
    "    w32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    bias32 = torch.randn(128, device=dev, dtype=torch.float32)\n",
    "    target32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x16 = x_big.to(amp_dtype)\n",
    "\n",
    "    ops = {\n",
    "        # Matmul-like (expect lower precision)\n",
    "        \"matmul (a @ b)\": lambda: a32 @ b32,\n",
    "        \"F.linear\": lambda: F.linear(a32, w32, bias32),\n",
    "        # Numerically sensitive (expect FP32)\n",
    "        \"F.softmax\": lambda: F.softmax(x128, dim=-1),\n",
    "        \"F.layer_norm\": lambda: F.layer_norm(x128, [x128.size(-1)]),\n",
    "        \"F.cross_entropy\": lambda: F.cross_entropy(a32[:10], torch.randint(0, 128, (10,), device=dev)),\n",
    "        \"F.mse_loss\": lambda: F.mse_loss(a32, target32),\n",
    "        \"torch.exp\": lambda: torch.exp(x128),\n",
    "        \"torch.log\": lambda: torch.log(x128.abs() + 1e-6),\n",
    "        \"torch.sum\": lambda: x_big.sum(),\n",
    "        \"torch.prod\": lambda: x_big[:, :10].prod(),\n",
    "        # Pass-through (expect input dtype)\n",
    "        \"F.relu\": lambda: F.relu(x_big),\n",
    "        \"torch.max\": lambda: x_big.max(),\n",
    "        \"torch.min\": lambda: x_big.min(),\n",
    "        \"torch.mean\": lambda: x_big.mean(),\n",
    "        \"F.dropout\": lambda: F.dropout(x_big, p=0.0, training=True),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for name, fn in ops.items():\n",
    "        # Without autocast\n",
    "        y_no = fn()\n",
    "        dt_no = str(y_no.dtype) if isinstance(y_no, torch.Tensor) else \"n/a\"\n",
    "        # With autocast (FP32 inputs)\n",
    "        with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "            y_ac = fn()\n",
    "        dt_ac = str(y_ac.dtype) if isinstance(y_ac, torch.Tensor) else \"n/a\"\n",
    "\n",
    "        # With autocast (16-bit inputs, where applicable)\n",
    "        dt_ac16 = \"\"\n",
    "        try:\n",
    "            # Replace x32 temporarily\n",
    "            ops_16 = {\n",
    "                \"F.relu\": lambda: F.relu(x16),\n",
    "                \"torch.max\": lambda: x16.max(),\n",
    "                \"torch.min\": lambda: x16.min(),\n",
    "                \"torch.mean\": lambda: x16.mean(),\n",
    "                \"torch.sum\": lambda: x16.sum(),\n",
    "            }\n",
    "            if name in ops_16:\n",
    "                with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "                    y16 = ops_16[name]()\n",
    "                dt_ac16 = str(y16.dtype)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        changed = dt_no != dt_ac\n",
    "        rows.append({\n",
    "            \"op\": name,\n",
    "            \"no_autocast\": dt_no,\n",
    "            f\"autocast({amp_dtype})_fp32_input\": dt_ac,\n",
    "            \"16bit_input\": dt_ac16 if dt_ac16 else \"-\",\n",
    "            \"policy\": \"LOWER PREC\" if \"float16\" in dt_ac or \"bfloat16\" in dt_ac else (\"FP32\" if changed or dt_ac == \"torch.float32\" else \"pass-through\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "for amp_dt in [torch.float16, torch.bfloat16]:\n",
    "    if device.type == \"cuda\" and amp_dt is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n",
    "        continue\n",
    "    if device.type == \"cpu\" and amp_dt is torch.float16:\n",
    "        continue\n",
    "    if not supports_dtype_on_device(amp_dt, device):\n",
    "        continue\n",
    "    print(f\"\\n=== Autocast policy with amp_dtype={amp_dt} ===\")\n",
    "    display(probe_ops(device, amp_dt))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 The \"sum vs mean\" mystery\n",
    "\n",
    "Under autocast, `sum` produces FP32 output but `mean` stays in the input dtype. Why?\n",
    "\n",
    "**`sum`** is a reduction that accumulates values. Summing 20,000 BF16 values can easily exceed the representable range (BF16 max ~ $3.4 \\times 10^{38}$, but even FP16 max is only ~65,504). More critically, the rounding errors from adding many small values compound. PyTorch hardcodes `sum` to promote to FP32.\n",
    "\n",
    "**`mean`** inherently divides by $N$, keeping the output bounded between the min and max of the input. It cannot overflow by accumulation. PyTorch treats it as a pass-through — whatever dtype goes in, the same comes out.\n",
    "\n",
    "**`prod`** is similar to `sum` but even more extreme: multiplying many values can grow (or shrink) astronomically. Also forced to FP32.\n",
    "\n",
    "Let's verify this directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The sum vs mean mystery — direct probe\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    x16 = torch.randn(2, 20000, device=device, dtype=dtype_16bit)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "        results = {\n",
    "            \"max\": x16.max().dtype,\n",
    "            \"min\": x16.min().dtype,\n",
    "            \"sum\": x16.sum().dtype,\n",
    "            \"mean\": x16.mean().dtype,\n",
    "            \"prod\": x16.prod().dtype,\n",
    "            \"exp\": x16.exp().dtype,\n",
    "            \"log\": x16.abs().log().dtype,\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for op, dt in results.items():\n",
    "        rows.append({\n",
    "            \"op\": op,\n",
    "            \"output_dtype\": str(dt),\n",
    "            \"promoted_to_fp32\": \"YES\" if dt == torch.float32 else \"no\",\n",
    "            \"reason\": {\n",
    "                \"max\": \"element-wise selection, no accumulation\",\n",
    "                \"min\": \"element-wise selection, no accumulation\",\n",
    "                \"sum\": \"ACCUMULATION: rounding errors compound over N values\",\n",
    "                \"mean\": \"bounded output (divides by N), no overflow risk\",\n",
    "                \"prod\": \"ACCUMULATION: multiplicative explosion/collapse\",\n",
    "                \"exp\": \"can produce extreme magnitudes (overflow risk)\",\n",
    "                \"log\": \"can produce extreme magnitudes (underflow risk)\",\n",
    "            }.get(op, \"\"),\n",
    "        })\n",
    "\n",
    "    display(pd.DataFrame(rows))\n",
    "else:\n",
    "    print(\"Run on CUDA to see the sum vs mean mystery in action.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Per-layer precision toy examples: why autocast treats layers differently\n",
    "\n",
    "Autocast exists for two reasons:\n",
    "\n",
    "1. **Accumulation** — summing many terms compounds rounding error. A matmul of shape `[M, K] @ [K, N]` accumulates `K` products per output element. As `K` grows, FP16's 10-bit mantissa loses signal.\n",
    "2. **Exponentiation** — `exp(x)` amplifies errors and overflows in FP16 for `x > ~11`. Softmax depends on `exp`, so it's precision-sensitive.\n",
    "\n",
    "Below we test **three representative layer types** — attention, activation functions, and FFN linear layers — in up to **6 configurations** each: 3 dtypes (FP32, FP16, BF16) × 2 modes (no autocast, with autocast). (Some dtype/backend combos may be skipped if a kernel is unavailable.) All errors are measured against an FP64 reference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell A: Attention (Q·K^T → softmax → @V)\n",
    "#\n",
    "# Why precision-sensitive:\n",
    "#   - Two matmuls with accumulation\n",
    "#   - softmax with exp (overflow risk in FP16)\n",
    "\n",
    "set_seed(0)\n",
    "B, N_HEADS, SEQ, HEAD_DIM = 4, 4, 64, 32\n",
    "\n",
    "# Create random Q, K, V (scaled up by 5x to stress precision)\n",
    "q64 = torch.randn(B, N_HEADS, SEQ, HEAD_DIM, device=device, dtype=torch.float64) * 5\n",
    "k64 = q64.clone()  # K = Q → large diagonal in Q·K^T, stressing softmax with peaked logits\n",
    "v64 = torch.randn(B, N_HEADS, SEQ, HEAD_DIM, device=device, dtype=torch.float64) * 5\n",
    "\n",
    "def attention_forward(q, k, v):\n",
    "    scale = math.sqrt(q.size(-1))\n",
    "    scores = (q @ k.transpose(-2, -1)) / scale\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    output = weights @ v\n",
    "    return weights, output\n",
    "\n",
    "# FP64 reference\n",
    "w64, o64 = attention_forward(q64, k64, v64)\n",
    "\n",
    "attn_rows = []\n",
    "attn_skipped = []\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    for ac_label, use_ac in [(\"no autocast\", False), (\"autocast\", True)]:\n",
    "        try:\n",
    "            q_cast = q64.to(dtype)\n",
    "            k_cast = k64.to(dtype)\n",
    "            v_cast = v64.to(dtype)\n",
    "            if use_ac:\n",
    "                with amp_autocast(device, dtype, enabled=True):\n",
    "                    w_test, o_test = attention_forward(q_cast, k_cast, v_cast)\n",
    "            else:\n",
    "                w_test, o_test = attention_forward(q_cast, k_cast, v_cast)\n",
    "        except Exception as exc:\n",
    "            attn_skipped.append({\n",
    "                \"dtype\": dtype_name,\n",
    "                \"mode\": ac_label,\n",
    "                \"reason\": str(exc).splitlines()[0],\n",
    "            })\n",
    "            continue\n",
    "        # Errors vs FP64\n",
    "        w_err = float((w_test.double() - w64).abs().max())\n",
    "        cos = float(F.cosine_similarity(o_test.double().flatten().unsqueeze(0),\n",
    "                                         o64.flatten().unsqueeze(0)))\n",
    "        entropy = float(-(w_test.double() * (w_test.double() + 1e-12).log()).sum(-1).mean())\n",
    "        attn_rows.append({\n",
    "            \"dtype\": dtype_name, \"mode\": ac_label,\n",
    "            \"max_weight_err\": w_err, \"output_cos_sim\": cos, \"attn_entropy\": entropy,\n",
    "        })\n",
    "\n",
    "df_attn = pd.DataFrame(attn_rows)\n",
    "display(df_attn)\n",
    "if attn_skipped:\n",
    "    print(\"Skipped attention configs (unsupported dtype/backend path):\")\n",
    "    display(pd.DataFrame(attn_skipped))\n",
    "\n",
    "# Plot: grouped bar chart\n",
    "if len(df_attn) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    labels = [f\"{r['dtype']}\\n{r['mode']}\" for _, r in df_attn.iterrows()]\n",
    "    x = np.arange(len(labels))\n",
    "    colors = [\"#e74c3c\" if \"FP16\" in l else \"#3498db\" if \"BF16\" in l else \"#2ecc71\" for l in labels]\n",
    "\n",
    "    for ax, metric, title in zip(axes, [\"max_weight_err\", \"output_cos_sim\", \"attn_entropy\"],\n",
    "                                  [\"Max Attention Weight Error\", \"Output Cosine Similarity (vs FP64)\", \"Attention Entropy\"]):\n",
    "        vals = df_attn[metric].tolist()\n",
    "        bars = ax.bar(x, vals, color=colors, alpha=0.8, edgecolor=\"white\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, fontsize=7, rotation=30, ha=\"right\")\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        if metric == \"max_weight_err\":\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "    fig.suptitle(\"Attention: precision effects on Q·K^T → softmax → @V\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (Attention):**\n",
    "- FP16 without autocast shows the **largest attention weight errors** — `exp()` in softmax amplifies small rounding differences into large distribution shifts.\n",
    "- On many CUDA AMP policies, autocast routes softmax/reductions to FP32 while keeping matmuls in 16-bit, giving near-FP32-quality attention weights (verify with your local traces in Sections 3.2/3.3).\n",
    "- BF16's wider exponent range means it rarely overflows in softmax, but its lower mantissa precision still shows higher error than FP32."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell B: Activation Functions (ReLU vs GELU)\n",
    "#\n",
    "# Why interesting: Element-wise ops, no accumulation → autocast treats them as pass-through.\n",
    "\n",
    "x64 = torch.linspace(-10, 10, 10000, device=device, dtype=torch.float64)\n",
    "relu_ref = F.relu(x64)\n",
    "gelu_ref = F.gelu(x64)\n",
    "\n",
    "act_results = []\n",
    "act_skipped = []\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    for ac_label, use_ac in [(\"no autocast\", False), (\"autocast\", True)]:\n",
    "        try:\n",
    "            x_cast = x64.to(dtype)\n",
    "            if use_ac:\n",
    "                with amp_autocast(device, dtype, enabled=True):\n",
    "                    relu_out = F.relu(x_cast)\n",
    "                    gelu_out = F.gelu(x_cast)\n",
    "            else:\n",
    "                relu_out = F.relu(x_cast)\n",
    "                gelu_out = F.gelu(x_cast)\n",
    "        except Exception as exc:\n",
    "            act_skipped.append({\n",
    "                \"dtype\": dtype_name,\n",
    "                \"mode\": ac_label,\n",
    "                \"reason\": str(exc).splitlines()[0],\n",
    "            })\n",
    "            continue\n",
    "        relu_err = (relu_out.double() - relu_ref).abs()\n",
    "        gelu_err = (gelu_out.double() - gelu_ref).abs()\n",
    "        act_results.append({\n",
    "            \"dtype\": dtype_name, \"mode\": ac_label,\n",
    "            \"relu_max_err\": float(relu_err.max()),\n",
    "            \"gelu_max_err\": float(gelu_err.max()),\n",
    "            \"x\": x64.cpu().numpy(),\n",
    "            \"relu_err\": relu_err.cpu().numpy(),\n",
    "            \"gelu_err\": gelu_err.cpu().numpy(),\n",
    "            \"relu_out\": relu_out.float().cpu().numpy(),\n",
    "            \"gelu_out\": gelu_out.float().cpu().numpy(),\n",
    "        })\n",
    "\n",
    "# Quantify whether autocast itself changes activation outputs at a fixed dtype.\n",
    "delta_rows = []\n",
    "for dtype_name in [\"FP32\", \"FP16\", \"BF16\"]:\n",
    "    no_ac = next((r for r in act_results if r[\"dtype\"] == dtype_name and r[\"mode\"] == \"no autocast\"), None)\n",
    "    ac = next((r for r in act_results if r[\"dtype\"] == dtype_name and r[\"mode\"] == \"autocast\"), None)\n",
    "    if no_ac is None or ac is None:\n",
    "        continue\n",
    "    delta_rows.append({\n",
    "        \"dtype\": dtype_name,\n",
    "        \"relu_max_delta(ac-vs-noac)\": float(np.max(np.abs(ac[\"relu_out\"] - no_ac[\"relu_out\"]))),\n",
    "        \"gelu_max_delta(ac-vs-noac)\": float(np.max(np.abs(ac[\"gelu_out\"] - no_ac[\"gelu_out\"]))),\n",
    "    })\n",
    "if delta_rows:\n",
    "    print(\"Autocast-vs-no-autocast output deltas (same input dtype):\")\n",
    "    display(pd.DataFrame(delta_rows))\n",
    "if act_skipped:\n",
    "    print(\"Skipped activation configs (unsupported dtype/backend path):\")\n",
    "    display(pd.DataFrame(act_skipped))\n",
    "\n",
    "# Plot: error vs input value\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for row in act_results:\n",
    "    label = f\"{row['dtype']} {row['mode']}\"\n",
    "    ls = \"--\" if row[\"mode\"] == \"autocast\" else \"-\"\n",
    "    axes[0].plot(row[\"x\"], row[\"gelu_err\"], label=label, ls=ls, alpha=0.7, lw=1.2)\n",
    "    axes[1].plot(row[\"x\"], row[\"relu_err\"], label=label, ls=ls, alpha=0.7, lw=1.2)\n",
    "\n",
    "axes[0].set_title(\"GELU absolute error vs FP64\", fontsize=10)\n",
    "axes[0].set_xlabel(\"Input value\")\n",
    "axes[0].set_ylabel(\"Absolute error\")\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "axes[1].set_title(\"ReLU absolute error vs FP64 (the control)\", fontsize=10)\n",
    "axes[1].set_xlabel(\"Input value\")\n",
    "axes[1].set_ylabel(\"Absolute error\")\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Activation functions: in this setup, autocast adds little/no remapping\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (Activations):**\n",
    "- ReLU still inherits **input quantization error** (because positive outputs copy the rounded input), so error is small but generally not zero in FP16/BF16.\n",
    "- GELU shows larger low-precision error than ReLU (nonlinear approximation), but **autocast vs no-autocast curves typically overlap** at the same dtype.\n",
    "- Takeaway: element-wise activations are usually pass-through for autocast policy; most error here comes from the input dtype itself, not autocast remapping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell C: FFN Linear Layers (matmul accumulation)\n",
    "#\n",
    "# Why precision-sensitive: y = x @ W^T + b accumulates d_model products per output.\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# Part 1: FFN at d_model=512\n",
    "D_MODEL = 512\n",
    "D_FF = 2048\n",
    "x64_ffn = torch.randn(4, 64, D_MODEL, device=device, dtype=torch.float64)\n",
    "w1_64 = torch.randn(D_FF, D_MODEL, device=device, dtype=torch.float64) * 0.02\n",
    "b1_64 = torch.randn(D_FF, device=device, dtype=torch.float64) * 0.02\n",
    "w2_64 = torch.randn(D_MODEL, D_FF, device=device, dtype=torch.float64) * 0.02\n",
    "b2_64 = torch.randn(D_MODEL, device=device, dtype=torch.float64) * 0.02\n",
    "\n",
    "# FP64 reference\n",
    "y64 = F.linear(F.gelu(F.linear(x64_ffn, w1_64, b1_64)), w2_64, b2_64)\n",
    "\n",
    "ffn_results = []\n",
    "ffn_skipped = []\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    for ac_label, use_ac in [(\"no autocast\", False), (\"autocast\", True)]:\n",
    "        try:\n",
    "            x_c = x64_ffn.to(dtype)\n",
    "            w1_c, b1_c = w1_64.to(dtype), b1_64.to(dtype)\n",
    "            w2_c, b2_c = w2_64.to(dtype), b2_64.to(dtype)\n",
    "            if use_ac:\n",
    "                with amp_autocast(device, dtype, enabled=True):\n",
    "                    y_test = F.linear(F.gelu(F.linear(x_c, w1_c, b1_c)), w2_c, b2_c)\n",
    "            else:\n",
    "                y_test = F.linear(F.gelu(F.linear(x_c, w1_c, b1_c)), w2_c, b2_c)\n",
    "        except Exception as exc:\n",
    "            ffn_skipped.append({\n",
    "                \"dtype\": dtype_name,\n",
    "                \"mode\": ac_label,\n",
    "                \"phase\": f\"ffn_d{D_MODEL}\",\n",
    "                \"reason\": str(exc).splitlines()[0],\n",
    "            })\n",
    "            continue\n",
    "        rel_err = float((y_test.double() - y64).abs().mean() / (y64.abs().mean() + 1e-12))\n",
    "        ffn_results.append({\"dtype\": dtype_name, \"mode\": ac_label, \"mean_rel_err\": rel_err})\n",
    "\n",
    "# Part 2: Accumulation error vs dimension\n",
    "dims = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "scaling_results = {dname: {\"no_ac\": [], \"ac\": []} for dname in [\"FP32\", \"FP16\", \"BF16\"]}\n",
    "\n",
    "for d in dims:\n",
    "    set_seed(0)\n",
    "    a64 = torch.randn(64, d, device=device, dtype=torch.float64)\n",
    "    b64_mat = torch.randn(d, 64, device=device, dtype=torch.float64) * 0.02\n",
    "    ref = (a64 @ b64_mat)\n",
    "    for dname, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "        if not supports_dtype_on_device(dtype, device):\n",
    "            continue\n",
    "        a_c, b_c = a64.to(dtype), b64_mat.to(dtype)\n",
    "        # No autocast\n",
    "        try:\n",
    "            out_no_ac = (a_c @ b_c)\n",
    "            err_no_ac = float((out_no_ac.double() - ref).abs().mean() / (ref.abs().mean() + 1e-12))\n",
    "            scaling_results[dname][\"no_ac\"].append((d, err_no_ac))\n",
    "        except Exception as exc:\n",
    "            ffn_skipped.append({\n",
    "                \"dtype\": dname,\n",
    "                \"mode\": \"no autocast\",\n",
    "                \"phase\": f\"matmul_d{d}\",\n",
    "                \"reason\": str(exc).splitlines()[0],\n",
    "            })\n",
    "        # With autocast\n",
    "        try:\n",
    "            with amp_autocast(device, dtype, enabled=True):\n",
    "                out_ac = (a_c @ b_c)\n",
    "            err_ac = float((out_ac.double() - ref).abs().mean() / (ref.abs().mean() + 1e-12))\n",
    "            scaling_results[dname][\"ac\"].append((d, err_ac))\n",
    "        except Exception as exc:\n",
    "            ffn_skipped.append({\n",
    "                \"dtype\": dname,\n",
    "                \"mode\": \"autocast\",\n",
    "                \"phase\": f\"matmul_d{d}\",\n",
    "                \"reason\": str(exc).splitlines()[0],\n",
    "            })\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: grouped bars at d=512\n",
    "ax = axes[0]\n",
    "df_ffn = pd.DataFrame(ffn_results)\n",
    "if len(df_ffn) > 0:\n",
    "    labels = [f\"{r['dtype']}\\n{r['mode']}\" for _, r in df_ffn.iterrows()]\n",
    "    x_pos = np.arange(len(labels))\n",
    "    colors = [\"#e74c3c\" if \"FP16\" in l else \"#3498db\" if \"BF16\" in l else \"#2ecc71\" for l in labels]\n",
    "    ax.bar(x_pos, df_ffn[\"mean_rel_err\"].tolist(), color=colors, alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(labels, fontsize=7, rotation=30, ha=\"right\")\n",
    "    ax.set_ylabel(\"Mean relative error vs FP64\")\n",
    "    ax.set_title(f\"FFN output error (d_model={D_MODEL})\", fontsize=10)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "# Right: error vs dimension (the money plot)\n",
    "ax = axes[1]\n",
    "style_map = {\"FP32\": (\"#2ecc71\", \"o\"), \"FP16\": (\"#e74c3c\", \"s\"), \"BF16\": (\"#3498db\", \"^\")}\n",
    "for dname, data in scaling_results.items():\n",
    "    if dname not in style_map:\n",
    "        continue\n",
    "    color, marker = style_map[dname]\n",
    "    if data[\"no_ac\"]:\n",
    "        x_no = [t[0] for t in data[\"no_ac\"]]\n",
    "        y_no = [t[1] for t in data[\"no_ac\"]]\n",
    "        ax.plot(x_no, y_no, color=color, marker=marker,\n",
    "                ls=\"-\", label=f\"{dname} (no autocast)\", alpha=0.8)\n",
    "    if data[\"ac\"]:\n",
    "        x_ac = [t[0] for t in data[\"ac\"]]\n",
    "        y_ac = [t[1] for t in data[\"ac\"]]\n",
    "        ax.plot(x_ac, y_ac, color=color, marker=marker,\n",
    "                ls=\"--\", label=f\"{dname} (autocast)\", alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Accumulation dimension (d)\")\n",
    "ax.set_ylabel(\"Mean relative error vs FP64\")\n",
    "ax.set_title(\"Matmul error growth with dimension\", fontsize=10)\n",
    "ax.set_xscale(\"log\", base=2)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"FFN linear layers: accumulation error grows with dimension\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "if ffn_skipped:\n",
    "    print(\"Skipped FFN configs (unsupported dtype/backend path):\")\n",
    "    display(pd.DataFrame(ffn_skipped))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (FFN Linear Layers):**\n",
    "- At `d_model=512`, FP16 error is ~10× higher than FP32. BF16 is in between (wider range, less mantissa precision).\n",
    "- The **right plot is the money plot**: in random-sum regimes, FP16 matmul error often follows an approximately $O(\\sqrt{d})$ trend with accumulation dimension, while FP32 stays flatter.\n",
    "- Autocast usually doesn't change the matmul error much by itself because the multiply path remains low precision; on CUDA Tensor Cores, the key hardware fix is FP32 accumulation.\n",
    "- **Takeaway:** low-precision multiply can be fast, but high-precision accumulation is critical for quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Numerical stability stress tests: adversarial inputs for sensitive ops\n",
    "\n",
    "The toy examples above show *average-case* error. But precision failures often happen at the **edges** — extreme inputs that push operations into overflow, underflow, or degenerate behavior. Here we craft adversarial inputs for three precision-sensitive ops."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stress test: adversarial inputs for softmax, LayerNorm, and cross-entropy formulations\n",
    "\n",
    "stress_rows = []\n",
    "stress_skipped = []\n",
    "\n",
    "# ── Test 1: Softmax with nearly-identical large logits ──\n",
    "# In FP16, logits like [1000, 1000.001, 999.999] all round to the same value,\n",
    "# erasing small ranking differences and collapsing toward a uniform distribution.\n",
    "\n",
    "logits_64 = torch.tensor([[1000.0, 1000.001, 999.999]], device=device, dtype=torch.float64)\n",
    "ref_softmax = torch.softmax(logits_64, dim=-1)\n",
    "\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    try:\n",
    "        logits_cast = logits_64.to(dtype)\n",
    "        # softmax on cast inputs (some kernels may internally upcast)\n",
    "        probs_native = torch.softmax(logits_cast, dim=-1)\n",
    "        err = float((probs_native.double() - ref_softmax).abs().max())\n",
    "        spread = float((probs_native.max() - probs_native.min()).double())\n",
    "        is_degenerate = spread < 1e-6\n",
    "        stress_rows.append({\n",
    "            \"test\": \"Softmax (large close logits)\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"max_error\": f\"{err:.6f}\",\n",
    "            \"degenerate\": \"YES\" if is_degenerate else \"no\",\n",
    "            \"detail\": f\"spread={spread:.3e}, probs={[f'{p:.4f}' for p in probs_native[0].tolist()]}\",\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        stress_skipped.append({\n",
    "            \"test\": \"Softmax (large close logits)\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"reason\": str(exc).splitlines()[0],\n",
    "        })\n",
    "\n",
    "# ── Test 2: LayerNorm with near-constant input (variance ≈ 0) ──\n",
    "# When all inputs are nearly identical, variance → 0, and dividing by sqrt(var+eps)\n",
    "# amplifies tiny differences. Low precision can collapse these perturbations.\n",
    "\n",
    "near_const = torch.ones(1, 128, device=device, dtype=torch.float64) + \\\n",
    "    torch.randn(1, 128, device=device, dtype=torch.float64) * 1e-5\n",
    "ln = nn.LayerNorm(128, device=device, dtype=torch.float64)\n",
    "ln.weight.data.fill_(1.0)\n",
    "ln.bias.data.fill_(0.0)\n",
    "ref_ln = ln(near_const)\n",
    "\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    try:\n",
    "        ln_cast = nn.LayerNorm(128, device=device, dtype=dtype)\n",
    "        ln_cast.weight.data.fill_(1.0)\n",
    "        ln_cast.bias.data.fill_(0.0)\n",
    "        inp_cast = near_const.to(dtype)\n",
    "        out_cast = ln_cast(inp_cast)\n",
    "        err = float((out_cast.double() - ref_ln).abs().max())\n",
    "        out_std = float(out_cast.float().std(unbiased=False))\n",
    "        is_collapsed = out_std < 1e-6 or bool(torch.isnan(out_cast).any())\n",
    "        stress_rows.append({\n",
    "            \"test\": \"LayerNorm (var≈0)\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"max_error\": f\"{err:.4f}\",\n",
    "            \"degenerate\": \"YES\" if is_collapsed else \"no\",\n",
    "            \"detail\": f\"std={out_std:.3e}, range=[{float(out_cast.min()):.4f}, {float(out_cast.max()):.4f}]\",\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        stress_skipped.append({\n",
    "            \"test\": \"LayerNorm (var≈0)\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"reason\": str(exc).splitlines()[0],\n",
    "        })\n",
    "\n",
    "# ── Test 3: Cross-entropy decomposition vs fused implementation ──\n",
    "# Naively computing -log(softmax(logits)[target]) can produce log(0) in low precision.\n",
    "# F.cross_entropy uses a stabilized logsumexp path and is typically finite.\n",
    "\n",
    "logits_ce_64 = torch.tensor([[50.0, -50.0, -50.0]], device=device, dtype=torch.float64)\n",
    "target_wrong = torch.tensor([1], device=device)  # intentionally low-probability class\n",
    "ref_naive = -torch.log_softmax(logits_ce_64, dim=-1).gather(1, target_wrong[:, None]).squeeze(1)\n",
    "\n",
    "for dtype_name, dtype in [(\"FP32\", torch.float32), (\"FP16\", torch.float16), (\"BF16\", torch.bfloat16)]:\n",
    "    if not supports_dtype_on_device(dtype, device):\n",
    "        continue\n",
    "    try:\n",
    "        logits_cast = logits_ce_64.to(dtype)\n",
    "        probs_cast = torch.softmax(logits_cast, dim=-1)\n",
    "        p_t = probs_cast.gather(1, target_wrong[:, None]).squeeze(1)\n",
    "        naive_ce = -torch.log(p_t)\n",
    "        fused_ce = F.cross_entropy(logits_cast, target_wrong)\n",
    "        naive_finite = bool(torch.isfinite(naive_ce).all())\n",
    "        if naive_finite:\n",
    "            err = float((naive_ce.double() - ref_naive).abs().max())\n",
    "            err_txt = f\"{err:.6f}\"\n",
    "        else:\n",
    "            err_txt = \"inf/nan\"\n",
    "        stress_rows.append({\n",
    "            \"test\": \"CrossEntropy (naive log(softmax))\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"max_error\": err_txt,\n",
    "            \"degenerate\": \"YES\" if not naive_finite else \"no\",\n",
    "            \"detail\": f\"naive={float(naive_ce):.3g}, fused={float(fused_ce):.6f}\",\n",
    "        })\n",
    "    except Exception as exc:\n",
    "        stress_skipped.append({\n",
    "            \"test\": \"CrossEntropy (naive log(softmax))\",\n",
    "            \"dtype\": dtype_name,\n",
    "            \"reason\": str(exc).splitlines()[0],\n",
    "        })\n",
    "\n",
    "df_stress = pd.DataFrame(stress_rows)\n",
    "display(df_stress)\n",
    "if stress_skipped:\n",
    "    print(\"Skipped stress-test configs (unsupported dtype/backend path):\")\n",
    "    display(pd.DataFrame(stress_skipped))\n",
    "\n",
    "# Summary bar plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "tests = [\"Softmax (large close logits)\", \"LayerNorm (var≈0)\", \"CrossEntropy (naive log(softmax))\"]\n",
    "for ax, test_name in zip(axes, tests):\n",
    "    subset = df_stress[df_stress[\"test\"] == test_name].copy()\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "    labels_s = subset[\"dtype\"].tolist()\n",
    "    raw_errs = []\n",
    "    for e in subset[\"max_error\"].tolist():\n",
    "        try:\n",
    "            raw_errs.append(float(e))\n",
    "        except ValueError:\n",
    "            raw_errs.append(float(\"nan\"))\n",
    "    finite_errs = [v for v in raw_errs if np.isfinite(v)]\n",
    "    cap = max(finite_errs + [1e-8]) * 1.5\n",
    "    errs = [v if np.isfinite(v) else cap for v in raw_errs]\n",
    "    colors_s = [\"#e74c3c\" if \"FP16\" in l else \"#3498db\" if \"BF16\" in l else \"#2ecc71\" for l in labels_s]\n",
    "    ax.bar(range(len(labels_s)), errs, color=colors_s, alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_xticks(range(len(labels_s)))\n",
    "    ax.set_xticklabels(labels_s, fontsize=9)\n",
    "    ax.set_title(test_name, fontsize=10)\n",
    "    ax.set_ylabel(\"Max absolute error\")\n",
    "    ax.set_ylim(0, cap * 1.2)\n",
    "    # Mark degenerate\n",
    "    for i, (_, row) in enumerate(subset.iterrows()):\n",
    "        if row[\"degenerate\"] == \"YES\":\n",
    "            ax.annotate(\"FAILS\", (i, errs[i]),\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=8, color=\"red\", fontweight=\"bold\")\n",
    "\n",
    "fig.suptitle(\"Stress tests: adversarial inputs reveal concrete FP16 failure modes\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nKey findings:\")\n",
    "print(\"  - Softmax: low precision can erase tiny logit differences at large magnitude, collapsing rank information.\")\n",
    "print(\"  - LayerNorm: low precision can collapse near-zero-variance inputs, distorting normalized outputs.\")\n",
    "print(\"  - Naive CE: -log(softmax(.)) in low precision can hit log(0); fused F.cross_entropy is much more stable.\")\n",
    "print(\"  - These are precisely why AMP favors FP32 for sensitive reductions/log-space operations.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Watch dtype flow through a transformer (4 configurations)\n",
    "\n",
    "This is the \"visceral\" version of the operator policy: instead of probing individual ops, we observe dtypes flowing through a transformer forward pass.\n",
    "\n",
    "**Practical model:** if you have `transformers` installed, we'll trace dtypes through the Hugging Face `facebook/opt-125m` model (OPT-125M). Otherwise we fall back to a tiny self-contained transformer (`TinyGPT`) that still exercises the same building blocks (embeddings, LayerNorm, linear projections, residuals).\n",
    "\n",
    "The next cell can download OPT-125M once and cache it in a local directory (so the rest of the notebook doesn't depend on your global Hugging Face cache). Set `ALLOW_OPT_DOWNLOAD = False` to run the trace only if the weights/tokenizer are already cached locally.\n",
    "\n",
    "We'll test all four combinations from the toy example (source1):\n",
    "\n",
    "| Config | Model params | Autocast | What to observe |\n",
    "|---|---|---|---|\n",
    "| A | FP32 | OFF | Everything FP32 (baseline) |\n",
    "| B | FP16/BF16 | OFF | Everything 16-bit (no policy) |\n",
    "| C | FP16/BF16 | ON | LayerNorm→FP32, Linear→16-bit, residuals→16-bit |\n",
    "| D | FP32 | ON | Linear→16-bit even with FP32 weights! LayerNorm stays FP32, residuals→FP32 (promotion) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Self-contained tiny transformer (TinyGPT) used later (and as a dtype-tracing fallback).\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        att = att.masked_fill(mask, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer=2, n_embd=128, n_heads=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(n_embd, n_heads, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"TinyGPT defined: self-contained transformer for later experiments (and as a dtype-tracing fallback).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pick a model for dtype tracing: OPT-125M if available, else TinyGPT.\n",
    "#\n",
    "# Why the indirection?\n",
    "# - OPT-125M makes the trace look like real-world transformer code (Hugging Face module names).\n",
    "# - But it adds optional deps and (unless cached) a ~500MB download, so we keep a no-download fallback.\n",
    "\n",
    "TRACE_MODEL_KIND = \"tinygpt\"  # \"opt-125m\" if successfully loaded\n",
    "TRACE_OPT_MODEL_NAME = \"facebook/opt-125m\"\n",
    "\n",
    "# Download/caching controls (keep all \"extra dependencies\" and downloads in this one cell).\n",
    "ALLOW_OPT_DOWNLOAD = True\n",
    "from pathlib import Path\n",
    "HF_CACHE_DIR = Path(\"./.cache/hf\").resolve()\n",
    "HF_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trace_hf_model = None\n",
    "trace_hf_inputs = None\n",
    "trace_hf_tokenizer = None\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"[info] transformers not available ({e}); using TinyGPT for dtype tracing.\")\n",
    "    else:\n",
    "        try:\n",
    "            hf_kwargs = {\"cache_dir\": str(HF_CACHE_DIR), \"local_files_only\": not ALLOW_OPT_DOWNLOAD}\n",
    "            print(f\"[info] OPT cache_dir: {HF_CACHE_DIR}\")\n",
    "            trace_hf_tokenizer = AutoTokenizer.from_pretrained(TRACE_OPT_MODEL_NAME, use_fast=True, **hf_kwargs)\n",
    "            trace_hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "                TRACE_OPT_MODEL_NAME,\n",
    "                torch_dtype=torch.float32,\n",
    "                **hf_kwargs,\n",
    "            ).to(device).eval()\n",
    "\n",
    "            prompt = \"Autocast dtypes through OPT-125M.\"\n",
    "            trace_hf_inputs = trace_hf_tokenizer(prompt, return_tensors=\"pt\")\n",
    "            trace_hf_inputs = {k: v.to(device) for k, v in trace_hf_inputs.items()}\n",
    "\n",
    "            TRACE_MODEL_KIND = \"opt-125m\"\n",
    "            print(f\"[info] Using {TRACE_OPT_MODEL_NAME} for dtype tracing.\")\n",
    "        except Exception as e:\n",
    "            trace_hf_model = None\n",
    "            trace_hf_inputs = None\n",
    "            trace_hf_tokenizer = None\n",
    "            if not ALLOW_OPT_DOWNLOAD:\n",
    "                print(\n",
    "                    f\"[info] Could not load {TRACE_OPT_MODEL_NAME} from local cache_dir={HF_CACHE_DIR} ({e}). \"\n",
    "                    \"Set ALLOW_OPT_DOWNLOAD=True to download, or continue with TinyGPT.\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"[info] Could not load {TRACE_OPT_MODEL_NAME} ({e}). Falling back to TinyGPT.\")\n",
    "else:\n",
    "    print(f\"[info] Using TinyGPT for dtype tracing on device_type={device.type} (OPT trace is CUDA-focused).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dtype hooks + 4-configuration trace\n",
    "\n",
    "def install_dtype_hooks(model, watch=(nn.Linear, nn.LayerNorm, nn.Embedding)):\n",
    "    hooks, records = [], []\n",
    "    def make_hook(name):\n",
    "        def hook(m, inp, out):\n",
    "            def dt(x):\n",
    "                return str(x.dtype) if isinstance(x, torch.Tensor) else type(x).__name__\n",
    "            indt = dt(inp[0]) if isinstance(inp, (tuple, list)) and inp else dt(inp)\n",
    "            oudt = dt(out) if not isinstance(out, (tuple, list)) else dt(out[0])\n",
    "            records.append({\"module\": name, \"type\": type(m).__name__, \"in_dtype\": indt, \"out_dtype\": oudt})\n",
    "        return hook\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, watch):\n",
    "            hooks.append(m.register_forward_hook(make_hook(name)))\n",
    "    return hooks, records\n",
    "\n",
    "VOCAB = 128\n",
    "BLOCK = 64\n",
    "idx = torch.randint(0, VOCAB, (2, BLOCK), device=device)\n",
    "\n",
    "dtype_16 = torch.bfloat16\n",
    "if device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    dtype_16 = torch.float16\n",
    "elif device.type == \"mps\":\n",
    "    dtype_16 = torch.float16\n",
    "\n",
    "configs = [\n",
    "    (\"A: params=FP32, autocast=OFF\", torch.float32, False),\n",
    "    (\"B: params=16bit, autocast=OFF\", dtype_16, False),\n",
    "    (\"C: params=16bit, autocast=ON\",  dtype_16, True),\n",
    "    (\"D: params=FP32, autocast=ON\",   torch.float32, True),\n",
    "]\n",
    "\n",
    "import copy\n",
    "\n",
    "for title, param_dt, use_ac in configs:\n",
    "    if TRACE_MODEL_KIND == \"opt-125m\" and trace_hf_model is not None:\n",
    "        # Avoid mutating trace_hf_model in-place across configs.\n",
    "        model = copy.deepcopy(trace_hf_model).to(param_dt).eval()\n",
    "        hooks, rec = install_dtype_hooks(model)\n",
    "        ctx = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "        with torch.inference_mode(), ctx:\n",
    "            _ = model(**trace_hf_inputs, use_cache=False)\n",
    "    else:\n",
    "        model = TinyGPT(VOCAB, BLOCK).to(device).to(param_dt)\n",
    "        hooks, rec = install_dtype_hooks(model)\n",
    "        ctx = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "        with torch.inference_mode(), ctx:\n",
    "            _ = model(idx)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    df = pd.DataFrame(rec)\n",
    "    df[\"count\"] = 1\n",
    "    summary = df.groupby([\"type\", \"in_dtype\", \"out_dtype\"], as_index=False)[\"count\"].sum()\n",
    "    summary = summary.sort_values([\"type\", \"in_dtype\", \"out_dtype\"])\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    display(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 What the traces reveal\n",
    "\n",
    "**Config A (FP32, no autocast):** Everything is FP32. Baseline.\n",
    "\n",
    "**Config B (16-bit, no autocast):** Mostly 16-bit. No per-op autocast policy. Sensitive ops are more exposed to low-precision drift.\n",
    "\n",
    "**Config C (16-bit params, autocast ON):**\n",
    "- On common CUDA policies, LayerNorm frequently shows 16-bit input → **FP32 output** (autocast safety path for normalization).\n",
    "- Linear after LayerNorm often shows **FP32 input → 16-bit output** (autocast routes matmul compute to reduced precision).\n",
    "- This is the key insight: autocast doesn't just \"use 16-bit everywhere.\" It routes different ops to different precisions.\n",
    "\n",
    "**Config D (FP32 params, autocast ON):**\n",
    "- Linear frequently appears as **FP32 input → 16-bit output** (autocast downcasts compute inputs/weights for matmul-like ops).\n",
    "- LayerNorm usually stays FP32 in these traces.\n",
    "- Residual adds: 16-bit + FP32 → **FP32** (dtype promotion)\n",
    "- This is the \"standard\" AMP configuration: model stays in FP32, autocast handles per-op casting.\n",
    "\n",
    "```\n",
    "Config D data flow (FP32 params + autocast):\n",
    "\n",
    "[int64 tokens]\n",
    "       |\n",
    "  +----+---------------------+\n",
    "  |                          |\n",
    "[embed_tok] int64->fp32    [embed_pos] int64->fp32\n",
    "  |                          |\n",
    "  +-------- sum (fp32+fp32->fp32) --------+\n",
    "                                          |\n",
    "                                        [LN1] fp32->fp32\n",
    "                                          |\n",
    "                      +---------+---------+---------+\n",
    "                      |         |                   |\n",
    "                   [q_proj]  [k_proj]           [v_proj]\n",
    "                    fp32->bf16  fp32->bf16      fp32->bf16\n",
    "                      \\         |                 /\n",
    "                       \\        |                /\n",
    "                        +----[attn + softmax]----+   (bf16 compute)\n",
    "                                          |\n",
    "                                    [out_proj] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "                                          |\n",
    "                                        [LN2] fp32->fp32\n",
    "                                          |\n",
    "                                       [fc1] fp32->bf16\n",
    "                                          |\n",
    "                                       [fc2] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "```\n",
    "\n",
    "The promotion at residual connections is a key feature: it prevents precision loss from accumulating through the network's residual stream.\n",
    "\n",
    "> **Note on BatchNorm (for CNN practitioners):** While our transformer example uses LayerNorm, convolutional architectures typically use **BatchNorm**, which maintains running statistics (`running_mean`, `running_var`) that are updated with an exponential moving average across batches. These running statistics must stay in **FP32** — they are long-lived accumulators that would drift significantly in FP16/BF16. PyTorch's autocast handles this automatically (BatchNorm is on the FP32 \"keep\" list), but if you manually cast your model with `.half()` or `.bfloat16()`, the running statistics will also be cast to low precision, potentially corrupting them over many batches. If you must manually cast, keep normalization layers in FP32: `model.half(); for m in model.modules(): if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): m.float()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Autocast boundary flow diagram: visualize dtype transitions per module\n",
    "#\n",
    "# Convert the trace records from the 4-config experiment into a horizontal\n",
    "# stacked-bar visualization showing input→output dtype for each module.\n",
    "\n",
    "# Re-run the 4 configs to collect detailed per-module records for visualization.\n",
    "\n",
    "import copy\n",
    "\n",
    "all_config_records = {}\n",
    "for title, param_dt, use_ac in configs:\n",
    "    if TRACE_MODEL_KIND == \"opt-125m\" and trace_hf_model is not None:\n",
    "        # Keep trace_hf_model pristine for downstream sections (e.g., OPT training).\n",
    "        model_v = copy.deepcopy(trace_hf_model).to(param_dt).eval()\n",
    "        hooks_v, rec_v = install_dtype_hooks(model_v)\n",
    "        ctx_v = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "        with torch.inference_mode(), ctx_v:\n",
    "            _ = model_v(**trace_hf_inputs, use_cache=False)\n",
    "    else:\n",
    "        model_v = TinyGPT(VOCAB, BLOCK).to(device).to(param_dt)\n",
    "        hooks_v, rec_v = install_dtype_hooks(model_v)\n",
    "        ctx_v = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "        with torch.inference_mode(), ctx_v:\n",
    "            _ = model_v(idx)\n",
    "    for h_v in hooks_v:\n",
    "        h_v.remove()\n",
    "    all_config_records[title] = rec_v\n",
    "\n",
    "# Build the visualization\n",
    "dtype_colors = {\n",
    "    \"torch.float32\": \"#3498db\",   # blue\n",
    "    \"torch.float16\": \"#e74c3c\",   # red\n",
    "    \"torch.bfloat16\": \"#2ecc71\",  # green\n",
    "}\n",
    "dtype_short = {\n",
    "    \"torch.float32\": \"FP32\",\n",
    "    \"torch.float16\": \"FP16\",\n",
    "    \"torch.bfloat16\": \"BF16\",\n",
    "}\n",
    "\n",
    "n_configs = len(all_config_records)\n",
    "max_modules = max((len(records) for records in all_config_records.values()), default=1)\n",
    "fig, axes = plt.subplots(1, n_configs, figsize=(5 * n_configs, max(6, 0.22 * max_modules)),\n",
    "                          sharey=True)\n",
    "if n_configs == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (cfg_title, records) in zip(axes, all_config_records.items()):\n",
    "    if not records:\n",
    "        ax.set_title(cfg_title.split(\":\")[0], fontsize=9)\n",
    "        continue\n",
    "    modules = [r[\"module\"] for r in records]\n",
    "    in_dtypes = [r[\"in_dtype\"] for r in records]\n",
    "    out_dtypes = [r[\"out_dtype\"] for r in records]\n",
    "\n",
    "    y_pos = np.arange(len(modules))\n",
    "\n",
    "    # Draw two bars per module: input dtype (left half) and output dtype (right half)\n",
    "    in_colors = [dtype_colors.get(d, \"#95a5a6\") for d in in_dtypes]\n",
    "    out_colors = [dtype_colors.get(d, \"#95a5a6\") for d in out_dtypes]\n",
    "\n",
    "    ax.barh(y_pos, [0.45] * len(modules), left=0, color=in_colors, alpha=0.8,\n",
    "            edgecolor=\"white\", height=0.7, label=\"input dtype\")\n",
    "    ax.barh(y_pos, [0.45] * len(modules), left=0.55, color=out_colors, alpha=0.8,\n",
    "            edgecolor=\"white\", height=0.7, label=\"output dtype\")\n",
    "\n",
    "    # Annotate dtype transitions\n",
    "    for i, (in_d, out_d) in enumerate(zip(in_dtypes, out_dtypes)):\n",
    "        in_label = dtype_short.get(in_d, in_d.replace(\"torch.\", \"\"))\n",
    "        out_label = dtype_short.get(out_d, out_d.replace(\"torch.\", \"\"))\n",
    "        ax.text(0.225, i, in_label, ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "        ax.text(0.775, i, out_label, ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "        # Arrow marker for dtype promotion/demotion\n",
    "        if in_d != out_d:\n",
    "            ax.annotate(\"\", xy=(0.52, i), xytext=(0.48, i),\n",
    "                        arrowprops=dict(arrowstyle=\"->\", color=\"orange\", lw=1.5))\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([m[:30] for m in modules], fontsize=6)\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_xticks([0.225, 0.775])\n",
    "    ax.set_xticklabels([\"input\", \"output\"], fontsize=8)\n",
    "    ax.set_title(cfg_title.split(\":\")[0] + \":\" + cfg_title.split(\":\", 1)[1][:40], fontsize=8)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_handles = [Patch(color=c, label=l) for l, c in\n",
    "                  [(\"FP32\", \"#3498db\"), (\"FP16\", \"#e74c3c\"), (\"BF16\", \"#2ecc71\"), (\"Other\", \"#95a5a6\")]]\n",
    "fig.legend(handles=legend_handles, loc=\"lower center\", ncol=4, fontsize=8,\n",
    "           bbox_to_anchor=(0.5, -0.02))\n",
    "fig.suptitle(\"Autocast dtype flow: input→output per module across 4 configurations\",\n",
    "             fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "print(\"Orange arrows mark dtype transitions (promotion or demotion) between input and output.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Per-layer precision sensitivity: which parts of the transformer hurt most?\n",
    "\n",
    "The dtype hooks above show *what* dtype each layer uses. But a deeper question is: **which layers are most affected by the precision change?**\n",
    "\n",
    "We'll do this on `TinyGPT` (the self-contained transformer above) because it's small enough to hook every module without external downloads or long runtimes. The takeaways generalize to OPT-125M and other LLMs: attention/logits, reductions, and normalization are usually where precision matters most."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-layer output error: FP32 vs autocast\n",
    "\n",
    "import copy\n",
    "\n",
    "set_seed(0)\n",
    "sens_model = TinyGPT(VOCAB, BLOCK, n_layer=2, n_embd=128, n_heads=4, dropout=0.0).to(device).float()\n",
    "\n",
    "# Collect per-module outputs under FP32 and autocast\n",
    "def collect_outputs(model, idx, use_autocast_flag, amp_dt):\n",
    "    outputs = {}\n",
    "    hooks = []\n",
    "    def make_hook(name):\n",
    "        def hook(m, inp, out):\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                outputs[name] = out.detach().float().cpu().clone()\n",
    "            elif isinstance(out, (tuple, list)) and len(out) > 0 and isinstance(out[0], torch.Tensor):\n",
    "                outputs[name] = out[0].detach().float().cpu().clone()\n",
    "        return hook\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if name:  # skip root\n",
    "            hooks.append(m.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        ctx = amp_autocast(device, amp_dt, enabled=use_autocast_flag)\n",
    "        with ctx:\n",
    "            _ = model(idx)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    return outputs\n",
    "\n",
    "idx_sens = torch.randint(0, VOCAB, (2, BLOCK), device=device)\n",
    "\n",
    "out_fp32 = collect_outputs(sens_model, idx_sens, False, None)\n",
    "\n",
    "dtype_16 = torch.bfloat16\n",
    "if device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    dtype_16 = torch.float16\n",
    "elif device.type == \"mps\":\n",
    "    dtype_16 = torch.float16\n",
    "\n",
    "out_ac = collect_outputs(sens_model, idx_sens, True, dtype_16)\n",
    "\n",
    "# Compare\n",
    "rows = []\n",
    "for name in sorted(out_fp32.keys()):\n",
    "    if name not in out_ac:\n",
    "        continue\n",
    "    o32 = out_fp32[name]\n",
    "    oac = out_ac[name]\n",
    "    if o32.shape != oac.shape:\n",
    "        continue\n",
    "    abs_err = (oac - o32).abs()\n",
    "    rel_err = abs_err / (o32.abs() + 1e-8)\n",
    "    rows.append({\n",
    "        \"module\": name,\n",
    "        \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "        \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "        \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "        \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "    })\n",
    "\n",
    "df_sens = pd.DataFrame(rows)\n",
    "\n",
    "# Sort by mean_rel_err descending to show most sensitive layers first\n",
    "df_sens[\"_sort\"] = df_sens[\"mean_rel_err\"].apply(lambda x: float(x))\n",
    "df_sens = df_sens.sort_values(\"_sort\", ascending=False).drop(columns=[\"_sort\"])\n",
    "\n",
    "print(f\"Per-module output error: FP32 vs autocast({dtype_16})\")\n",
    "print(f\"{'='*70}\")\n",
    "display(df_sens)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Layers with HIGHER error are more precision-sensitive.\")\n",
    "print(\"  - LayerNorm, softmax, and final head outputs tend to show larger errors\")\n",
    "print(\"    because they involve reductions and normalization.\")\n",
    "print(\"  - Linear layers often show small errors because Tensor Cores accumulate in FP32.\")\n",
    "print(\"  - This explains why autocast's policy protects normalization and loss ops in FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize per-layer sensitivity as a bar chart\n",
    "\n",
    "if len(rows) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, max(4, len(rows) * 0.3)))\n",
    "\n",
    "    names_plot = df_sens[\"module\"].tolist()\n",
    "    errors_plot = [float(x) for x in df_sens[\"mean_rel_err\"].tolist()]\n",
    "\n",
    "    # Color by module type\n",
    "    colors_plot = []\n",
    "    for n in names_plot:\n",
    "        if \"ln\" in n.lower() or \"norm\" in n.lower():\n",
    "            colors_plot.append(\"#e74c3c\")  # red for normalization\n",
    "        elif \"attn\" in n.lower() or \"qkv\" in n.lower() or \"proj\" in n.lower():\n",
    "            colors_plot.append(\"#3498db\")  # blue for attention\n",
    "        elif \"mlp\" in n.lower() or \"fc\" in n.lower():\n",
    "            colors_plot.append(\"#2ecc71\")  # green for MLP\n",
    "        elif \"head\" in n.lower():\n",
    "            colors_plot.append(\"#9b59b6\")  # purple for output head\n",
    "        else:\n",
    "            colors_plot.append(\"#95a5a6\")  # gray for other\n",
    "\n",
    "    y_pos = range(len(names_plot))\n",
    "    ax.barh(y_pos, errors_plot, color=colors_plot, alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(names_plot, fontsize=7)\n",
    "    ax.set_xlabel(\"Mean relative error (autocast vs FP32)\")\n",
    "    ax.set_title(\"Per-layer precision sensitivity: which modules are most affected by autocast?\", fontsize=11)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_el = [\n",
    "        Patch(color=\"#e74c3c\", label=\"Normalization (most sensitive)\"),\n",
    "        Patch(color=\"#3498db\", label=\"Attention\"),\n",
    "        Patch(color=\"#2ecc71\", label=\"MLP/Linear\"),\n",
    "        Patch(color=\"#9b59b6\", label=\"Output head\"),\n",
    "        Patch(color=\"#95a5a6\", label=\"Other\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_el, loc=\"lower right\", fontsize=8)\n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Per-layer *gradient* sensitivity: which layers contribute most error in the backward pass?\n",
    "\n",
    "The forward-pass sensitivity above shows output differences. But for training, what matters is whether **gradients** are corrupted. Here we measure: if you force *only one layer* to FP16 (while everything else stays FP32), how much does each layer's gradient contribution change?\n",
    "\n",
    "This directly answers: *\"If I need to keep some layers in FP32 for training stability, which ones?\"*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-layer gradient sensitivity: force one layer at a time to FP16\n",
    "\n",
    "import copy\n",
    "\n",
    "set_seed(0)\n",
    "model_ref = TinyGPT(vocab_size=128, block_size=64, n_layer=2, n_embd=128, n_heads=4, dropout=0.0).to(device).float()\n",
    "idx_gs = torch.randint(0, 128, (2, 64), device=device)\n",
    "x_gs = idx_gs[:, :-1]\n",
    "y_gs = idx_gs[:, 1:]\n",
    "\n",
    "# Reference: full FP32 forward + backward\n",
    "model_ref.zero_grad()\n",
    "logits_ref = model_ref(x_gs)\n",
    "loss_ref = F.cross_entropy(logits_ref.reshape(-1, logits_ref.size(-1)),\n",
    "                            y_gs.reshape(-1))\n",
    "loss_ref.backward()\n",
    "ref_grads = {}\n",
    "for name, p in model_ref.named_parameters():\n",
    "    if p.grad is not None:\n",
    "        ref_grads[name] = p.grad.detach().clone().float()\n",
    "\n",
    "def cast_floating(x, dtype):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(dtype) if x.is_floating_point() else x\n",
    "    if isinstance(x, tuple):\n",
    "        return tuple(cast_floating(v, dtype) for v in x)\n",
    "    if isinstance(x, list):\n",
    "        return [cast_floating(v, dtype) for v in x]\n",
    "    return x\n",
    "\n",
    "# For each named module with parameters, force ONLY that module to FP16\n",
    "grad_sensitivity = []\n",
    "for mod_name, mod in model_ref.named_modules():\n",
    "    params_in_mod = list(mod.parameters(recurse=False))\n",
    "    if not params_in_mod:\n",
    "        continue\n",
    "\n",
    "    # Deep copy model, cast only this module to FP16.\n",
    "    # Hooks cast module boundaries so the rest of the network stays FP32.\n",
    "    model_test = copy.deepcopy(model_ref).float()\n",
    "    mod_map = dict(model_test.named_modules())\n",
    "    target_mod = mod_map.get(mod_name, None)\n",
    "    if target_mod is None:\n",
    "        continue\n",
    "    target_mod.half()\n",
    "\n",
    "    h_pre = target_mod.register_forward_pre_hook(\n",
    "        lambda module, args: cast_floating(args, torch.float16)\n",
    "    )\n",
    "    h_post = target_mod.register_forward_hook(\n",
    "        lambda module, args, out: cast_floating(out, torch.float32)\n",
    "    )\n",
    "\n",
    "    model_test.zero_grad(set_to_none=True)\n",
    "    try:\n",
    "        logits_test = model_test(x_gs)\n",
    "        loss_test = F.cross_entropy(logits_test.reshape(-1, logits_test.size(-1)),\n",
    "                                     y_gs.reshape(-1))\n",
    "        loss_test.backward()\n",
    "    except Exception as exc:\n",
    "        grad_sensitivity.append({\"module\": mod_name, \"type\": type(mod).__name__,\n",
    "                                  \"grad_rel_err\": float(\"nan\"), \"note\": str(exc).splitlines()[0]})\n",
    "        h_pre.remove()\n",
    "        h_post.remove()\n",
    "        continue\n",
    "    h_pre.remove()\n",
    "    h_post.remove()\n",
    "\n",
    "    # Measure gradient error across ALL parameters (not just this module's)\n",
    "    total_err = 0.0\n",
    "    total_norm = 0.0\n",
    "    for name, p in model_test.named_parameters():\n",
    "        if p.grad is not None and name in ref_grads:\n",
    "            err = (p.grad.float() - ref_grads[name]).abs().sum().item()\n",
    "            norm = ref_grads[name].abs().sum().item() + 1e-12\n",
    "            total_err += err\n",
    "            total_norm += norm\n",
    "\n",
    "    grad_sensitivity.append({\n",
    "        \"module\": mod_name if mod_name else \"(root)\",\n",
    "        \"type\": type(mod).__name__,\n",
    "        \"grad_rel_err\": total_err / total_norm,\n",
    "        \"note\": \"\",\n",
    "    })\n",
    "\n",
    "# Sort and plot\n",
    "df_gs_all = pd.DataFrame(grad_sensitivity)\n",
    "df_failed = df_gs_all[df_gs_all[\"grad_rel_err\"].isna()].copy()\n",
    "df_gs = df_gs_all.dropna(subset=[\"grad_rel_err\"]).copy()\n",
    "df_gs = df_gs[df_gs[\"grad_rel_err\"] > 0].sort_values(\"grad_rel_err\", ascending=False)\n",
    "\n",
    "if len(df_gs) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, max(4, len(df_gs) * 0.35)))\n",
    "\n",
    "    mod_names = df_gs[\"module\"].tolist()\n",
    "    mod_types = df_gs[\"type\"].tolist()\n",
    "    errs = df_gs[\"grad_rel_err\"].tolist()\n",
    "\n",
    "    colors_gs = []\n",
    "    for n, t in zip(mod_names, mod_types):\n",
    "        n_l = n.lower()\n",
    "        t_l = t.lower()\n",
    "        if \"ln\" in n_l or \"norm\" in n_l or \"layernorm\" in t_l:\n",
    "            colors_gs.append(\"#e74c3c\")\n",
    "        elif \"attn\" in n_l or \"attention\" in n_l or \"qkv\" in n_l:\n",
    "            colors_gs.append(\"#3498db\")\n",
    "        elif \"mlp\" in n_l or \"ffn\" in n_l or \"fc\" in n_l or \"linear\" in t_l:\n",
    "            colors_gs.append(\"#2ecc71\")\n",
    "        else:\n",
    "            colors_gs.append(\"#95a5a6\")\n",
    "\n",
    "    y_pos = range(len(mod_names))\n",
    "    ax.barh(y_pos, errs, color=colors_gs, alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([f\"{n} ({t})\" for n, t in zip(mod_names, mod_types)], fontsize=7)\n",
    "    ax.set_xlabel(\"Relative gradient error (forcing this layer to FP16)\")\n",
    "    ax.set_title(\"Per-layer gradient sensitivity: which module hurts training most in FP16?\", fontsize=11)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_gs = [\n",
    "        Patch(color=\"#e74c3c\", label=\"LayerNorm\"),\n",
    "        Patch(color=\"#3498db\", label=\"Attention\"),\n",
    "        Patch(color=\"#2ecc71\", label=\"Linear/MLP\"),\n",
    "        Patch(color=\"#95a5a6\", label=\"Other\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_gs, loc=\"lower right\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"  - Bars are sorted from highest to lowest gradient sensitivity.\")\n",
    "    print(\"  - LayerNorm and attention modules typically dominate (reductions + softmax).\")\n",
    "    print(\"  - Linear layers are often less sensitive than normalization/attention in this test.\")\n",
    "    print(\"    (On CUDA Tensor Cores, FP32 accumulation in matmuls is a big reason.)\")\n",
    "    print(\"  - If you must keep some layers in FP32 for stability, prioritize the top bars.\")\n",
    "else:\n",
    "    print(\"No gradient sensitivity data collected (need TinyGPT defined above).\")\n",
    "\n",
    "if len(df_failed) > 0:\n",
    "    print(\"\\nModules skipped (no valid mixed-precision backward on this device/runtime):\")\n",
    "    display(df_failed[[\"module\", \"type\", \"note\"]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gradient underflow and why loss scaling works\n",
    "\n",
    "We'll do a controlled experiment:\n",
    "1. Create a synthetic gradient distribution spanning many orders of magnitude.\n",
    "2. Cast it to FP16 and count how many values become exactly 0.\n",
    "3. Apply a scale factor $S$, cast, then unscale.\n",
    "\n",
    "This shows the core mechanism of loss scaling without needing a full training run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient underflow + rescue via scaling\n",
    "\n",
    "N = 200_000\n",
    "log10_mag = torch.empty(N).uniform_(-12, 0)  # 1e-12 to 1\n",
    "sign = torch.randint(0, 2, (N,)) * 2 - 1\n",
    "synthetic = (10 ** log10_mag) * sign\n",
    "synthetic = synthetic.to(torch.float32)\n",
    "\n",
    "rows = []\n",
    "for S_label, S in [(\"unscaled (S=1)\", 1), (\"S=2^10 (1024)\", 2**10), (\"S=2^13 (8192)\", 2**13), (\"S=2^16 (65536)\", 2**16)]:\n",
    "    scaled = synthetic * S\n",
    "    for dt in [torch.float16, torch.bfloat16]:\n",
    "        g = scaled.to(dt)\n",
    "        zeros = float((g == 0).float().mean())\n",
    "        infs = float(torch.isinf(g).float().mean())\n",
    "        rows.append({\n",
    "            \"scaling\": S_label,\n",
    "            \"dtype\": str(dt),\n",
    "            \"zero_frac\": f\"{zeros:.3f}\",\n",
    "            \"inf_frac\": f\"{infs:.4f}\",\n",
    "            \"preserved_frac\": f\"{1 - zeros - infs:.3f}\",\n",
    "        })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize gradient distribution vs FP16 thresholds\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal = float(fi16.tiny)\n",
    "min_sub = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "vals = synthetic.abs().cpu().numpy()\n",
    "axes[0].hist(np.log10(vals + 1e-30), bins=200, alpha=0.7, color=\"steelblue\")\n",
    "axes[0].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=f\"FP16 min normal ({min_normal:.1e})\")\n",
    "axes[0].axvline(np.log10(min_sub), color=\"m\", ls=\":\", label=f\"FP16 min subnormal ({min_sub:.1e})\")\n",
    "axes[0].set_title(\"Unscaled |grad| distribution\")\n",
    "axes[0].set_xlabel(\"log10(|grad|)\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# After scaling by 2^13\n",
    "scaled_vals = (synthetic * 2**13).abs().cpu().numpy()\n",
    "axes[1].hist(np.log10(scaled_vals + 1e-30), bins=200, alpha=0.7, color=\"darkorange\")\n",
    "axes[1].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=\"FP16 min normal\")\n",
    "axes[1].axvline(np.log10(float(fi16.max)), color=\"darkred\", ls=\"-.\", label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "axes[1].set_title(\"Scaled by 2^13: distribution shifts right\")\n",
    "axes[1].set_xlabel(\"log10(|grad| * S)\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Loss scaling shifts gradients into FP16's representable range\", fontsize=11, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Underflow in a real backward pass\n",
    "\n",
    "Now we show the actual training failure mode with a tiny FP16 network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Real backward underflow demo\n",
    "\n",
    "def tiny_backward(use_scaling, scale=2**13):\n",
    "    model = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1)).to(device).half()\n",
    "    x = (torch.randn(256, 128, device=device) * 1e-3).half()\n",
    "    y = (torch.randn(256, 1, device=device) * 1e-3).half()\n",
    "    pred = model(x)\n",
    "    loss = ((pred - y)**2).mean()\n",
    "    if use_scaling:\n",
    "        (loss * scale).backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.div_(scale)\n",
    "    else:\n",
    "        loss.backward()\n",
    "    grads = torch.cat([p.grad.flatten().abs().float() for p in model.parameters() if p.grad is not None])\n",
    "    return float(loss), float((grads == 0).float().mean()), float(grads.median()), grads\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    loss0, z0, med0, g0 = tiny_backward(use_scaling=False)\n",
    "    loss1, z1, med1, g1 = tiny_backward(use_scaling=True)\n",
    "    display(pd.DataFrame([\n",
    "        {\"setting\": \"FP16, no scaling\", \"loss\": f\"{loss0:.6f}\", \"zero_grad_frac\": f\"{z0:.3f}\", \"median|grad|\": f\"{med0:.2e}\"},\n",
    "        {\"setting\": \"FP16, scaled+unscaled\", \"loss\": f\"{loss1:.6f}\", \"zero_grad_frac\": f\"{z1:.3f}\", \"median|grad|\": f\"{med1:.2e}\"},\n",
    "    ]))\n",
    "else:\n",
    "    print(\"Run on CUDA for the FP16 backward demo.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 The Micikevicius gradient histogram: where do real gradients live?\n",
    "\n",
    "The Micikevicius paper's most famous figure shows a histogram of gradient magnitudes during FP32 training, overlaid with FP16's representable range. Let's reproduce this analysis with our model.\n",
    "\n",
    "This visualization answers the question: *What fraction of the training signal would you lose by switching to FP16 or BF16?*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient histogram analysis (Micikevicius-style)\n",
    "\n",
    "def collect_gradient_histogram(model_class, device, dtype=torch.float32, steps=20):\n",
    "    # Collect all gradient values from several training steps.\n",
    "    set_seed(42)\n",
    "    model = model_class().to(device).to(dtype)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    all_grads = []\n",
    "\n",
    "    for xb, yb in get_batches(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        pred = model(xb.to(dtype))\n",
    "        loss = F.mse_loss(pred, yb.to(dtype))\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                all_grads.append(p.grad.detach().float().flatten().cpu())\n",
    "        opt.step()\n",
    "\n",
    "    return torch.cat(all_grads)\n",
    "\n",
    "grads_fp32 = collect_gradient_histogram(SimpleMLP, device, torch.float32, steps=30)\n",
    "nonzero_grads = grads_fp32[grads_fp32 != 0]\n",
    "log_abs_grads = torch.log10(nonzero_grads.abs()).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: gradient histogram with FP16 thresholds\n",
    "ax = axes[0]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal_16 = float(fi16.tiny)\n",
    "min_sub_16 = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=2, label=f\"FP16 min normal ({min_normal_16:.1e})\")\n",
    "ax.axvline(np.log10(min_sub_16), color=\"darkred\", ls=\":\", lw=1.5, label=f\"FP16 min subnormal ({min_sub_16:.1e})\")\n",
    "ax.axvline(np.log10(float(fi16.max)), color=\"orange\", ls=\"-.\", lw=1.5, label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "\n",
    "# Shade the underflow zone\n",
    "ax.axvspan(ax.get_xlim()[0], np.log10(min_sub_16), alpha=0.15, color=\"red\", label=\"FP16 underflow zone\")\n",
    "\n",
    "ax.set_title(\"FP32 gradient magnitudes vs FP16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "# Right: same but with BF16 thresholds\n",
    "ax = axes[1]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi_bf16 = torch.finfo(torch.bfloat16)\n",
    "min_normal_bf16 = float(fi_bf16.tiny)\n",
    "\n",
    "ax.axvline(np.log10(min_normal_bf16), color=\"green\", ls=\"--\", lw=2, label=f\"BF16 min normal ({min_normal_bf16:.1e})\")\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=1.5, alpha=0.5, label=f\"FP16 min normal (for comparison)\")\n",
    "\n",
    "ax.set_title(\"Same gradients vs BF16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "fig.suptitle(\"Micikevicius-style analysis: gradient magnitude distribution\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Quantify\n",
    "fp16_underflow_frac = float((nonzero_grads.abs() < min_normal_16).float().mean())\n",
    "bf16_underflow_frac = float((nonzero_grads.abs() < min_normal_bf16).float().mean())\n",
    "print(f\"\\nGradients collected: {len(nonzero_grads):,}\")\n",
    "print(f\"Fraction that would underflow in FP16: {fp16_underflow_frac:.4f} ({fp16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"Fraction that would underflow in BF16: {bf16_underflow_frac:.4f} ({bf16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"\\nThis is why FP16 needs loss scaling and BF16 usually doesn't.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2b Gradient distribution evolution: how the histogram shifts during training\n",
    "\n",
    "The Micikevicius snapshot above is from early training. But gradient distributions **shift** as training progresses: early gradients tend to be larger (random initialization → big loss), late gradients tend to be smaller (convergence → tiny updates). This matters because FP16 underflow becomes *worse* as training proceeds.\n",
    "\n",
    "Here we train TinyGPT for ~200 steps and capture full gradient snapshots at checkpoints, showing:\n",
    "1. How the gradient distribution shifts leftward (toward smaller magnitudes) over training.\n",
    "2. How a representative loss scale would shift that distribution into FP16's representable range."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient distribution evolution over training\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Self-contained mini-corpus (avoids dependency on Section 3.6 data)\n",
    "SNAP_STEPS = [0, 10, 50, 100, 200]\n",
    "TOTAL_STEPS = max(SNAP_STEPS) + 1\n",
    "BLOCK_EVO = 64\n",
    "BATCH_EVO = 32\n",
    "VOCAB_EVO = 128\n",
    "\n",
    "train_data_evo = torch.randint(0, VOCAB_EVO, (BLOCK_EVO * BATCH_EVO * 10,), device=device)\n",
    "\n",
    "model_evo = TinyGPT(vocab_size=VOCAB_EVO, block_size=BLOCK_EVO, n_layer=2,\n",
    "                     n_embd=128, n_heads=4, dropout=0.0).to(device).float()\n",
    "opt_evo = torch.optim.Adam(model_evo.parameters(), lr=3e-4)\n",
    "\n",
    "grad_snapshots = {}\n",
    "LOSS_SCALE_DEMO = 1024.0  # fixed scale for illustration\n",
    "\n",
    "for step in range(TOTAL_STEPS):\n",
    "    # Sample batch from self-contained mini-corpus\n",
    "    max_start = train_data_evo.size(0) - BLOCK_EVO - 1\n",
    "    ix = torch.randint(0, max_start, (BATCH_EVO,), device=device)\n",
    "    offsets = torch.arange(BLOCK_EVO, device=device).unsqueeze(0)\n",
    "    xb = train_data_evo[ix.unsqueeze(1) + offsets]\n",
    "    yb = train_data_evo[ix.unsqueeze(1) + offsets + 1]\n",
    "\n",
    "    opt_evo.zero_grad(set_to_none=True)\n",
    "    logits_evo = model_evo(xb)\n",
    "    loss_evo = F.cross_entropy(logits_evo.reshape(-1, logits_evo.size(-1)), yb.reshape(-1))\n",
    "    loss_evo.backward()\n",
    "\n",
    "    if step in SNAP_STEPS:\n",
    "        all_g = torch.cat([p.grad.detach().float().flatten().cpu()\n",
    "                           for p in model_evo.parameters() if p.grad is not None])\n",
    "        grad_snapshots[step] = all_g\n",
    "\n",
    "    opt_evo.step()\n",
    "\n",
    "# Plot: one subplot per snapshot\n",
    "n_snaps = len(grad_snapshots)\n",
    "fig, axes = plt.subplots(1, n_snaps, figsize=(4 * n_snaps, 5), sharey=True)\n",
    "if n_snaps == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal_fp16 = float(fi16.tiny)\n",
    "min_sub_fp16 = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16),\n",
    "                                       torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "for ax, (step, grads) in zip(axes, sorted(grad_snapshots.items())):\n",
    "    nonzero = grads[grads != 0]\n",
    "    if len(nonzero) == 0:\n",
    "        continue\n",
    "    log_abs = torch.log10(nonzero.abs()).numpy()\n",
    "\n",
    "    # Unscaled distribution\n",
    "    ax.hist(log_abs, bins=150, alpha=0.6, color=\"steelblue\", edgecolor=\"none\",\n",
    "            density=True, label=\"Unscaled gradients\")\n",
    "\n",
    "    # Scaled distribution (shifted by log10(scale))\n",
    "    log_abs_scaled = log_abs + np.log10(LOSS_SCALE_DEMO)\n",
    "    ax.hist(log_abs_scaled, bins=150, alpha=0.4, color=\"orange\", edgecolor=\"none\",\n",
    "            density=True, label=f\"Scaled (×{LOSS_SCALE_DEMO:.0f})\")\n",
    "\n",
    "    # FP16 range markers\n",
    "    ax.axvline(np.log10(min_normal_fp16), color=\"red\", ls=\"--\", lw=1.5, alpha=0.8)\n",
    "    ax.axvline(np.log10(min_sub_fp16), color=\"darkred\", ls=\":\", lw=1, alpha=0.6)\n",
    "    ax.axvspan(ax.get_xlim()[0] if ax.get_xlim()[0] < np.log10(min_sub_fp16) else -20,\n",
    "               np.log10(min_sub_fp16), alpha=0.1, color=\"red\")\n",
    "\n",
    "    # Fractions below FP16 thresholds\n",
    "    below_normal = float((nonzero.abs() < min_normal_fp16).float().mean())\n",
    "    true_underflow = float((nonzero.abs() < min_sub_fp16).float().mean())\n",
    "    ax.set_title(\n",
    "        f\"Step {step}\\n(<min normal: {below_normal*100:.1f}%, <min sub: {true_underflow*100:.2f}%)\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "    ax.set_xlabel(\"log₁₀(|grad|)\", fontsize=8)\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel(\"density\", fontsize=8)\n",
    "\n",
    "axes[0].legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "fig.suptitle(\"Gradient distribution evolution: how gradients shift during training\\n\"\n",
    "             f\"(red dashed = FP16 min normal, dark-red dotted = min subnormal)\",\n",
    "             fontsize=11, y=1.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\nGradient evolution summary (FP16 thresholds):\")\n",
    "for step, grads in sorted(grad_snapshots.items()):\n",
    "    nonzero = grads[grads != 0]\n",
    "    frac_below_normal = float((nonzero.abs() < min_normal_fp16).float().mean()) * 100\n",
    "    frac_true_underflow = float((nonzero.abs() < min_sub_fp16).float().mean()) * 100\n",
    "    print(f\"  Step {step:>3d}: {frac_below_normal:.2f}% < min normal, {frac_true_underflow:.4f}% < min subnormal\")\n",
    "print(f\"\\nTakeaway: As training converges, more gradients move into FP16's low-magnitude region.\")\n",
    "print(f\"The orange overlay is a fixed-scale what-if (not dynamic GradScaler), showing why scaling helps.\")\n",
    "\n",
    "del model_evo, opt_evo, train_data_evo"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Dynamic loss scaling (GradScaler) in action\n",
    "\n",
    "Loss scaling has two failure modes:\n",
    "\n",
    "- **Scale too small** → doesn't rescue underflow (gradients still become 0 in FP16).\n",
    "- **Scale too large** → causes overflow (gradients become `inf`/`nan`).\n",
    "\n",
    "`GradScaler` automates this tradeoff: it tries to keep the scale as large as possible *without* overflow.\n",
    "\n",
    "In this demo we intentionally start with an absurdly large scale to trigger overflows, and watch GradScaler back off and skip optimizer steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# GradScaler overflow + step skipping demo (CUDA only)\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Run on CUDA to see GradScaler dynamically adjust scale and skip steps.\")\n",
    "else:\n",
    "    set_seed(0)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    model = nn.Linear(256, 256, bias=False).to(device).float()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Large inputs to create large (but finite) gradients.\n",
    "    x = (torch.randn(256, 256, device=device) * 5000).float()\n",
    "\n",
    "    try:\n",
    "        scaler = GradScaler(\n",
    "            init_scale=2**16,\n",
    "            growth_factor=2.0,\n",
    "            backoff_factor=0.5,\n",
    "            growth_interval=2,\n",
    "            enabled=True,\n",
    "        )\n",
    "    except TypeError:\n",
    "        scaler = GradScaler(enabled=True)\n",
    "        print(\"[warn] GradScaler init args not supported on this version; using defaults.\")\n",
    "\n",
    "    logs = []\n",
    "    for step in range(12):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scale_before = float(scaler.get_scale()) if hasattr(scaler, \"get_scale\") else float(\"nan\")\n",
    "\n",
    "        with amp_autocast(device, torch.float16, enabled=True):\n",
    "            y = model(x)  # FP16 matmul compute under autocast\n",
    "            # Force loss computation in FP32 to avoid forward overflow; we want overflow from scaling.\n",
    "            loss = (y.float() ** 2).mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale so we can inspect gradients in their true magnitude (and clip here if desired).\n",
    "        scaler.unscale_(opt)\n",
    "        grads = [p.grad for p in model.parameters() if p.grad is not None]\n",
    "        found_inf = bool(any((~torch.isfinite(g)).any().item() for g in grads))\n",
    "        max_abs_grad = float(torch.stack([g.detach().abs().max() for g in grads]).max())\n",
    "\n",
    "        w_before = model.weight.detach().float().clone()\n",
    "        scaler.step(opt)     # skipped if found_inf=True\n",
    "        scaler.update()\n",
    "        w_after = model.weight.detach().float()\n",
    "\n",
    "        scale_after = float(scaler.get_scale()) if hasattr(scaler, \"get_scale\") else float(\"nan\")\n",
    "        step_ran = bool((w_after - w_before).abs().max().item() != 0.0)\n",
    "\n",
    "        logs.append({\n",
    "            \"step\": step,\n",
    "            \"loss(fp32)\": float(loss.detach().cpu()),\n",
    "            \"scale_before\": scale_before,\n",
    "            \"found_inf\": found_inf,\n",
    "            \"optimizer_step_ran\": step_ran,\n",
    "            \"max_abs_grad(after_unscale)\": max_abs_grad,\n",
    "            \"scale_after\": scale_after,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(logs)\n",
    "    display(df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    ax.plot(df[\"step\"], df[\"scale_before\"], marker=\"o\", label=\"scale_before\")\n",
    "    ax.plot(df[\"step\"], df[\"scale_after\"], marker=\"o\", ls=\"--\", label=\"scale_after\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.set_ylabel(\"scale (log)\")\n",
    "    ax.set_title(\"GradScaler: backoff on overflow + step skipping\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[\"step\"], df[\"found_inf\"].astype(int), color=\"red\", alpha=0.3, lw=2, label=\"found_inf\")\n",
    "    ax2.set_ylabel(\"found_inf (0/1)\")\n",
    "    plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Weight update stagnation (why FP32 master weights matter)\n",
    "\n",
    "Even if you avoid underflow, you can lose learning signal if weight updates are **below the ULP** of the weight's dtype.\n",
    "\n",
    "If $w \\approx 1$ in FP16, the ULP is ~$10^{-3}$. Any update $\\Delta w < 10^{-3}$ is rounded away."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weight stagnation demo\n",
    "\n",
    "def apply_updates(dtype, w0=1.0, delta=1e-5, steps=2000):\n",
    "    w = torch.tensor(w0, dtype=dtype)\n",
    "    changed = 0\n",
    "    for _ in range(steps):\n",
    "        w_new = w - torch.tensor(delta, dtype=dtype)\n",
    "        changed += int(w_new.item() != w.item())\n",
    "        w = w_new\n",
    "    return {\n",
    "        \"dtype\": str(dtype),\n",
    "        \"w0\": w0,\n",
    "        \"delta\": f\"{delta:.0e}\",\n",
    "        \"steps\": steps,\n",
    "        \"steps_where_w_changed\": changed,\n",
    "        \"final_w\": f\"{float(w):.6f}\",\n",
    "        \"expected_final\": f\"{w0 - delta * steps:.6f}\",\n",
    "        \"ulp_at_1.0\": f\"{_ulp_at_one(dtype):.2e}\",\n",
    "    }\n",
    "\n",
    "rows = [apply_updates(dt) for dt in [torch.float16, torch.bfloat16, torch.float32]]\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nFP16: delta=1e-5 is below ULP at 1.0 (~1e-3). Weight NEVER changes.\")\n",
    "print(\"BF16: delta=1e-5 is below ULP at 1.0 (~8e-3). Weight NEVER changes.\")\n",
    "print(\"FP32: delta=1e-5 is above ULP at 1.0 (~1e-7). Weight changes every step.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Stochastic rounding micro-demo (why low-precision rounding mode matters)\n",
    "\n",
    "Section 2 introduced the idea from Gupta et al.: deterministic rounding can bias tiny updates toward zero, while stochastic rounding is unbiased in expectation.\n",
    "\n",
    "This demo intentionally uses an update (`1e-4`) below FP16's ULP at `1.0` (~`1e-3`) to show the behavior directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stochastic rounding demo: accumulate tiny updates in FP16\n",
    "\n",
    "def stochastic_round_fp16(x_fp32):\n",
    "    # Round FP32 value to FP16 with stochastic rounding.\n",
    "    x_lo = torch.tensor(float(x_fp32), dtype=torch.float16).float()\n",
    "    x_hi = torch.nextafter(\n",
    "        torch.tensor(float(x_fp32), dtype=torch.float16),\n",
    "        torch.tensor(float(\"inf\"), dtype=torch.float16),\n",
    "    ).float()\n",
    "    if float(x_hi) == float(x_lo):\n",
    "        return torch.tensor(float(x_lo), dtype=torch.float16)\n",
    "    p_up = (x_fp32 - x_lo) / (x_hi - x_lo + 1e-30)\n",
    "    p_up = float(p_up.clamp(0, 1))\n",
    "    if random.random() < p_up:\n",
    "        return torch.tensor(float(x_hi), dtype=torch.float16)\n",
    "    return torch.tensor(float(x_lo), dtype=torch.float16)\n",
    "\n",
    "delta = 1e-4\n",
    "N = 5000\n",
    "expected = 1.0 + delta * N\n",
    "\n",
    "# Deterministic FP16 rounding\n",
    "w_det = torch.tensor(1.0, dtype=torch.float16)\n",
    "for _ in range(N):\n",
    "    w_det = (w_det.float() + delta).half()\n",
    "\n",
    "# Stochastic FP16 rounding\n",
    "set_seed(42)\n",
    "w_stoch = torch.tensor(1.0, dtype=torch.float16)\n",
    "for _ in range(N):\n",
    "    w_stoch = stochastic_round_fp16(w_stoch.float() + delta)\n",
    "\n",
    "# FP32 reference\n",
    "w_fp32 = torch.tensor(1.0, dtype=torch.float32)\n",
    "for _ in range(N):\n",
    "    w_fp32 = w_fp32 + delta\n",
    "\n",
    "print(f\"Accumulating {N} updates of delta={delta} from 1.0\")\n",
    "print(f\"Expected: {expected:.6f}\")\n",
    "print(f\"  FP16 deterministic: {float(w_det):.6f}  (error: {abs(float(w_det) - expected):.4e})\")\n",
    "print(f\"  FP16 stochastic:    {float(w_stoch):.6f}  (error: {abs(float(w_stoch) - expected):.4e})\")\n",
    "print(f\"  FP32 deterministic: {float(w_fp32):.6f}  (error: {abs(float(w_fp32) - expected):.4e})\")\n",
    "print()\n",
    "print(\"Deterministic FP16 rounds each tiny update away. Stochastic FP16 preserves\")\n",
    "print(\"the update in expectation. AMP avoids this in practice by updating in FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 The main event: train a tiny causal LM under different precision regimes\n",
    "\n",
    "We'll train TinyGPT on a character-level next-token prediction task using an in-notebook corpus.\n",
    "\n",
    "**Why character-level?** No external downloads, stable, deterministic, and still exercises the transformer mechanics that matter for autocast (attention, layernorm, softmax, embeddings).\n",
    "\n",
    "**We will compare (device-dependent):**\n",
    "- **Always:** FP32 baseline\n",
    "- **CUDA:** (optional) **strict FP32 baseline** vs **TF32-allowed FP32** + naive FP16/BF16 (cast-everything baselines) + AMP FP16 (with GradScaler) + AMP BF16 (if supported)\n",
    "- **CPU:** BF16 autocast (numerics-focused; speedups vary by CPU/kernel support)\n",
    "- **MPS:** FP16 autocast (+ BF16 autocast if your MPS backend supports it)\n",
    "\n",
    "Before the full training suite, we'll also do:\n",
    "- A **single-batch numerical drift** comparison: FP32 vs autocast (loss + gradient deltas).\n",
    "- A qualitative **text-generation sanity check** from each successfully trained regime.\n",
    "\n",
    "**We will log:**\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Gradient norm + exact-zero gradient fraction (underflow proxy)\n",
    "- Step time + throughput (tokens/s)\n",
    "- CUDA peak memory (if CUDA)\n",
    "- GradScaler scale (for FP16 AMP)\n",
    "\n",
    "Implementation detail: we keep the tokenized corpus on the selected device and sample batches with vectorized indexing, so the step-time graphs are not dominated by Python loops or host→device copies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tiny corpus + character-level tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Use the repo's reference material as training text (no external downloads).\n",
    "# Falls back to a small built-in corpus if the files aren't present.\n",
    "FALLBACK_LINES = [\n",
    "    \"Autocast is not a global cast: it is an operator policy that routes each operation to the right precision.\",\n",
    "    \"Matmuls/linears are the primary AMP targets because they map cleanly to Tensor Cores with FP32 accumulation.\",\n",
    "    \"Softmax, layer normalization, exp/log, and many reductions are numerically sensitive and often run in FP32.\",\n",
    "    \"FP16 has narrow exponent range → gradient underflow; BF16 keeps FP32 exponent range → underflow is rare.\",\n",
    "    \"GradScaler implements dynamic loss scaling: scale loss → backward → unscale grads → step → update scale.\",\n",
    "]\n",
    "\n",
    "def _read_text(path: str) -> str | None:\n",
    "    try:\n",
    "        return Path(path).read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "sources_used = []\n",
    "parts = []\n",
    "for p in [\"sources/source4.md\", \"sources/source5.md\"]:\n",
    "    t = _read_text(p)\n",
    "    if t:\n",
    "        parts.append(t)\n",
    "        sources_used.append(p)\n",
    "\n",
    "raw = \"\\n\".join(FALLBACK_LINES).strip()\n",
    "if parts:\n",
    "    raw = raw + \"\\n\\n\" + \"\\n\\n\".join(parts)\n",
    "\n",
    "# Minimal cleanup: remove code fences (if any) and normalize whitespace.\n",
    "raw = raw.replace(\"\\r\\n\", \"\\n\")\n",
    "raw = re.sub(r\"```.*?```\", \"\", raw, flags=re.S)\n",
    "raw = re.sub(r\"[ \\t]+\", \" \", raw)\n",
    "raw = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw)\n",
    "corpus = raw.strip()\n",
    "\n",
    "# Keep runtime predictable: cap corpus size and repeat if too small.\n",
    "TARGET_CHARS = 120_000\n",
    "if len(corpus) < TARGET_CHARS:\n",
    "    repeats = (TARGET_CHARS // max(len(corpus), 1)) + 1\n",
    "    corpus = ((corpus + \"\\n\") * repeats)[:TARGET_CHARS]\n",
    "else:\n",
    "    corpus = corpus[:TARGET_CHARS]\n",
    "\n",
    "chars = sorted(set(corpus))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode_tokens(ids):\n",
    "    return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "data = torch.tensor(encode(corpus), dtype=torch.long).to(device)\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_data):,}, Val tokens: {len(val_data):,}\")\n",
    "print(f\"Corpus sources used: {sources_used if sources_used else ['fallback_only']}\")\n",
    "print(f\"Sample: {decode_tokens(train_data[:80].tolist())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Batch sampling + evaluation\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    # Vectorized slicing (no Python loops) and stays on `device`.\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    max_start = src.size(0) - block_size - 1\n",
    "    ix = torch.randint(0, max_start, (batch_size,), device=src.device)\n",
    "    offsets = torch.arange(block_size, device=src.device).unsqueeze(0)\n",
    "    x = src[ix.unsqueeze(1) + offsets]\n",
    "    y = src[ix.unsqueeze(1) + offsets + 1]\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size, iters=20, use_autocast=False, amp_dtype=None):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            with amp_autocast(device, amp_dtype, enabled=use_autocast):\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "            losses.append(float(loss))\n",
    "        out[split] = float(np.mean(losses))\n",
    "    model.train()\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.0 A microscope view: what autocast changes numerically (one fixed batch)\n",
    "\n",
    "Before we look at full training curves, let's make autocast *concrete*:\n",
    "\n",
    "- Take the **same** model initialization and the **same** batch.\n",
    "- Compute loss + gradients in strict **FP32**.\n",
    "- Compute loss + gradients under **autocast** (FP16/BF16) with **FP32 parameters** (the usual AMP setup).\n",
    "- Quantify the deltas.\n",
    "\n",
    "This is the most direct way to understand \"mixed precision\": it introduces **small, structured numerical error** in specific parts of the forward/backward graph. The goal of AMP is not \"no error\" — it's \"bounded error that doesn't break training, in exchange for speed/memory gains\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Single-batch numerical drift: FP32 vs autocast\n",
    "import copy\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "# A tiny fixed batch to make this fast and deterministic.\n",
    "DRIFT_BS = 4\n",
    "DRIFT_BLOCK = 64\n",
    "x_drift, y_drift = get_batch(\"train\", DRIFT_BS, DRIFT_BLOCK)\n",
    "\n",
    "base = TinyGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=DRIFT_BLOCK,\n",
    "    n_layer=2,\n",
    "    n_embd=128,\n",
    "    n_heads=4,\n",
    "    dropout=0.0,\n",
    ").to(device).float()\n",
    "\n",
    "def loss_and_grads(model, x, y, use_autocast=False, amp_dtype=None):\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    with amp_autocast(device, amp_dtype, enabled=use_autocast):\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "    loss.backward()\n",
    "    grads = {n: p.grad.detach().float().cpu().clone() for n, p in model.named_parameters() if p.grad is not None}\n",
    "    return float(loss.detach().cpu()), grads\n",
    "\n",
    "loss_fp32, grads_fp32 = loss_and_grads(base, x_drift, y_drift, use_autocast=False, amp_dtype=None)\n",
    "\n",
    "def compare_grads(grads_ref, grads_test):\n",
    "    names = sorted(grads_ref.keys())\n",
    "    v0 = torch.cat([grads_ref[n].flatten() for n in names])\n",
    "    v1 = torch.cat([grads_test[n].flatten() for n in names])\n",
    "    rel_l2 = float((v1 - v0).norm() / (v0.norm() + 1e-12))\n",
    "    cos = float(F.cosine_similarity(v0, v1, dim=0))\n",
    "\n",
    "    per_param = []\n",
    "    for n in names:\n",
    "        g0 = grads_ref[n]\n",
    "        g1 = grads_test[n]\n",
    "        denom = float(g0.norm()) + 1e-12\n",
    "        per_param.append({\n",
    "            \"param\": n,\n",
    "            \"ref_norm\": float(g0.norm()),\n",
    "            \"rel_l2\": float((g1 - g0).norm() / denom),\n",
    "            \"max_abs_diff\": float((g1 - g0).abs().max()),\n",
    "        })\n",
    "    df = pd.DataFrame(per_param)\n",
    "    summary = {\n",
    "        \"grad_rel_l2\": rel_l2,\n",
    "        \"grad_cosine\": cos,\n",
    "        \"param_rel_l2_median\": float(df[\"rel_l2\"].median()),\n",
    "        \"param_rel_l2_p90\": float(df[\"rel_l2\"].quantile(0.90)),\n",
    "        \"param_rel_l2_max\": float(df[\"rel_l2\"].max()),\n",
    "    }\n",
    "    return df, summary\n",
    "\n",
    "candidates = []\n",
    "for dt in [torch.float16, torch.bfloat16]:\n",
    "    if dt is torch.float16 and device.type == \"cpu\":\n",
    "        continue\n",
    "    if device.type == \"cuda\" and dt is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n",
    "        continue\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    candidates.append(dt)\n",
    "\n",
    "rows = []\n",
    "per_param_dfs = {}\n",
    "\n",
    "print(f\"Reference FP32 loss: {loss_fp32:.6f}\")\n",
    "\n",
    "for amp_dt in candidates:\n",
    "    name = str(amp_dt).replace(\"torch.\", \"\")\n",
    "    try:\n",
    "        m = copy.deepcopy(base)\n",
    "        loss_amp, grads_amp = loss_and_grads(m, x_drift, y_drift, use_autocast=True, amp_dtype=amp_dt)\n",
    "        df_param, s = compare_grads(grads_fp32, grads_amp)\n",
    "        per_param_dfs[name] = df_param\n",
    "        rows.append({\n",
    "            \"amp_dtype\": name,\n",
    "            \"status\": \"ok\",\n",
    "            \"loss_amp\": loss_amp,\n",
    "            \"loss_abs_diff\": abs(loss_amp - loss_fp32),\n",
    "            \"loss_rel_diff\": abs(loss_amp - loss_fp32) / max(abs(loss_fp32), 1e-12),\n",
    "            **s,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"amp_dtype\": name,\n",
    "            \"status\": f\"failed: {type(e).__name__}\",\n",
    "            \"loss_amp\": float(\"nan\"),\n",
    "            \"loss_abs_diff\": float(\"nan\"),\n",
    "            \"loss_rel_diff\": float(\"nan\"),\n",
    "            \"grad_rel_l2\": float(\"nan\"),\n",
    "            \"grad_cosine\": float(\"nan\"),\n",
    "            \"param_rel_l2_median\": float(\"nan\"),\n",
    "            \"param_rel_l2_p90\": float(\"nan\"),\n",
    "            \"param_rel_l2_max\": float(\"nan\"),\n",
    "        })\n",
    "        print(f\"[warn] autocast drift compare failed for {amp_dt}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n",
    "\n",
    "for amp_name, df_param in per_param_dfs.items():\n",
    "    print(f\"\\nTop gradient-relative-error parameters for autocast({amp_name}) vs FP32:\")\n",
    "    display(df_param.sort_values('rel_l2', ascending=False).head(10))\n",
    "\n",
    "if per_param_dfs:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for amp_name, df_param in per_param_dfs.items():\n",
    "        plt.hist(np.log10(df_param[\"rel_l2\"].to_numpy() + 1e-20), bins=40, alpha=0.5, label=amp_name)\n",
    "    plt.title(\"Per-parameter gradient relative L2 error (autocast vs FP32)\")\n",
    "    plt.xlabel(\"log10(rel_l2 + 1e-20)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training infrastructure\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    name: str\n",
    "    steps: int = 200\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 64\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    use_autocast: bool = False\n",
    "    amp_dtype: torch.dtype = None\n",
    "    use_grad_scaler: bool = False\n",
    "    param_dtype: torch.dtype = torch.float32\n",
    "    # CUDA only: control whether float32 matmuls are allowed to use TF32 internally.\n",
    "    # None = leave current global setting unchanged.\n",
    "    allow_tf32: bool | None = None\n",
    "    eval_interval: int = 50\n",
    "    eval_iters: int = 10\n",
    "\n",
    "def global_grad_norm(model):\n",
    "    total_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_sq += float(p.grad.detach().float().norm())**2\n",
    "    return math.sqrt(total_sq)\n",
    "\n",
    "def global_zero_grad_frac(model):\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g = p.grad.detach()\n",
    "        zeros += int((g == 0).sum())\n",
    "        total += g.numel()\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "def train_one(cfg):\n",
    "    set_seed(42)\n",
    "    tf32_matmul_orig = None\n",
    "    tf32_cudnn_orig = None\n",
    "    if device.type == \"cuda\" and cfg.allow_tf32 is not None:\n",
    "        tf32_matmul_orig = torch.backends.cuda.matmul.allow_tf32\n",
    "        tf32_cudnn_orig = torch.backends.cudnn.allow_tf32\n",
    "        torch.backends.cuda.matmul.allow_tf32 = bool(cfg.allow_tf32)\n",
    "        torch.backends.cudnn.allow_tf32 = bool(cfg.allow_tf32)\n",
    "\n",
    "    try:\n",
    "        model = TinyGPT(\n",
    "            vocab_size=vocab_size, block_size=cfg.block_size,\n",
    "            n_layer=2, n_embd=128, n_heads=4, dropout=0.0,\n",
    "        ).to(device).to(cfg.param_dtype)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        scaler = GradScaler(enabled=True) if (cfg.use_grad_scaler and device.type == \"cuda\") else None\n",
    "\n",
    "        logs = {\n",
    "            \"step\": [], \"train_loss\": [], \"grad_norm\": [], \"zero_grad_frac\": [],\n",
    "            \"step_time_ms\": [], \"tokens_per_s\": [], \"scale\": [],\n",
    "            \"cuda_mem_mb\": [], \"val_step\": [], \"val_loss\": [],\n",
    "        }\n",
    "        status = \"ok\"\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        tokens_per_step = cfg.batch_size * cfg.block_size\n",
    "\n",
    "        pbar = tqdm(range(cfg.steps), desc=cfg.name, leave=False, unit=\"step\")\n",
    "        for step in pbar:\n",
    "            # GPU kernels are async; synchronize so step_time_ms reflects actual compute.\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()\n",
    "            x, y = get_batch(\"train\", cfg.batch_size, cfg.block_size)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with amp_autocast(device, cfg.amp_dtype, enabled=cfg.use_autocast):\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                status = \"non_finite_loss\"\n",
    "                break\n",
    "\n",
    "            if scaler is None:\n",
    "                loss.backward()\n",
    "                grad_norm = global_grad_norm(model)\n",
    "                zero_frac = global_zero_grad_frac(model)\n",
    "                optimizer.step()\n",
    "                scale_val = float(\"nan\")\n",
    "            else:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = global_grad_norm(model)\n",
    "                zero_frac = global_zero_grad_frac(model)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scale_val = float(scaler.get_scale())\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            dt = max(time.perf_counter() - t0, 1e-12)\n",
    "            logs[\"step\"].append(step)\n",
    "            logs[\"train_loss\"].append(float(loss))\n",
    "            logs[\"grad_norm\"].append(float(grad_norm))\n",
    "            logs[\"zero_grad_frac\"].append(float(zero_frac))\n",
    "            logs[\"step_time_ms\"].append(dt * 1000)\n",
    "            logs[\"tokens_per_s\"].append(tokens_per_step / dt)\n",
    "            logs[\"scale\"].append(scale_val)\n",
    "            logs[\"cuda_mem_mb\"].append(torch.cuda.max_memory_allocated() / 1024**2 if device.type == \"cuda\" else float(\"nan\"))\n",
    "\n",
    "            # Update progress bar with live metrics\n",
    "            pbar.set_postfix(loss=f\"{float(loss):.3f}\", tok_s=f\"{tokens_per_step / dt:.0f}\")\n",
    "\n",
    "            if cfg.eval_interval and (step % cfg.eval_interval == 0 or step == cfg.steps - 1):\n",
    "                try:\n",
    "                    ev = estimate_loss(\n",
    "                        model,\n",
    "                        cfg.block_size,\n",
    "                        cfg.batch_size,\n",
    "                        cfg.eval_iters,\n",
    "                        use_autocast=cfg.use_autocast,\n",
    "                        amp_dtype=cfg.amp_dtype,\n",
    "                    )\n",
    "                    logs[\"val_step\"].append(step)\n",
    "                    logs[\"val_loss\"].append(ev[\"val\"])\n",
    "                except Exception:\n",
    "                    logs[\"val_step\"].append(step)\n",
    "                    logs[\"val_loss\"].append(float(\"nan\"))\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # Save final weights for downstream analysis (e.g., text generation).\n",
    "        # Store as FP32 on CPU for portability and to avoid holding multiple GPU copies.\n",
    "        state_dict_fp32 = {k: v.detach().float().cpu() for k, v in model.state_dict().items()}\n",
    "        return {\"config\": cfg, \"status\": status, \"logs\": logs, \"state_dict\": state_dict_fp32}\n",
    "    finally:\n",
    "        if device.type == \"cuda\" and cfg.allow_tf32 is not None:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = tf32_matmul_orig\n",
    "            torch.backends.cudnn.allow_tf32 = tf32_cudnn_orig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define experiment suite\n",
    "\n",
    "FAST_DEV_RUN = (device.type != \"cuda\")\n",
    "BASE_STEPS = 60 if FAST_DEV_RUN else 300\n",
    "\n",
    "INCLUDE_STRICT_FP32_BASELINE = (device.type == \"cuda\")\n",
    "\n",
    "suite = []\n",
    "if device.type == \"cuda\":\n",
    "    # Make the \"FP32 baseline\" explicit: on Ampere+ GPUs, this typically means TF32 is allowed for matmuls.\n",
    "    suite.append(TrainConfig(name=\"fp32\", steps=BASE_STEPS, param_dtype=torch.float32, allow_tf32=True))\n",
    "    if INCLUDE_STRICT_FP32_BASELINE:\n",
    "        suite.append(TrainConfig(name=\"fp32_strict\", steps=BASE_STEPS, param_dtype=torch.float32, allow_tf32=False))\n",
    "else:\n",
    "    suite.append(TrainConfig(name=\"fp32\", steps=BASE_STEPS, param_dtype=torch.float32))\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"fp16_naive\", steps=BASE_STEPS,\n",
    "        param_dtype=torch.float16,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"bf16_naive\", steps=BASE_STEPS,\n",
    "            param_dtype=torch.bfloat16,\n",
    "        ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16_no_scaler\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=False, param_dtype=torch.float32,\n",
    "    ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=True, param_dtype=torch.float32,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_bf16\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "            param_dtype=torch.float32,\n",
    "        ))\n",
    "elif device.type == \"cpu\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_bf16_cpu\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "        param_dtype=torch.float32,\n",
    "    ))\n",
    "elif device.type == \"mps\":\n",
    "    if supports_dtype_on_device(torch.float16, device):\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_fp16_mps\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.float16,\n",
    "            use_grad_scaler=False, param_dtype=torch.float32,\n",
    "        ))\n",
    "    if supports_dtype_on_device(torch.bfloat16, device):\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_bf16_mps\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "            use_grad_scaler=False, param_dtype=torch.float32,\n",
    "        ))\n",
    "\n",
    "print(\"Planned experiments:\")\n",
    "for cfg in suite:\n",
    "    tf32 = f\", tf32={cfg.allow_tf32}\" if device.type == \"cuda\" else \"\"\n",
    "    print(f\"  {cfg.name}: params={cfg.param_dtype}, autocast={cfg.use_autocast} {cfg.amp_dtype}, scaler={cfg.use_grad_scaler}{tf32}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run all experiments\n",
    "\n",
    "def _empty_logs():\n",
    "    return {\n",
    "        \"step\": [], \"train_loss\": [], \"grad_norm\": [], \"zero_grad_frac\": [],\n",
    "        \"step_time_ms\": [], \"tokens_per_s\": [], \"scale\": [],\n",
    "        \"cuda_mem_mb\": [], \"val_step\": [], \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "def _safe_ppl(loss):\n",
    "    try:\n",
    "        return float(math.exp(float(loss)))\n",
    "    except OverflowError:\n",
    "        return float(\"inf\")\n",
    "\n",
    "results = []\n",
    "suite_pbar = tqdm(suite, desc=\"Experiment suite\", unit=\"exp\")\n",
    "for cfg in suite_pbar:\n",
    "    suite_pbar.set_description(f\"Running: {cfg.name}\")\n",
    "    try:\n",
    "        res = train_one(cfg)\n",
    "        print(f\"  {cfg.name}: status={res['status']}, steps={len(res['logs']['step'])}\", end=\"\")\n",
    "        if res['logs']['train_loss']:\n",
    "            print(f\", final_loss={res['logs']['train_loss'][-1]:.4f}\")\n",
    "        else:\n",
    "            print()\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        print(f\"  {cfg.name}: exception: {type(e).__name__}: {e}\")\n",
    "        results.append({\n",
    "            \"config\": cfg,\n",
    "            \"status\": f\"exception: {type(e).__name__}\",\n",
    "            \"logs\": _empty_logs(),\n",
    "            \"state_dict\": None,\n",
    "        })\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    cfg, logs = r[\"config\"], r[\"logs\"]\n",
    "    n = len(logs[\"step\"])\n",
    "    final_train_loss = logs[\"train_loss\"][-1] if n else None\n",
    "    final_val_loss = logs[\"val_loss\"][-1] if logs[\"val_loss\"] else None\n",
    "    summary_rows.append({\n",
    "        \"name\": cfg.name,\n",
    "        \"status\": r[\"status\"],\n",
    "        \"allow_tf32\": (cfg.allow_tf32 if device.type == \"cuda\" else \"n/a\"),\n",
    "        \"steps\": n,\n",
    "        \"final_train_loss\": f\"{final_train_loss:.4f}\" if final_train_loss is not None else \"n/a\",\n",
    "        \"final_train_ppl\": f\"{_safe_ppl(final_train_loss):.2f}\" if final_train_loss is not None else \"n/a\",\n",
    "        \"final_val_loss\": f\"{final_val_loss:.4f}\" if final_val_loss is not None else \"n/a\",\n",
    "        \"final_val_ppl\": f\"{_safe_ppl(final_val_loss):.2f}\" if final_val_loss is not None else \"n/a\",\n",
    "        \"mean_step_ms\": f\"{np.mean(logs['step_time_ms']):.1f}\" if n else \"n/a\",\n",
    "        \"mean_tok/s\": f\"{np.mean(logs['tokens_per_s']):.0f}\" if n else \"n/a\",\n",
    "        \"peak_cuda_MB\": f\"{np.nanmax(logs['cuda_mem_mb']):.1f}\" if device.type == \"cuda\" and n else \"n/a\",\n",
    "    })\n",
    "\n",
    "print(\"\\n\\n=== Summary ===\")\n",
    "display(pd.DataFrame(summary_rows))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Training + Validation loss curves (annotated)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "color_map = {\n",
    "    \"fp32\": \"C0\", \"fp32_strict\": \"C2\",\n",
    "    \"fp16_naive\": \"C3\", \"bf16_naive\": \"C5\",\n",
    "    \"amp_fp16_no_scaler\": \"C6\", \"amp_fp16\": \"C4\", \"amp_bf16\": \"C1\",\n",
    "    \"amp_bf16_cpu\": \"C1\",\n",
    "    \"amp_fp16_mps\": \"C4\", \"amp_bf16_mps\": \"C1\",\n",
    "}\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    suffix = f\" ({r['status']})\" if r[\"status\"] != \"ok\" else \"\"\n",
    "    color = color_map.get(name, \"C7\")\n",
    "    ls = \"--\" if (\"naive\" in name or \"strict\" in name) else \"-\"\n",
    "    if logs[\"train_loss\"]:\n",
    "        axes[0].plot(logs[\"step\"], logs[\"train_loss\"], label=f\"{name}{suffix}\",\n",
    "                     alpha=0.85, color=color, linestyle=ls, linewidth=1.5 if \"naive\" not in name else 1.0)\n",
    "    if logs[\"val_loss\"]:\n",
    "        axes[1].plot(logs[\"val_step\"], logs[\"val_loss\"], marker=\"o\", ms=5, label=f\"{name}{suffix}\",\n",
    "                     alpha=0.85, color=color, linestyle=ls)\n",
    "\n",
    "axes[0].set_title(\"Training loss vs step\", fontsize=11)\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"cross-entropy loss\")\n",
    "axes[0].legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "axes[1].set_title(\"Validation loss (periodic eval)\", fontsize=11)\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"cross-entropy loss\")\n",
    "axes[1].legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "fig.suptitle(\"TinyGPT Training: How precision regime affects convergence\", fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"What to look for:\")\n",
    "print(\"  - FP32 (blue): smooth reference curve\")\n",
    "if any(r[\"config\"].name == \"fp32_strict\" for r in results):\n",
    "    print(\"  - FP32 strict (dashed green): usually overlaps FP32 — TF32 differences are typically small\")\n",
    "print(\"  - Naive FP16 (dashed red): may stagnate, diverge, or NaN\")\n",
    "print(\"  - Naive BF16 (dashed olive): often converges — BF16 range prevents gradient death\")\n",
    "print(\"  - AMP variants (solid): should track FP32 closely, showing that AMP preserves quality\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Step time + throughput\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    axes[0].plot(logs[\"step\"], logs[\"step_time_ms\"], label=name, alpha=0.7)\n",
    "    axes[1].plot(logs[\"step\"], logs[\"tokens_per_s\"], label=name, alpha=0.7)\n",
    "\n",
    "axes[0].set_title(\"Step time (ms)\")\n",
    "axes[0].set_xlabel(\"step\"); axes[0].set_ylabel(\"ms\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"Throughput (tokens/s)\")\n",
    "axes[1].set_xlabel(\"step\"); axes[1].set_ylabel(\"tokens/s\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Gradient norms across precision regimes\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    color = color_map.get(name, \"C7\")\n",
    "    ls = \"--\" if (\"naive\" in name or \"strict\" in name) else \"-\"\n",
    "    plt.plot(logs[\"step\"], logs[\"grad_norm\"], label=name, alpha=0.75, color=color, linestyle=ls)\n",
    "\n",
    "# Add FP16 min normal threshold line\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "plt.axhline(float(fi16.tiny), color=\"red\", ls=\":\", alpha=0.4, label=f\"FP16 min normal ({fi16.tiny:.1e})\")\n",
    "\n",
    "plt.title(\"Gradient L2 norm vs step (after unscaling)\", fontsize=11)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"||grad||_2\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"If gradients fall below the FP16 min normal line, they underflow to zero in FP16.\")\n",
    "print(\"BF16 gradients essentially never underflow (BF16 min normal ≈ 1.2e-38).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Exact-zero gradient fraction (a proxy for underflow / dead signal)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    if \"zero_grad_frac\" not in logs:\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], logs[\"zero_grad_frac\"], label=name, alpha=0.75)\n",
    "\n",
    "plt.title(\"Fraction of gradients that are exactly 0\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"zero_grad_frac\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: GradScaler dynamic scale (FP16 AMP only)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plotted = False\n",
    "for r in results:\n",
    "    if not r[\"config\"].use_grad_scaler:\n",
    "        continue\n",
    "    logs = r[\"logs\"]\n",
    "    scale = np.array(logs[\"scale\"], dtype=np.float64)\n",
    "    if len(scale) == 0 or np.all(np.isnan(scale)):\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], scale, label=r[\"config\"].name)\n",
    "    plotted = True\n",
    "\n",
    "if plotted:\n",
    "    plt.title(\"GradScaler dynamic scale factor\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"scale\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No GradScaler data to plot (did amp_fp16 run?)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: CUDA peak memory\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    for r in results:\n",
    "        logs = r[\"logs\"]\n",
    "        if not logs[\"step\"]:\n",
    "            continue\n",
    "        ax.plot(logs[\"step\"], logs[\"cuda_mem_mb\"], label=r[\"config\"].name, alpha=0.7)\n",
    "    ax.set_title(\"Peak CUDA memory allocated (MB)\")\n",
    "    ax.set_xlabel(\"step\"); ax.set_ylabel(\"MB\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"CUDA not available; skipping memory plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary bar charts: compare all experiments at a glance\n",
    "\n",
    "completed = [r for r in results if r[\"status\"] == \"ok\" and r[\"logs\"][\"train_loss\"]]\n",
    "\n",
    "if len(completed) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    names = [r[\"config\"].name for r in completed]\n",
    "    x_pos = np.arange(len(names))\n",
    "\n",
    "    # 1. Final training loss\n",
    "    final_losses = [r[\"logs\"][\"train_loss\"][-1] for r in completed]\n",
    "    colors = [color_map.get(r[\"config\"].name, \"C7\") for r in completed]\n",
    "\n",
    "    axes[0].bar(x_pos, final_losses, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[0].set_ylabel(\"Final train loss\")\n",
    "    axes[0].set_title(\"Final Training Loss (lower is better)\")\n",
    "\n",
    "    # 2. Mean step time\n",
    "    mean_times = [np.mean(r[\"logs\"][\"step_time_ms\"]) for r in completed]\n",
    "    axes[1].bar(x_pos, mean_times, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[1].set_ylabel(\"Mean step time (ms)\")\n",
    "    axes[1].set_title(\"Training Speed (lower is better)\")\n",
    "\n",
    "    # 3. Peak CUDA memory\n",
    "    if device.type == \"cuda\":\n",
    "        peak_mem = [np.nanmax(r[\"logs\"][\"cuda_mem_mb\"]) for r in completed]\n",
    "        axes[2].bar(x_pos, peak_mem, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "        axes[2].set_xticks(x_pos)\n",
    "        axes[2].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "        axes[2].set_ylabel(\"Peak memory (MB)\")\n",
    "        axes[2].set_title(\"Peak GPU Memory (lower is better)\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"CUDA memory\\nnot available\", transform=axes[2].transAxes,\n",
    "                     ha=\"center\", va=\"center\", fontsize=12, color=\"gray\")\n",
    "        axes[2].set_title(\"Peak GPU Memory\")\n",
    "\n",
    "    fig.suptitle(\"Experiment Summary: Loss, Speed, and Memory across Precision Regimes\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Need at least 2 completed experiments for summary charts.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Sanity check: generate text from each trained model\n",
    "\n",
    "Loss curves are the real metric, but they're abstract. Since we're training a tiny **causal language model**, we can do a qualitative sanity check: generate a short continuation from a fixed prompt for each successfully trained regime.\n",
    "\n",
    "Notes:\n",
    "- This is **not** a rigorous evaluation — it's a learning aid.\n",
    "- We run generation in **FP32** for comparability (we're comparing the *trained weights*, not inference autocast)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text generation samples (one prompt per precision regime)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)[:, -1, :] / max(float(temperature), 1e-8)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, int(top_k))\n",
    "            logits = logits.clone()\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "def safe_encode(s: str):\n",
    "    return [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "PROMPT = \"Autocast is \"\n",
    "NEW_TOKENS = 240\n",
    "TEMPERATURE = 0.9\n",
    "TOP_K = 20\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "printed = 0\n",
    "for r in results:\n",
    "    if r[\"status\"] != \"ok\":\n",
    "        continue\n",
    "    state = r.get(\"state_dict\", None)\n",
    "    if state is None:\n",
    "        continue\n",
    "    cfg = r[\"config\"]\n",
    "    m = TinyGPT(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=cfg.block_size,\n",
    "        n_layer=2,\n",
    "        n_embd=128,\n",
    "        n_heads=4,\n",
    "        dropout=0.0,\n",
    "    ).to(device).float()\n",
    "    m.load_state_dict(state)\n",
    "\n",
    "    idx0 = torch.tensor([safe_encode(PROMPT)], dtype=torch.long, device=device)\n",
    "    out = generate(m, idx0, NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "    text = decode_tokens(out[0].tolist())\n",
    "    print(f\"\\n=== {cfg.name} ===\\n{text}\")\n",
    "    printed += 1\n",
    "\n",
    "if printed == 0:\n",
    "    print(\"No successful runs to sample from.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 OPT-125M training loss curves: real-world AMP on a Hugging Face model\n",
    "\n",
    "TinyGPT above demonstrates AMP mechanics at small scale. Now we repeat the experiment with **OPT-125M** (125M parameters) — a real pretrained transformer — to show that the same patterns hold at realistic scale.\n",
    "\n",
    "We fine-tune for **60 steps** under 6 precision configurations:\n",
    "\n",
    "| # | Name | Params | Autocast | Scaler | Expected |\n",
    "|---|------|--------|----------|--------|----------|\n",
    "| 1 | `no_ac_fp32` | FP32 | OFF | No | Stable baseline |\n",
    "| 2 | `no_ac_fp16` | FP16 | OFF | No | Often unstable / may diverge |\n",
    "| 3 | `no_ac_bf16` | BF16 | OFF | No | May work (wider range) |\n",
    "| 4 | `ac_fp32` | FP32 | ON | Yes | Canonical AMP baseline |\n",
    "| 5 | `ac_fp16` | FP16 | ON | Yes | Non-standard; may improve vs naive FP16 |\n",
    "| 6 | `ac_bf16` | BF16 | ON | No | Usually stable where BF16 is supported |\n",
    "\n",
    "These 6 runs intentionally vary **both** parameter dtype and autocast policy to show practical regimes you will encounter in real code, not just a single-axis ablation.\n",
    "\n",
    "> **Guard:** This section requires `trace_hf_model` (OPT-125M) loaded earlier + CUDA. Skipped otherwise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OPT-125M training setup\n",
    "\n",
    "opt_train_results = []\n",
    "\n",
    "if trace_hf_model is None or device.type != \"cuda\":\n",
    "    print(\"Skipping OPT-125M training: requires trace_hf_model (OPT-125M) + CUDA.\")\n",
    "    print(\"To enable: set ALLOW_OPT_DOWNLOAD=True and run on a CUDA device.\")\n",
    "else:\n",
    "    import copy as _copy\n",
    "\n",
    "    # Tokenize a corpus with the OPT BPE tokenizer\n",
    "    opt_corpus_text = corpus[:20000]  # reuse the char-level corpus text\n",
    "    opt_tokens = trace_hf_tokenizer(opt_corpus_text, return_tensors=\"pt\",\n",
    "                                     truncation=True, max_length=4096)[\"input_ids\"].squeeze(0).to(device)\n",
    "    n_opt = int(0.9 * len(opt_tokens))\n",
    "    opt_train_tokens = opt_tokens[:n_opt]\n",
    "    opt_val_tokens = opt_tokens[n_opt:]\n",
    "\n",
    "    OPT_BLOCK = 128\n",
    "    OPT_BATCH = 4\n",
    "    OPT_STEPS = 60\n",
    "    OPT_LR = 5e-5\n",
    "\n",
    "    def get_opt_batch(split):\n",
    "        src = opt_train_tokens if split == \"train\" else opt_val_tokens\n",
    "        max_start = src.size(0) - OPT_BLOCK - 1\n",
    "        if max_start <= 0:\n",
    "            max_start = 1\n",
    "        ix = torch.randint(0, max_start, (OPT_BATCH,), device=device)\n",
    "        offsets = torch.arange(OPT_BLOCK, device=device).unsqueeze(0)\n",
    "        x = src[ix.unsqueeze(1) + offsets]\n",
    "        y = src[ix.unsqueeze(1) + offsets + 1]\n",
    "        return x, y\n",
    "\n",
    "    print(f\"OPT-125M tokens: train={len(opt_train_tokens):,}, val={len(opt_val_tokens):,}\")\n",
    "    print(f\"Training: {OPT_STEPS} steps, batch={OPT_BATCH}, block={OPT_BLOCK}, lr={OPT_LR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OPT-125M training loop\n",
    "\n",
    "if trace_hf_model is not None and device.type == \"cuda\":\n",
    "    import copy as _copy\n",
    "\n",
    "    opt_configs = [\n",
    "        {\"name\": \"no_ac_fp32\",  \"param_dtype\": torch.float32,  \"use_autocast\": False, \"ac_dtype\": None,           \"use_scaler\": False},\n",
    "        {\"name\": \"no_ac_fp16\",  \"param_dtype\": torch.float16,  \"use_autocast\": False, \"ac_dtype\": None,           \"use_scaler\": False},\n",
    "        {\"name\": \"no_ac_bf16\",  \"param_dtype\": torch.bfloat16, \"use_autocast\": False, \"ac_dtype\": None,           \"use_scaler\": False},\n",
    "        {\"name\": \"ac_fp32\",     \"param_dtype\": torch.float32,  \"use_autocast\": True,  \"ac_dtype\": torch.float16,  \"use_scaler\": True},\n",
    "        {\"name\": \"ac_fp16\",     \"param_dtype\": torch.float16,  \"use_autocast\": True,  \"ac_dtype\": torch.float16,  \"use_scaler\": True},\n",
    "        {\"name\": \"ac_bf16\",     \"param_dtype\": torch.bfloat16, \"use_autocast\": True,  \"ac_dtype\": torch.bfloat16, \"use_scaler\": False},\n",
    "    ]\n",
    "\n",
    "    # Check BF16 support\n",
    "    if not torch.cuda.is_bf16_supported():\n",
    "        opt_configs = [c for c in opt_configs if c[\"param_dtype\"] != torch.bfloat16]\n",
    "\n",
    "    def train_opt(cfg):\n",
    "        set_seed(42)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_t = _copy.deepcopy(trace_hf_model).to(cfg[\"param_dtype\"]).train()\n",
    "        opt_t = torch.optim.AdamW(model_t.parameters(), lr=OPT_LR, weight_decay=0.01)\n",
    "        scaler = GradScaler(enabled=True) if cfg[\"use_scaler\"] else None\n",
    "\n",
    "        losses, grad_norms, zero_fracs = [], [], []\n",
    "        status = \"ok\"\n",
    "\n",
    "        for step in range(OPT_STEPS):\n",
    "            xb, yb = get_opt_batch(\"train\")\n",
    "            opt_t.zero_grad(set_to_none=True)\n",
    "\n",
    "            ctx = amp_autocast(device, cfg[\"ac_dtype\"], enabled=cfg[\"use_autocast\"]) if cfg[\"use_autocast\"] else nullcontext()\n",
    "            with ctx:\n",
    "                out = model_t(xb, labels=yb)\n",
    "                loss = out.loss\n",
    "\n",
    "            if not torch.isfinite(loss):\n",
    "                status = f\"diverged@step{step}\"\n",
    "                losses.append(float(\"nan\"))\n",
    "                break\n",
    "\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt_t)\n",
    "                gn = torch.nn.utils.clip_grad_norm_(model_t.parameters(), 1.0)\n",
    "                scaler.step(opt_t)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                gn = torch.nn.utils.clip_grad_norm_(model_t.parameters(), 1.0)\n",
    "                opt_t.step()\n",
    "\n",
    "            losses.append(float(loss))\n",
    "            grad_norms.append(float(gn) if torch.isfinite(gn) else float(\"nan\"))\n",
    "\n",
    "            # Zero gradient fraction\n",
    "            zf = 0.0\n",
    "            total_params = 0\n",
    "            for p in model_t.parameters():\n",
    "                if p.grad is not None:\n",
    "                    g = p.grad.float().flatten()\n",
    "                    zf += float((g == 0).sum())\n",
    "                    total_params += g.numel()\n",
    "            zero_fracs.append(zf / max(total_params, 1))\n",
    "\n",
    "        del model_t, opt_t\n",
    "        if scaler is not None:\n",
    "            del scaler\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\"name\": cfg[\"name\"], \"status\": status, \"losses\": losses,\n",
    "                \"grad_norms\": grad_norms, \"zero_fracs\": zero_fracs}\n",
    "\n",
    "    for cfg in tqdm(opt_configs, desc=\"OPT-125M configs\"):\n",
    "        print(f\"\\n--- {cfg['name']} ---\")\n",
    "        result = train_opt(cfg)\n",
    "        opt_train_results.append(result)\n",
    "        print(f\"  Status: {result['status']}, \"\n",
    "              f\"Final loss: {result['losses'][-1]:.4f}\" if result['losses'] and not np.isnan(result['losses'][-1]) else f\"  Status: {result['status']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OPT-125M: plot loss curves + gradient norms + final loss comparison\n",
    "\n",
    "if len(opt_train_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Color/style mapping\n",
    "    color_map_opt = {\n",
    "        \"no_ac_fp32\": (\"#2ecc71\", \"-\"),   \"no_ac_fp16\": (\"#e74c3c\", \"--\"),\n",
    "        \"no_ac_bf16\": (\"#3498db\", \"--\"),   \"ac_fp32\": (\"#27ae60\", \"-\"),\n",
    "        \"ac_fp16\": (\"#c0392b\", \"-\"),       \"ac_bf16\": (\"#2980b9\", \"-\"),\n",
    "    }\n",
    "\n",
    "    # Top-left: loss curves\n",
    "    ax = axes[0, 0]\n",
    "    for r in opt_train_results:\n",
    "        color, ls = color_map_opt.get(r[\"name\"], (\"#95a5a6\", \"-\"))\n",
    "        label = r[\"name\"]\n",
    "        if r[\"status\"] != \"ok\":\n",
    "            label += f\" ({r['status']})\"\n",
    "        ax.plot(r[\"losses\"], color=color, ls=ls, lw=1.5, alpha=0.8, label=label)\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"OPT-125M: Training Loss Curves\", fontsize=10)\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    # Top-right: gradient norms\n",
    "    ax = axes[0, 1]\n",
    "    for r in opt_train_results:\n",
    "        if not r[\"grad_norms\"]:\n",
    "            continue\n",
    "        color, ls = color_map_opt.get(r[\"name\"], (\"#95a5a6\", \"-\"))\n",
    "        ax.plot(r[\"grad_norms\"], color=color, ls=ls, lw=1.2, alpha=0.7, label=r[\"name\"])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Gradient Norm\")\n",
    "    ax.set_title(\"Gradient Norm Over Training\", fontsize=10)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "    # Bottom-left: final loss bar chart\n",
    "    ax = axes[1, 0]\n",
    "    ok_results = [r for r in opt_train_results if r[\"losses\"] and not np.isnan(r[\"losses\"][-1])]\n",
    "    if ok_results:\n",
    "        names = [r[\"name\"] for r in ok_results]\n",
    "        final_losses = [r[\"losses\"][-1] for r in ok_results]\n",
    "        bar_colors = [color_map_opt.get(n, (\"#95a5a6\", \"-\"))[0] for n in names]\n",
    "        ax.bar(range(len(names)), final_losses, color=bar_colors, alpha=0.8, edgecolor=\"white\")\n",
    "        ax.set_xticks(range(len(names)))\n",
    "        ax.set_xticklabels(names, fontsize=7, rotation=30, ha=\"right\")\n",
    "        ax.set_ylabel(\"Final Loss\")\n",
    "        ax.set_title(\"Final Loss Comparison\", fontsize=10)\n",
    "\n",
    "        # Mark diverged (offset vertically to avoid overlap)\n",
    "        diverged = [r for r in opt_train_results if r[\"status\"] != \"ok\" and r[\"name\"] not in names]\n",
    "        for i, r in enumerate(diverged):\n",
    "            ax.annotate(f\"{r['name']} ({r['status']})\", xy=(0.98, 0.95 - 0.08 * i),\n",
    "                        xycoords=\"axes fraction\", ha=\"right\", va=\"top\",\n",
    "                        fontsize=7, color=\"red\")\n",
    "\n",
    "    # Bottom-right: zero-gradient fraction\n",
    "    ax = axes[1, 1]\n",
    "    for r in opt_train_results:\n",
    "        if not r[\"zero_fracs\"]:\n",
    "            continue\n",
    "        color, ls = color_map_opt.get(r[\"name\"], (\"#95a5a6\", \"-\"))\n",
    "        ax.plot(r[\"zero_fracs\"], color=color, ls=ls, lw=1.2, alpha=0.7, label=r[\"name\"])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Zero-Gradient Fraction\")\n",
    "    ax.set_title(\"Zero-Gradient Fraction (FP16 underflow indicator)\", fontsize=10)\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "    fig.suptitle(\"OPT-125M fine-tuning: 6 precision configurations × 60 steps\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No OPT-125M training results (skipped: need OPT-125M + CUDA).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (OPT-125M Training):**\n",
    "- **`no_ac_fp32`** (FP32 baseline): stable, converging loss — the reference.\n",
    "- **`no_ac_fp16`** (naive FP16): often unstable (divergence, NaNs, or noisy optimization), but exact behavior depends on GPU, LR, and clipping.\n",
    "- **`no_ac_bf16`** (naive BF16): usually more stable than FP16 (wider exponent range), sometimes with a small quality tradeoff.\n",
    "- **`ac_fp32`** (FP32 params + autocast FP16): canonical AMP setup; usually best stability/speed tradeoff.\n",
    "- **`ac_fp16`** (FP16 params + autocast): can improve over naive FP16, but remains less robust than FP32-master AMP because params/optimizer states stay low precision.\n",
    "- **`ac_bf16`** (BF16 params + autocast): typically stable without a scaler on BF16-capable hardware.\n",
    "\n",
    "Use this section as an empirical benchmark: the plotted outcomes are authoritative for your run, and the qualitative trend to look for is that **policy-driven mixed precision is safer than naive global casting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Memory breakdown comparison (bytes per parameter)\n",
    "\n",
    "For LLM-scale training, it helps to separate memory into:\n",
    "\n",
    "- **Fixed-size memory** (scales with number of parameters): parameters + gradients + optimizer state (+ optional master weights).\n",
    "- **Activation memory** (scales with batch size × sequence length × layers): intermediate tensors saved for backward.\n",
    "\n",
    "AMP/autocast primarily helps with **activation memory** (and speed). With AdamW, the fixed-size **bytes/parameter** are often **~16 bytes/param** in both FP32 and \"standard AMP\" because optimizer state dominates.\n",
    "\n",
    "### The key comparison: FP32 vs mixed precision (AdamW)\n",
    "\n",
    "| Component | FP32 Training | Mixed Precision (AMP) | Savings |\n",
    "|---|---|---|---|\n",
    "| **Parameters** | 4 B/param (FP32) | 4 B/param (FP32 master) | 0% |\n",
    "| **Gradients** | 4 B/param (FP32) | 4 B/param (FP32) | 0% |\n",
    "| **Optimizer state** ($m$) | 4 B/param (FP32) | 4 B/param (FP32) | 0% |\n",
    "| **Optimizer state** ($v$) | 4 B/param (FP32) | 4 B/param (FP32) | 0% |\n",
    "| **Total fixed** | **16 B/param** | **16 B/param** | **0%** |\n",
    "| **Activations** (varies) | FP32 | FP16/BF16 (~half) | **~50%** |\n",
    "\n",
    "**The surprise:** Standard PyTorch AMP does *not* reduce fixed per-parameter memory at all! The savings come entirely from **activations** (which are stored in FP16/BF16 for the backward pass) and from **faster compute** (Tensor Cores). For large-batch, long-sequence training, activation memory dominates, so AMP's 50% activation reduction is substantial.\n",
    "\n",
    "To reduce fixed per-parameter memory, you need additional techniques:\n",
    "- **8-bit optimizers** (§2.12): reduce optimizer state from 8 B/param to ~2 B/param\n",
    "- **FP16/BF16 master weights** (e.g., in some FSDP/ZeRO configs): reduce param storage\n",
    "- **Gradient compression**: reduce gradient storage during distributed training\n",
    "\n",
    "We'll compute the fixed-size accounting for a few common patterns and show what that looks like for our TinyGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.0 Where AMP memory savings *really* come from (measured on this model)\n",
    "\n",
    "Let's make this concrete with one controlled measurement on **the same TinyGPT model and same batch shape**:\n",
    "\n",
    "1. **FP32 training step** (no autocast)\n",
    "2. **AMP training step** (FP32 params + autocast)\n",
    "\n",
    "For each run we measure:\n",
    "- **Parameters** memory\n",
    "- **Gradients** memory\n",
    "- **Optimizer state** memory (AdamW moments, after first optimizer step)\n",
    "- **Activation peak** memory during forward\n",
    "\n",
    "This gives the exact intuition: AMP usually keeps fixed-size memory (params/grads/optimizer) close to FP32, while cutting activation memory substantially."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Measured memory composition on TinyGPT: FP32 vs AMP (same hardware, same model shape)\n",
    "\n",
    "def _tensor_nbytes(t: torch.Tensor) -> int:\n",
    "    return int(t.numel()) * int(t.element_size())\n",
    "\n",
    "def _optimizer_state_nbytes(opt: torch.optim.Optimizer) -> int:\n",
    "    total = 0\n",
    "    for state in opt.state.values():\n",
    "        for v in state.values():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                total += _tensor_nbytes(v)\n",
    "    return int(total)\n",
    "\n",
    "def measure_memory_breakdown(use_autocast: bool, ac_dtype: torch.dtype | None, batch_size: int = 32, block_size: int = 64):\n",
    "    if device.type != \"cuda\":\n",
    "        raise RuntimeError(\"CUDA required for reliable activation memory profiling.\")\n",
    "\n",
    "    set_seed(123)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    model_m = TinyGPT(vocab_size=vocab_size, block_size=block_size, n_layer=2, n_embd=128, n_heads=4, dropout=0.0).to(device).float()\n",
    "    opt_m = torch.optim.AdamW(model_m.parameters(), lr=3e-4)\n",
    "\n",
    "    # Fixed components from tensor dtypes.\n",
    "    param_bytes = sum(_tensor_nbytes(p) for p in model_m.parameters())\n",
    "\n",
    "    max_start = train_data.size(0) - block_size - 1\n",
    "    ix = torch.randint(0, max_start, (batch_size,), device=device)\n",
    "    offsets = torch.arange(block_size, device=device).unsqueeze(0)\n",
    "    xb = train_data[ix.unsqueeze(1) + offsets]\n",
    "    yb = train_data[ix.unsqueeze(1) + offsets + 1]\n",
    "\n",
    "    # Warm-up in the target precision path to reduce one-time kernel/workspace noise.\n",
    "    with torch.no_grad():\n",
    "        with amp_autocast(device, ac_dtype, enabled=use_autocast):\n",
    "            _ = model_m(xb[:2])\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Baseline allocation after model + optimizer construction.\n",
    "    base_alloc = torch.cuda.memory_allocated()\n",
    "\n",
    "    # Forward (capture peak incremental memory, which is dominated by activations/workspaces).\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model_m.zero_grad(set_to_none=True)\n",
    "    with amp_autocast(device, ac_dtype, enabled=use_autocast):\n",
    "        logits = model_m(xb)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), yb.reshape(-1))\n",
    "    torch.cuda.synchronize()\n",
    "    forward_peak = torch.cuda.max_memory_allocated()\n",
    "    activation_peak_bytes = max(0, int(forward_peak - base_alloc))\n",
    "\n",
    "    # Backward to materialize gradients.\n",
    "    loss.backward()\n",
    "    grad_bytes = sum(_tensor_nbytes(p.grad) for p in model_m.parameters() if p.grad is not None)\n",
    "\n",
    "    # One optimizer step initializes AdamW moments (optimizer state memory).\n",
    "    opt_m.step()\n",
    "    opt_state_bytes = _optimizer_state_nbytes(opt_m)\n",
    "\n",
    "    out = {\n",
    "        \"params_MB\": param_bytes / 1024**2,\n",
    "        \"grads_MB\": grad_bytes / 1024**2,\n",
    "        \"optimizer_MB\": opt_state_bytes / 1024**2,\n",
    "        \"activations_peak_MB\": activation_peak_bytes / 1024**2,\n",
    "    }\n",
    "\n",
    "    # Cleanup\n",
    "    del model_m, opt_m, xb, yb, logits, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    return out\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    print(\"CUDA not available. This measured FP32-vs-AMP activation breakdown requires CUDA.\")\n",
    "else:\n",
    "    amp_dtype_mem = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "    fp32_mem = measure_memory_breakdown(use_autocast=False, ac_dtype=None)\n",
    "    amp_mem = measure_memory_breakdown(use_autocast=True, ac_dtype=amp_dtype_mem)\n",
    "\n",
    "    components = [\"Parameters\", \"Gradients\", \"Optimizer state\", \"Activations (peak)\"]\n",
    "    color_map_mem = {\n",
    "        \"Parameters\": \"#1f77b4\",\n",
    "        \"Gradients\": \"#ff7f0e\",\n",
    "        \"Optimizer state\": \"#2ca02c\",\n",
    "        \"Activations (peak)\": \"#d62728\",\n",
    "    }\n",
    "\n",
    "    fp32_vals = [fp32_mem[\"params_MB\"], fp32_mem[\"grads_MB\"], fp32_mem[\"optimizer_MB\"], fp32_mem[\"activations_peak_MB\"]]\n",
    "    amp_vals = [amp_mem[\"params_MB\"], amp_mem[\"grads_MB\"], amp_mem[\"optimizer_MB\"], amp_mem[\"activations_peak_MB\"]]\n",
    "\n",
    "    rows = []\n",
    "    for comp, a, b in zip(components, fp32_vals, amp_vals):\n",
    "        rows.append({\n",
    "            \"component\": comp,\n",
    "            \"fp32_MB\": float(f\"{a:.2f}\"),\n",
    "            f\"amp_{str(amp_dtype_mem).replace('torch.', '')}_MB\": float(f\"{b:.2f}\"),\n",
    "            \"delta_MB\": float(f\"{(b - a):.2f}\"),\n",
    "            \"delta_pct\": float(f\"{(100.0 * (b - a) / max(a, 1e-9)):.2f}\"),\n",
    "        })\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "    scenarios = [\n",
    "        (\"FP32 (no autocast)\", fp32_vals),\n",
    "        (f\"AMP autocast ({str(amp_dtype_mem).replace('torch.', '')})\", amp_vals),\n",
    "    ]\n",
    "\n",
    "    for ax, (title, vals) in zip(axes, scenarios):\n",
    "        bottom = 0.0\n",
    "        for comp, val in zip(components, vals):\n",
    "            ax.bar([0], [val], width=0.55, bottom=[bottom], color=color_map_mem[comp], edgecolor=\"white\")\n",
    "            if val > 1.0:\n",
    "                ax.text(0, bottom + val / 2, f\"{val:.1f} MB\", ha=\"center\", va=\"center\",\n",
    "                        fontsize=8, color=\"white\", fontweight=\"bold\")\n",
    "            bottom += val\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.set_xticks([0])\n",
    "        ax.set_xticklabels([\"TinyGPT step\"])\n",
    "        ax.set_ylabel(\"Memory (MB)\")\n",
    "        ax.set_xlim(-0.6, 0.6)\n",
    "        ax.text(0, bottom + 1.0, f\"Total: {bottom:.1f} MB\", ha=\"center\", va=\"bottom\",\n",
    "                fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_handles = [Patch(color=color_map_mem[k], label=k) for k in components]\n",
    "    fig.legend(handles=legend_handles, loc=\"lower center\", ncol=4, fontsize=8, bbox_to_anchor=(0.5, -0.02))\n",
    "    fig.suptitle(\"Memory composition per training step: FP32 vs AMP on the same model\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    act_save = fp32_mem[\"activations_peak_MB\"] - amp_mem[\"activations_peak_MB\"]\n",
    "    act_save_pct = 100 * act_save / max(fp32_mem[\"activations_peak_MB\"], 1e-9)\n",
    "    total_fp32 = sum(fp32_vals)\n",
    "    total_amp = sum(amp_vals)\n",
    "    total_save_pct = 100 * (total_fp32 - total_amp) / max(total_fp32, 1e-9)\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  - Parameters, gradients, and optimizer state stay in roughly the same range.\")\n",
    "    print(\"  - The large drop is in activation memory under autocast.\")\n",
    "    print(f\"  - Activation peak savings: {act_save:.2f} MB ({act_save_pct:.1f}%)\")\n",
    "    print(f\"  - Total shown-memory savings: {total_save_pct:.1f}%\")\n",
    "    print(\"  - This is why AMP's practical memory win is mainly 'more activation room' for bigger batch/sequence.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fixed-size memory breakdown analysis (parameters + grads + optimizer state)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model_tmp = TinyGPT(vocab_size=vocab_size, block_size=64, n_layer=2, n_embd=128, n_heads=4)\n",
    "n_params = count_params(model_tmp)\n",
    "del model_tmp\n",
    "\n",
    "rows = []\n",
    "configs = [\n",
    "    # AdamW has 2 FP32 moment buffers by default: m and v (8 bytes/param).\n",
    "    {\n",
    "        \"config\": \"FP32 AdamW (baseline)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Params/grads/Adam moments all FP32\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"PyTorch AMP FP16 (compute)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Typical AMP: params/grads/state FP32; matmuls run in FP16 under autocast\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"PyTorch AMP BF16 (compute)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Typical AMP: params/grads/state FP32; matmuls run in BF16 under autocast\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"Naive FP16 AdamW (NOT recommended)\",\n",
    "        \"param_B\": 2, \"master_B\": 0, \"grad_B\": 2, \"opt_B\": 4,\n",
    "        \"notes\": \"Small fixed memory, but numerically fragile (underflow/overflow + low-precision optimizer state)\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"Naive BF16 AdamW (sometimes works)\",\n",
    "        \"param_B\": 2, \"master_B\": 0, \"grad_B\": 2, \"opt_B\": 4,\n",
    "        \"notes\": \"Often trains, but sensitive reductions/normalizations can drift without an FP32 policy\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"FP16 params + FP32 master + FP32 AdamW (classic mixed precision)\",\n",
    "        \"param_B\": 2, \"master_B\": 4, \"grad_B\": 2, \"opt_B\": 8,\n",
    "        \"notes\": \"Common in some stacks: FP16 model copy + FP32 master weights + FP32 optimizer moments\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    total_B = int(cfg[\"param_B\"] + cfg[\"master_B\"] + cfg[\"grad_B\"] + cfg[\"opt_B\"])\n",
    "    param_mb = n_params * cfg[\"param_B\"] / 1024**2\n",
    "    master_mb = n_params * cfg[\"master_B\"] / 1024**2\n",
    "    grad_mb = n_params * cfg[\"grad_B\"] / 1024**2\n",
    "    opt_mb = n_params * cfg[\"opt_B\"] / 1024**2\n",
    "    total_mb = param_mb + master_mb + grad_mb + opt_mb\n",
    "    rows.append({\n",
    "        \"config\": cfg[\"config\"],\n",
    "        \"bytes_per_param\": total_B,\n",
    "        \"param_MB\": float(f\"{param_mb:.3f}\"),\n",
    "        \"master_MB\": float(f\"{master_mb:.3f}\"),\n",
    "        \"grad_MB\": float(f\"{grad_mb:.3f}\"),\n",
    "        \"optimizer_MB\": float(f\"{opt_mb:.3f}\"),\n",
    "        \"total_fixed_MB\": float(f\"{total_mb:.3f}\"),\n",
    "        \"notes\": cfg[\"notes\"],\n",
    "    })\n",
    "\n",
    "print(f\"TinyGPT parameters: {n_params:,}\")\n",
    "print(\"Fixed-size bytes/param = params + master + grads + optimizer state (activations NOT included).\")\n",
    "print(\"(Real LLM AMP savings come mostly from activations, which scale with batch×seq_len.)\\n\")\n",
    "\n",
    "df_mem = pd.DataFrame(rows)\n",
    "display(df_mem)\n",
    "\n",
    "# Visualize fixed memory breakdown as stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "x = np.arange(len(rows))\n",
    "w = 0.5\n",
    "names_m = df_mem[\"config\"].tolist()\n",
    "param_mb = df_mem[\"param_MB\"].tolist()\n",
    "master_mb = df_mem[\"master_MB\"].tolist()\n",
    "grad_mb = df_mem[\"grad_MB\"].tolist()\n",
    "opt_mb = df_mem[\"optimizer_MB\"].tolist()\n",
    "\n",
    "ax.bar(x, param_mb, w, label=\"Model params\", color=\"C0\", alpha=0.8)\n",
    "ax.bar(x, master_mb, w, bottom=param_mb, label=\"Master params\", color=\"C4\", alpha=0.8)\n",
    "ax.bar(x, grad_mb, w, bottom=[p+m for p,m in zip(param_mb, master_mb)], label=\"Gradients\", color=\"C1\", alpha=0.8)\n",
    "ax.bar(\n",
    "    x, opt_mb, w,\n",
    "    bottom=[p+m+g for p,m,g in zip(param_mb, master_mb, grad_mb)],\n",
    "    label=\"Optimizer state\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names_m, rotation=25, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Memory (MB)\")\n",
    "ax.set_title(f\"Fixed memory breakdown by precision config ({n_params:,} params)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nNote: This shows only fixed-size memory (params + grads + optimizer).\")\n",
    "print(\"Activations (which scale with batch×seq_len) are the real memory win for AMP.\")\n",
    "print(\"For large models, activation memory often exceeds parameter memory by 5-10x.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Side-by-side: FP32 vs AMP (with estimated activation memory)\n",
    "# Illustrates where AMP memory savings actually come from.\n",
    "\n",
    "# Assume a representative activation memory estimate:\n",
    "# For a transformer, activation memory per token ≈ 2 * n_layers * n_embd * bytes_per_element\n",
    "# With batch_size=32, block_size=64 → 2048 tokens\n",
    "n_tokens = 32 * 64  # batch_size * block_size\n",
    "n_layers_est = 2\n",
    "n_embd_est = 128\n",
    "\n",
    "# Rough activation bytes: each layer stores ~4 intermediate tensors of shape [batch, seq, embd]\n",
    "act_tensors_per_layer = 4  # (attention input, attention output, FFN input, FFN output)\n",
    "act_elements = n_tokens * n_embd_est * n_layers_est * act_tensors_per_layer\n",
    "\n",
    "fp32_fixed = n_params * 16  # 16 B/param\n",
    "amp_fixed = n_params * 16   # same: 16 B/param\n",
    "fp32_act = act_elements * 4  # FP32 activations\n",
    "amp_act = act_elements * 2   # FP16/BF16 activations\n",
    "\n",
    "categories = [\"Fixed\\n(params+grads+opt)\", \"Activations\\n(saved for backward)\", \"Total\"]\n",
    "fp32_vals = np.array([fp32_fixed, fp32_act, fp32_fixed + fp32_act]) / 1024**2\n",
    "amp_vals = np.array([amp_fixed, amp_act, amp_fixed + amp_act]) / 1024**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.arange(len(categories))\n",
    "w = 0.35\n",
    "bars1 = ax.bar(x - w/2, fp32_vals, w, label=\"FP32\", color=\"C0\", alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "bars2 = ax.bar(x + w/2, amp_vals, w, label=\"AMP (FP16/BF16)\", color=\"C1\", alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "# Annotate bars with values\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, h + 0.01, f\"{h:.2f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories, fontsize=9)\n",
    "ax.set_ylabel(\"Memory (MB)\")\n",
    "ax.set_title(f\"FP32 vs AMP memory breakdown (TinyGPT, {n_params:,} params, batch=32×64)\", fontsize=11)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "savings_pct = (1 - (amp_fixed + amp_act) / (fp32_fixed + fp32_act)) * 100\n",
    "print(f\"Fixed memory: identical (16 B/param in both regimes)\")\n",
    "print(f\"Activation memory: {fp32_act/1024**2:.2f} MB (FP32) → {amp_act/1024**2:.2f} MB (AMP) = 50% reduction\")\n",
    "print(f\"Total estimated savings: {savings_pct:.1f}%\")\n",
    "print()\n",
    "print(\"At LLM scale (billions of params, long sequences), activation memory dominates,\")\n",
    "print(\"so the ~50% activation reduction from AMP translates to substantial GPU memory savings.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 LLM-scale memory projections: when does AMP become mandatory?\n",
    "\n",
    "The TinyGPT numbers above are small. Let's project the fixed memory costs to real model sizes to see where AMP (and distributed training) become necessary.\n",
    "\n",
    "**Important caveat:** These projections show only **fixed-size memory** (parameters + gradients + optimizer state). Activation memory (which scales with batch size $\\times$ sequence length $\\times$ layers) adds significantly more, and that's where AMP's 50% savings on activations really matters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LLM-scale memory projections\n",
    "\n",
    "model_scales = {\n",
    "    \"TinyGPT (this nb)\": n_params,\n",
    "    \"GPT-2 Small (125M)\": 125_000_000,\n",
    "    \"GPT-2 XL (1.5B)\": 1_500_000_000,\n",
    "    \"LLaMA-7B\": 7_000_000_000,\n",
    "    \"LLaMA-13B\": 13_000_000_000,\n",
    "    \"LLaMA-70B\": 70_000_000_000,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, n in model_scales.items():\n",
    "    # AdamW: 2 FP32 moment buffers + FP32 params + FP32 grads = 16 B/param\n",
    "    std_gb = n * 16 / 1024**3\n",
    "    # AMP with AdamW: SAME fixed cost (AMP only saves on activations)\n",
    "    amp_gb = n * 16 / 1024**3\n",
    "    # Naive 16-bit: 2 param + 2 grad + 4 m + 4 v = ~12 B/param (optimizer still FP32-ish)\n",
    "    # ... or full naive: 2+2+2+2 = 8 B/param (risky)\n",
    "    naive_gb = n * 8 / 1024**3\n",
    "    # 8-bit optimizer: 4 param + 4 grad + 1 m + 1 v = ~10 B/param\n",
    "    opt8_gb = n * 10 / 1024**3\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Params\": f\"{n / 1e6:.0f}M\" if n < 1e9 else f\"{n / 1e9:.1f}B\",\n",
    "        \"FP32+AdamW (GB)\": f\"{std_gb:.1f}\",\n",
    "        \"AMP+AdamW (GB)\": f\"{amp_gb:.1f}\",\n",
    "        \"Naive 16bit (GB)\": f\"{naive_gb:.1f}\",\n",
    "        \"AMP+8bit opt (GB)\": f\"{opt8_gb:.1f}\",\n",
    "        \"1x 24GB GPU?\": \"YES\" if amp_gb < 22 else \"NO\",\n",
    "        \"1x 80GB GPU?\": \"YES\" if amp_gb < 75 else \"NO\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  - Standard AMP does NOT reduce fixed memory vs FP32 (both 16 B/param with AdamW)\")\n",
    "print(\"  - A 7B model needs ~112 GB just for params+grads+optimizer (before activations!)\")\n",
    "print(\"  - This exceeds even an 80GB A100 → distributed training (FSDP/ZeRO) is mandatory\")\n",
    "print(\"  - 8-bit optimizers reduce fixed cost by ~37% (16 → 10 B/param)\")\n",
    "print(\"  - The real AMP savings come from activations (50% reduction) + Tensor Core speedups\")\n",
    "print()\n",
    "print(\"  Rule of thumb: you need roughly 4x the model size in GB for inference (FP32),\")\n",
    "print(\"  and 16x for training with AdamW (params + grads + 2 moment buffers).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Empirical activation memory: FP32 vs autocast (CUDA only)\n",
    "\n",
    "The theoretical analysis shows that activation memory halves under AMP. Let's verify this by measuring actual CUDA memory allocation during forward passes at different batch sizes.\n",
    "\n",
    "Activation memory scales with **batch_size $\\times$ sequence_length $\\times$ hidden_dim $\\times$ num_layers**, so the savings from AMP become more significant as you scale up."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Empirical activation memory measurement (CUDA only)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    set_seed(0)\n",
    "\n",
    "    dtype_16_test = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "    test_mem_configs = [\n",
    "        (\"FP32 (no autocast)\", torch.float32, False, None),\n",
    "        (f\"AMP ({dtype_16_test})\", torch.float32, True, dtype_16_test),\n",
    "        (f\"Naive {dtype_16_test}\", dtype_16_test, False, None),\n",
    "    ]\n",
    "\n",
    "    BLOCK_MEM = 64\n",
    "    BATCH_SIZES_MEM = [4, 16, 32, 64]\n",
    "\n",
    "    rows = []\n",
    "    for batch_size in BATCH_SIZES_MEM:\n",
    "        for cfg_name, param_dt, use_ac, ac_dt in test_mem_configs:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "                m_test = TinyGPT(vocab_size, BLOCK_MEM, n_layer=2, n_embd=128, n_heads=4, dropout=0.0)\n",
    "                m_test = m_test.to(device).to(param_dt)\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                mem_model = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "                idx_test = torch.randint(0, vocab_size, (batch_size, BLOCK_MEM), device=device)\n",
    "                with amp_autocast(device, ac_dt, enabled=use_ac):\n",
    "                    logits_test = m_test(idx_test)\n",
    "                    loss_test = F.cross_entropy(\n",
    "                        logits_test.reshape(-1, logits_test.size(-1)),\n",
    "                        idx_test.roll(-1, dims=1).reshape(-1),\n",
    "                    )\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                mem_after_fwd = torch.cuda.memory_allocated() / 1024**2\n",
    "                activation_mem = mem_after_fwd - mem_model\n",
    "\n",
    "                loss_test.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "                rows.append({\n",
    "                    \"batch\": batch_size,\n",
    "                    \"config\": cfg_name,\n",
    "                    \"model_MB\": f\"{mem_model:.1f}\",\n",
    "                    \"activation_MB\": f\"{activation_mem:.1f}\",\n",
    "                    \"peak_MB\": f\"{peak_mem:.1f}\",\n",
    "                })\n",
    "\n",
    "                del m_test, logits_test, loss_test, idx_test\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"batch\": batch_size,\n",
    "                    \"config\": cfg_name,\n",
    "                    \"model_MB\": \"-\",\n",
    "                    \"activation_MB\": \"-\",\n",
    "                    \"peak_MB\": f\"error: {type(e).__name__}\",\n",
    "                })\n",
    "\n",
    "    df_act_mem = pd.DataFrame(rows)\n",
    "    display(df_act_mem)\n",
    "\n",
    "    # Plot activation memory vs batch size for each config\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    for cfg_name, _, _, _ in test_mem_configs:\n",
    "        subset = df_act_mem[df_act_mem[\"config\"] == cfg_name]\n",
    "        try:\n",
    "            bs_vals = subset[\"batch\"].tolist()\n",
    "            act_vals = [float(v) for v in subset[\"activation_MB\"].tolist()]\n",
    "            ax.plot(bs_vals, act_vals, marker=\"o\", label=cfg_name, linewidth=2)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    ax.set_xlabel(\"Batch size\")\n",
    "    ax.set_ylabel(\"Activation memory (MB)\")\n",
    "    ax.set_title(\"Activation memory vs batch size — AMP halves activation storage\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(\"\\nKey observation:\")\n",
    "    print(\"  Activation memory (the gap between model-loaded and after-forward) scales with batch size.\")\n",
    "    print(\"  Under autocast, activations are stored in FP16/BF16 → roughly half the memory.\")\n",
    "    print(\"  This is the PRIMARY memory benefit of AMP for training.\")\n",
    "    print(\"  At large batch sizes and long sequences, this savings is substantial.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Activation memory profiling requires CUDA.\")\n",
    "    print(\"Key point: autocast halves activation memory because intermediate tensors\")\n",
    "    print(\"saved for backward are stored in FP16/BF16 instead of FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Interpreting results\n",
    "\n",
    "### Loss curves\n",
    "- **FP32**: should decrease smoothly. This is your reference.\n",
    "- **FP16 naive**: may diverge, stagnate, or produce NaN. This demonstrates why casting everything to half precision is not safe.\n",
    "- **BF16 naive**: often converges to a similar loss as FP32. This is the dramatic proof that BF16's range (8-bit exponent) matters more than FP16's precision (10-bit mantissa) for training stability. The network tolerates noisy gradient values; it cannot tolerate zero gradient values.\n",
    "- **AMP FP16 (no scaler)**: may work but may show more zero gradients than the scaled version. This shows that autocast alone helps (per-op policy keeps sensitive ops in FP32) but doesn't fully solve the backward-pass underflow problem.\n",
    "- **AMP FP16 (with scaler)**: should be stable. The GradScaler rescues gradients from underflow.\n",
    "- **AMP BF16**: typically matches FP32 quality without needing a scaler.\n",
    "\n",
    "### Step time\n",
    "- On modern GPUs, AMP often reduces step time because matmuls hit Tensor Cores.\n",
    "- If you don't see speedup: model may be too small (overhead dominates), or you're CPU-bound.\n",
    "- The smallest models sometimes see AMP *overhead* because the per-op dispatch cost exceeds the Tensor Core gains. This goes away at realistic model sizes.\n",
    "\n",
    "### Gradient norms\n",
    "- Should be comparable across regimes for a well-behaved model.\n",
    "- If FP16 naive shows erratic norms, that's the underflow/overflow instability.\n",
    "- BF16 naive norms should be close to FP32 (same gradient magnitudes, just represented with fewer mantissa bits).\n",
    "\n",
    "### Zero gradients\n",
    "- A high `zero_grad_frac` indicates underflow or dead signal (exact zeros).\n",
    "- **FP16 naive** and **AMP FP16 no scaler** may show elevated zero-gradient fractions.\n",
    "- **BF16 naive** should show low zero-gradient fractions (gradients don't underflow with 8-bit exponent).\n",
    "- The difference between FP16+scaler and FP16 without scaler directly measures the benefit of loss scaling.\n",
    "\n",
    "### GradScaler scale\n",
    "- If scale drops repeatedly, the model is hitting overflow events and GradScaler is skipping steps.\n",
    "- If scale grows steadily, training is stable and GradScaler is increasing headroom.\n",
    "- A \"spiky\" pattern (rapid drops followed by slow climbs) is normal and expected.\n",
    "\n",
    "### Memory\n",
    "- AMP typically uses ~same or slightly more memory than FP32 for parameters+optimizer (due to master weights), but saves on activations.\n",
    "- Naive 16-bit uses less total memory but trades stability.\n",
    "- The savings become dramatic at larger batch sizes and sequence lengths.\n",
    "- At LLM scale with Adam: 16 bytes/param (mixed precision) vs 16 bytes/param (FP32). The *activation* memory savings are where AMP really wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Practical checklist\n",
    "\n",
    "### Defaults that usually work\n",
    "- **CUDA:** prefer **BF16 autocast** if your GPU supports it (Ampere+). No GradScaler needed.\n",
    "- **CUDA (no BF16):** use **FP16 autocast + GradScaler**.\n",
    "- **CPU:** use **BF16 autocast** (mostly for numerics; speedups depend on CPU/kernel support).\n",
    "- **MPS:** use **FP16 autocast** (operator coverage differs from CUDA; verify with probes in Section 3).\n",
    "- Keep optimizer state in FP32 (default for most PyTorch optimizers).\n",
    "\n",
    "### When things go wrong\n",
    "1. **Loss becomes `nan`/`inf`:** Check for overflow sources (attention logits, exp/log, unstable loss). Consider lowering learning rate or adding gradient clipping.\n",
    "2. **Gradients mostly zero in FP16:** Use GradScaler with higher initial scale. Consider switching to BF16.\n",
    "3. **Training \"does nothing\":** Check for weight update stagnation — are weights actually changing? Ensure you have FP32 master weights (standard when model is FP32 + autocast).\n",
    "4. **Unexplained dtype behavior:** Use dtype hooks (Section 3.3) to confirm what's running in what dtype.\n",
    "\n",
    "### Common gotchas\n",
    "- Mixing manual `.half()` casts with autocast can lead to unexpected behavior.\n",
    "- Gradient clipping should happen **after** `scaler.unscale_(optimizer)`.\n",
    "- `autocast` should cover forward + loss, but **not** the optimizer step.\n",
    "- Be careful with `out=` tensors and in-place ops: autocast does not rewrite an `out=` buffer’s dtype, and in-place ops can't change dtype.\n",
    "- Autocast has an internal cast cache (weight-cast reuse). If you see weird memory growth or do unusual in-place weight mutation, try `cache_enabled=False`.\n",
    "- On Ampere+ GPUs, your \"FP32 baseline\" may use TF32 for matmuls. Check `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9.1 Decision tree: which precision regime should I use?\n",
    "\n",
    "```\n",
    "START\n",
    "  │\n",
    "  ├─ Is this inference only?\n",
    "  │   YES → autocast(dtype=bf16 or fp16) + inference_mode()\n",
    "  │          No GradScaler needed. Pick bf16 if supported.\n",
    "  │\n",
    "  ├─ Is this training?\n",
    "  │   │\n",
    "  │   ├─ Does your GPU support BF16? (Ampere+, TPU, recent AMD)\n",
    "  │   │   YES → autocast(dtype=bfloat16), NO GradScaler\n",
    "  │   │          Keep model params in FP32. Autocast handles per-op casting.\n",
    "  │   │          This is the simplest and most robust mixed-precision path.\n",
    "  │   │\n",
    "  │   ├─ GPU supports FP16 but NOT BF16? (Volta, Turing, older)\n",
    "  │   │   → autocast(dtype=float16) + GradScaler\n",
    "  │   │     Keep model params in FP32. GradScaler prevents gradient underflow.\n",
    "  │   │\n",
    "  │   ├─ CPU only?\n",
    "  │   │   → autocast(dtype=bfloat16) on CPU\n",
    "  │   │     Speedups vary by CPU. Mostly useful for numeric consistency with GPU training.\n",
    "  │   │\n",
    "  │   └─ Need maximum speed / memory savings beyond standard AMP?\n",
    "  │       → Consider: 8-bit optimizers, FP8 (H100+), gradient checkpointing,\n",
    "  │         FSDP/ZeRO for distributed, or combinations thereof.\n",
    "  │\n",
    "  └─ ALWAYS:\n",
    "      - Keep optimizer state in FP32 (default for PyTorch optimizers)\n",
    "      - Keep model parameters in FP32 (let autocast handle per-op casting)\n",
    "      - Make tensor dimensions multiples of 8 for Tensor Core alignment\n",
    "      - Put forward + loss inside autocast context; optimizer step OUTSIDE\n",
    "```\n",
    "\n",
    "**The most common mistake:** calling `model.half()` or `model.bfloat16()` and thinking that's \"mixed precision.\" It's not — it's *uniform* low precision with no per-op safety policy. Use autocast instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Real-world AMP patterns (copy/paste templates)\n",
    "\n",
    "This section is intentionally practical: patterns that show up once you move from a notebook demo to a real training run.\n",
    "\n",
    "### 3.10.1 Gradient accumulation (microbatches)\n",
    "\n",
    "If you do gradient accumulation, the safest pattern is:\n",
    "\n",
    "```python\n",
    "scaler = GradScaler()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for micro in range(grad_accum_steps):\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        loss = loss_fn(model(x_micro), y_micro)\n",
    "        loss = loss / grad_accum_steps      # IMPORTANT: normalize\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "scaler.unscale_(optimizer)                  # IMPORTANT: before clipping / inspecting grads\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "### 3.10.2 Gradient clipping with FP16 AMP\n",
    "\n",
    "Clipping should happen **after** `scaler.unscale_(optimizer)`. If you clip *scaled* gradients, you're clipping the wrong numbers.\n",
    "\n",
    "### 3.10.3 Multiple optimizers / parameter groups\n",
    "\n",
    "Use **one scaler**. Call `scaler.step(optimizer_i)` for each optimizer, then `scaler.update()` once per iteration.\n",
    "\n",
    "### 3.10.4 Custom `autograd.Function` / custom ops\n",
    "\n",
    "If you write custom autograd functions (or CUDA extensions), make them autocast-safe. In PyTorch there are decorators for this (API varies by version):\n",
    "\n",
    "```python\n",
    "try:\n",
    "    from torch.amp import custom_fwd, custom_bwd\n",
    "except Exception:\n",
    "    from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "class MyFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @custom_fwd(cast_inputs=torch.float16)\n",
    "    def forward(ctx, x, w):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_out):\n",
    "        ...\n",
    "```\n",
    "\n",
    "If you skip this, autocast may feed your op tensors in unexpected dtypes, and you'll get silent accuracy bugs or runtime errors.\n",
    "\n",
    "### 3.10.5 Inference autocast: simpler than training\n",
    "\n",
    "For inference you only need `autocast` + `inference_mode` (no GradScaler):\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        output = model(input_ids)\n",
    "```\n",
    "\n",
    "The benefit: matmuls run on Tensor Cores (faster) and activations are 16-bit (less memory), letting you serve larger batches or longer sequences. No gradient-related concerns.\n",
    "\n",
    "### 3.10.6 Troubleshooting table\n",
    "\n",
    "| Symptom | Likely cause | Quick check | Fix |\n",
    "|---|---|---|---|\n",
    "| Loss becomes `nan`/`inf` quickly | overflow in activations (attention logits, exp/log) or too-high LR | check `torch.isfinite(loss)`; watch GradScaler `found_inf`/scale drops | lower LR, add grad clipping, prefer BF16, check initialization |\n",
    "| Gradients mostly 0 in FP16 | underflow | measure `zero_grad_frac` or histogram of `log10(|grad|)` | use GradScaler (bigger initial scale), or switch to BF16 |\n",
    "| GradScaler scale keeps collapsing | frequent overflow events | scale drops + many skipped steps | lower LR, clip grads, check for unstable ops, switch to BF16 |\n",
    "| Training \"does nothing\" | weight update below ULP (stagnation) | log `max(|w_{t+1}-w_t|)` in model dtype | keep FP32 master weights (standard in AMP), avoid FP16 optimizer state |\n",
    "| No speedup from AMP | model too small, CPU-bound, or bad matmul shapes | compare tokens/s; check shapes are multiples of 8 | increase batch/seq, use Tensor Core-friendly dims, benchmark with realistic sizes |\n",
    "\n",
    "### 3.10.7 Autocast gotchas: `out=`, in-place ops, and the cast cache\n",
    "\n",
    "These are easy to miss because they look like \"normal PyTorch\", but they interact with autocast in non-obvious ways.\n",
    "\n",
    "- **`out=` buffers:** autocast does not change the dtype of an `out=` tensor. If `out` is FP32, the op will usually write FP32 even inside an FP16 autocast region (and vice-versa). For AMP code, prefer the functional form **without** `out=`.\n",
    "- **In-place ops:** in-place ops cannot change dtype. Mixed autocast + in-place updates can create dtype mismatches or force unexpected promotions.\n",
    "- **Autocast cast cache:** CUDA autocast caches some casts (especially weight casts) for speed. This is usually what you want. If you do unusual in-place weight mutation inside autocast regions or see unexpected memory behavior, try `cache_enabled=False`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "with autocast(device_type=\"cuda\", dtype=torch.float16, cache_enabled=False):\n",
    "    loss = model(x).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix — References\n",
    "\n",
    "## Papers\n",
    "\n",
    "| Paper | Year | Key contribution |\n",
    "|---|---|---|\n",
    "| Gupta et al., *Deep Learning with Limited Numerical Precision* | 2015 | Low-precision training needs deliberate rounding/scaling; stochastic rounding intuition |\n",
    "| Micikevicius et al., *Mixed Precision Training* | 2017 | FP32 master weights + loss scaling + per-op policies |\n",
    "| Kalamkar et al., *A Study of BFLOAT16 for Deep Learning Training* | 2019 | BF16 empirical validation, no loss scaling needed |\n",
    "| Rajbhandari et al., *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models* | 2020 | Memory breakdown of mixed-precision training |\n",
    "| NVIDIA (and others), *FP8 Formats for Deep Learning* | 2022 | FP8 formats (E4M3/E5M2), scaling metadata, and accumulation strategies |\n",
    "| Dettmers et al., *8-bit Optimizers via Block-wise Quantization* | 2022 | Reducing optimizer-state memory beyond AMP |\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- [PyTorch `torch.amp` docs](https://pytorch.org/docs/stable/amp.html) — autocast op reference, GradScaler API\n",
    "- [PyTorch AMP examples](https://docs.pytorch.org/docs/stable/notes/amp_examples.html) — canonical loop + edge cases\n",
    "- [Autocast availability API](https://docs.pytorch.org/docs/stable/amp.html#torch.amp.autocast_mode.is_autocast_available) — backend support probing\n",
    "- [NVIDIA mixed precision blog](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n",
    "- [Google BF16 blog](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    "- [DeepSpeed config docs](https://www.deepspeed.ai/docs/config-json/) — FP16/BF16 training config\n",
    "- [PyTorch FSDP mixed precision](https://pytorch.org/docs/stable/fsdp.html)\n",
    "- [PyTorch blog: Training with float8 and FSDP2](https://docs.pytorch.org/blog/training-using-float8-fsdp2/) — large-scale float8 practicals\n",
    "- [PyTorch tutorial: Supercharging training with float8 and FSDP2](https://docs.pytorch.org/tutorials/unstable/float8_fsdp2_tutorial.html) — implementation details and benchmarks\n",
    "- [NVIDIA Transformer Engine FP8 primer](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html) — FP8/MXFP8/NVFP4 format and scaling details\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's experiments are intentionally small; the *mechanisms* are the same at scale."
   ]
  }
 ]
}
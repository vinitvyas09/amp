{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocast / AMP in PyTorch: A Deep Practical Reference\n",
    "\n",
    "Every neural network is fundamentally a giant mathematical expression, and training is the process of tuning the constants (weights) so that expression approximates reality. We do this by computing a loss, backpropagating to get gradients, and nudging the weights a tiny bit in the right direction. Repeat billions of times.\n",
    "\n",
    "Why do we care about this *now*? Because modern deep learning training is GPU-dominated and compute-hungry. Matrix multiplications are embarrassingly parallel, so GPUs can run thousands of tiny operations at once — but as models grow, the cost of training keeps rising. Mixed precision training is one of the highest-leverage efficiency techniques we have: it uses fast low-precision hardware (Tensor Cores / specialized units) where it's safe, while preserving FP32 where training would otherwise become numerically unstable.\n",
    "\n",
    "But here's the thing: those weights and gradients are stored as **floating-point numbers**. And floating-point numbers are *not* real numbers. They are a finite approximation with a specific *resolution*. That resolution depends on the format — FP32, FP16, BF16, TF32, FP8 — and each format makes a different tradeoff between **range** (how large and small the numbers can get) and **precision** (how fine the steps between consecutive representable values are).\n",
    "\n",
    "This matters because during training, we routinely deal with numbers spanning many orders of magnitude: weights around $10^0$, gradients as small as $10^{-8}$, attention logits that can spike to $10^3$, and accumulations over thousands of values. A format that can't represent tiny gradients kills learning (they become zero). A format that can't represent large attention logits kills stability (they become infinity). A format that rounds too aggressively corrupts the running statistics that normalization layers depend on.\n",
    "\n",
    "The fundamental question this notebook answers is: **when can we get away with lower resolution, and when does it break training?**\n",
    "\n",
    "Autocast (AMP) is PyTorch's answer: a per-operation precision policy that routes each computation to the right format. Some ops get the speed of 16-bit. Some ops get the safety of 32-bit. The combined effect is faster training with (nearly) no loss in quality. But to understand *when* and *why* AMP works — and to debug it when it doesn't — you need to understand what floating-point formats can and cannot represent.\n",
    "\n",
    "That's what this notebook is for.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is organized into **three sections**:\n",
    "\n",
    "| # | Section | What you get |\n",
    "|---|---|---|\n",
    "| **1** | **Theory** | Floating-point range vs precision, FP16/BF16/FP32/TF32 comparison tables, underflow/overflow, what AMP is really doing |\n",
    "| **2** | **What the literature says** | Paper-driven mental models (Micikevicius et al., Kalamkar et al., ZeRO, NVIDIA guidance), written explanations — *no experiments* |\n",
    "| **3** | **Practicalities** | Hands-on experiments + graphs: progressive mixed-precision implementation from scratch, operator policy probes, dtype flow through a transformer, loss/gradient/scale curves under different precision regimes |\n",
    "\n",
    "---\n",
    "\n",
    "**After completing this notebook you should be able to answer (and debug) questions like:**\n",
    "\n",
    "- Why does FP16 often need **loss scaling**, but BF16 often does not?\n",
    "- What does `autocast` *actually* do per operation (matmul vs softmax vs layernorm)?\n",
    "- Why do people talk about **FP32 master weights** and **optimizer state precision**?\n",
    "- Why does adding `1e-4` to `1.0` in FP16 produce exactly `1.0`? (with the exact bit-level explanation)\n",
    "- How can you *see* autocast happening inside a transformer forward pass?\n",
    "- What fails if you try to \"just train in half precision everywhere\"?\n",
    "- Why does naive BF16 often work where naive FP16 fails?\n",
    "- What is the \"sum vs mean\" mystery under autocast?\n",
    "- How does the gradient distribution relate to FP16's representable range?\n",
    "- How many bytes per parameter does mixed-precision Adam training actually use?\n",
    "- Why is floating-point addition NOT associative, and why does that matter for GPU reductions?\n",
    "- What is catastrophic cancellation, and why does it make LayerNorm precision-sensitive?\n",
    "- How does stochastic rounding differ from deterministic rounding, and when does it help?\n",
    "- Which layers in a transformer are most affected by autocast precision changes?\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "- Read the markdown, then run the code cells.\n",
    "- Most experiments are designed to run in a few minutes on a single GPU.\n",
    "- CPU-only runs are supported for the *conceptual* demos, but some mixed-precision behaviors (and speedups) are fundamentally GPU-driven.\n",
    "\n",
    "### If you're in a hurry (recommended run path)\n",
    "\n",
    "- **10 minutes:** run setup → Quick Reference table → **3.1 progressive mixed precision** plots\n",
    "- **30–60 minutes (GPU):** add **3.6 TinyGPT training suite** (loss/time/memory graphs)\n",
    "- **Deep dive:** run everything in order; each experiment builds on the previous mental model\n",
    "\n",
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "**Section 1 — Theory**\n",
    "- Quick reference cheat sheet (the table you should memorize)\n",
    "- Floating-point: range vs precision, IEEE 754 anatomy\n",
    "- Number line visualization: where representable floats live\n",
    "- FP16 vs BF16 vs FP32 vs TF32 (tables you can trust)\n",
    "- The bit-level addition trap (why `1 + 1e-4 = 1` in FP16) — with step-by-step binary alignment\n",
    "- Underflow, overflow, accumulation error\n",
    "- **Non-associativity**: why summation order matters in low precision\n",
    "- **Catastrophic cancellation**: why LayerNorm/BatchNorm need FP32\n",
    "- **Kahan summation**: the compensated-sum fix\n",
    "- What AMP is (autocast + grad scaling)\n",
    "- Master weights, optimizer state, and accumulation\n",
    "\n",
    "**Section 2 — What the literature says**\n",
    "- Micikevicius et al. — *Mixed Precision Training*\n",
    "- Kalamkar et al. — *A Study of BFLOAT16 for Deep Learning Training*\n",
    "- NVIDIA mixed precision guidance\n",
    "- BF16 design intent\n",
    "- PyTorch AMP operator policy\n",
    "- **Stochastic rounding**: why it helps low-precision training (Gupta et al.)\n",
    "- Rajbhandari et al. — ZeRO and optimizer state precision\n",
    "- LLM training stacks (FSDP/ZeRO) and where AMP fits\n",
    "- FP8 and 8-bit optimizers\n",
    "\n",
    "**Section 3 — Practicalities**\n",
    "- Progressive mixed-precision implementation from scratch (FP32 → naive FP16 → **naive BF16** → master weights → loss scaling → PyTorch AMP)\n",
    "- Build an operator policy table *from your local PyTorch*\n",
    "- The \"sum vs mean\" mystery\n",
    "- Visualize dtype flow through a transformer (4 configurations)\n",
    "- **Per-layer precision sensitivity**: which parts of the transformer hurt most?\n",
    "- Gradient underflow + the effect of loss scaling\n",
    "- **Micikevicius-style gradient histogram analysis**\n",
    "- Weight update stagnation\n",
    "- Train a tiny causal LM under different precision regimes (FP32, FP16 naive, BF16 naive, AMP FP16, AMP BF16)\n",
    "- Plot and interpret loss/time/scale/gradient curves + summary bar charts\n",
    "\n",
    "---\n",
    "\n",
    "## Quick glossary\n",
    "\n",
    "| Term | Meaning |\n",
    "|---|---|\n",
    "| **AMP** | Automatic Mixed Precision (in PyTorch: `torch.amp`) |\n",
    "| **autocast** | Context manager that applies a per-operation dtype policy |\n",
    "| **GradScaler / loss scaling** | Rescales loss to avoid FP16 gradient underflow |\n",
    "| **master weights** | Keep weights in FP32 for updates, cast for compute |\n",
    "| **underflow** | Magnitude too small → becomes 0 (or subnormal/denormal) |\n",
    "| **overflow** | Magnitude too large → becomes `inf` |\n",
    "| **TF32** | TensorFloat-32 — an NVIDIA format with FP32 range but 10-bit mantissa, used transparently in Ampere+ matmuls |\n",
    "| **ULP** | Unit in the Last Place — the spacing between adjacent representable floats |\n",
    "| **epsilon** | Smallest number such that `1.0 + eps > 1.0` in a given format |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need:\n",
    "- Python 3.10+\n",
    "- PyTorch 2.x\n",
    "- `matplotlib`, `numpy`, `pandas`\n",
    "\n",
    "### Install (CPU-only quick start)\n",
    "```bash\n",
    "pip install torch numpy pandas matplotlib\n",
    "```\n",
    "\n",
    "### Install (CUDA)\n",
    "Install the correct PyTorch + CUDA build from the [official PyTorch instructions](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is written to *degrade gracefully*:\n",
    "- If BF16 is not supported on your GPU, BF16 experiments will be skipped.\n",
    "- If FP16 training without scaling explodes (often does), we record that as a result rather than pretending it \"worked\".\n",
    "\n",
    "> **Note:** on Apple Silicon, this notebook defaults to **CPU** for maximum compatibility. If you want to try the Apple GPU backend, set `USE_MPS_IF_AVAILABLE = True` in the first code cell. AMP/autocast behavior is best-defined on CUDA; CPU/MPS support exists but has different operator coverage and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core imports + environment report\n",
    "import os, math, time, random, struct, platform\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: numpy. Install with: pip install numpy\") from e\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: pandas. Install with: pip install pandas\") from e\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.ticker as ticker\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\"Missing dependency: matplotlib. Install with: pip install matplotlib\") from e\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 4),\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 100,\n",
    "})\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"PyTorch is required. CPU-only: pip install torch\"\n",
    "    ) from e\n",
    "\n",
    "# Prefer torch.amp (newer API)\n",
    "if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\"):\n",
    "    autocast = torch.amp.autocast\n",
    "    GradScaler = torch.amp.GradScaler\n",
    "else:\n",
    "    autocast = torch.cuda.amp.autocast\n",
    "    GradScaler = torch.cuda.amp.GradScaler\n",
    "\n",
    "def amp_autocast(dev: torch.device, dtype: torch.dtype | None, enabled: bool = True, cache_enabled: bool = True):\n",
    "    # Best-effort autocast context manager that degrades gracefully.\n",
    "    if (not enabled) or (dtype is None):\n",
    "        return nullcontext()\n",
    "    try:\n",
    "        return autocast(device_type=dev.type, dtype=dtype, enabled=True, cache_enabled=cache_enabled)\n",
    "    except TypeError:\n",
    "        # Older signatures may not support cache_enabled.\n",
    "        try:\n",
    "            return autocast(device_type=dev.type, dtype=dtype, enabled=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "            return nullcontext()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] autocast unavailable for device_type={dev.type}: {e}. Running without autocast.\")\n",
    "        return nullcontext()\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# ---- User knobs -------------------------------------------------------------\n",
    "# Leave as-is for \"works everywhere\" defaults; tweak if you have specific HW.\n",
    "PREFERRED_DEVICE = None   # one of: \"cuda\", \"mps\", \"cpu\" (or None for auto)\n",
    "USE_MPS_IF_AVAILABLE = False  # Apple Silicon: set True to try MPS when no CUDA\n",
    "\n",
    "def choose_device():\n",
    "    if PREFERRED_DEVICE is not None:\n",
    "        pref = str(PREFERRED_DEVICE).lower()\n",
    "        if pref == \"cuda\" and torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        if pref == \"mps\" and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        if pref == \"cpu\":\n",
    "            return torch.device(\"cpu\")\n",
    "        print(f\"[warn] Requested device '{PREFERRED_DEVICE}' not available; falling back to auto.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if USE_MPS_IF_AVAILABLE and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = choose_device()\n",
    "\n",
    "def supports_dtype_on_device(dtype: torch.dtype, dev: torch.device) -> bool:\n",
    "    try:\n",
    "        torch.tensor([0.0], device=dev, dtype=dtype)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"Python  {platform.python_version()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA:   {torch.version.cuda}\")\n",
    "    print(f\"GPU:    {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"BF16:   {'supported' if torch.cuda.is_bf16_supported() else 'NOT supported'}\")\n",
    "elif device.type == \"mps\":\n",
    "    print(\"MPS:    available (Apple Silicon)\")\n",
    "    print(f\"FP16:   {'supported' if supports_dtype_on_device(torch.float16, device) else 'NOT supported'}\")\n",
    "    print(f\"BF16:   {'supported' if supports_dtype_on_device(torch.bfloat16, device) else 'NOT supported'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Floating-Point Formats for Deep Learning\n",
    "\n",
    "This is the table you should memorize. Every design decision in AMP traces back to these numbers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate the cheat sheet from your local PyTorch (so the numbers are trustworthy)\n",
    "\n",
    "def _ulp_at_one(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "formats = [\n",
    "    # FP8 is increasingly common for LLM training/inference, but support depends on HW + kernel stack.\n",
    "    (\"FP8 (E4M3)\", getattr(torch, \"float8_e4m3fn\", None), 4, 3),\n",
    "    (\"FP8 (E5M2)\", getattr(torch, \"float8_e5m2\", None), 5, 2),\n",
    "    (\"FP16 (IEEE half)\", torch.float16, 5, 10),\n",
    "    (\"BF16 (brain float)\", torch.bfloat16, 8, 7),\n",
    "    (\"FP32 (single)\", torch.float32, 8, 23),\n",
    "    (\"FP64 (double)\", torch.float64, 11, 52),\n",
    "]\n",
    "\n",
    "for name, dt, exp_b, mant_b in formats:\n",
    "    if dt is None:\n",
    "        continue\n",
    "    try:\n",
    "        fi = torch.finfo(dt)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    ulp1 = _ulp_at_one(dt)\n",
    "    sub = _smallest_subnormal(dt)\n",
    "\n",
    "    rows.append({\n",
    "        \"Format\": name,\n",
    "        \"Total bits\": fi.bits,\n",
    "        \"Exponent bits\": exp_b,\n",
    "        \"Mantissa bits\": mant_b,\n",
    "        \"Precision bits (incl hidden 1)\": mant_b + 1,\n",
    "        \"Approx decimal digits\": round((mant_b + 1) * math.log10(2), 1),\n",
    "        \"Exponent range\": f\"[{-2**(exp_b-1)+2}, {2**(exp_b-1)-1}]\",\n",
    "        \"epsilon (ULP at 1.0)\": f\"{fi.eps:.2e}\",\n",
    "        \"ULP at 1.0\": f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\",\n",
    "        \"Min normal\": f\"{fi.tiny:.2e}\",\n",
    "        \"Min subnormal\": f\"{sub:.2e}\" if sub is not None else \"n/a\",\n",
    "        \"Max finite\": f\"{fi.max:.2e}\",\n",
    "    })\n",
    "\n",
    "# Add TF32 manually (not a storable dtype in PyTorch, but important to know)\n",
    "tf32_row = {\n",
    "    \"Format\": \"TF32 (tensor float)\",\n",
    "    \"Total bits\": \"19*\",\n",
    "    \"Exponent bits\": 8,\n",
    "    \"Mantissa bits\": 10,\n",
    "    \"Precision bits (incl hidden 1)\": 11,\n",
    "    \"Approx decimal digits\": 3.3,\n",
    "    \"Exponent range\": \"[-126, 127]\",\n",
    "    \"epsilon (ULP at 1.0)\": \"9.77e-04\",\n",
    "    \"ULP at 1.0\": \"9.77e-04\",\n",
    "    \"Min normal\": \"1.18e-38\",\n",
    "    \"Min subnormal\": \"n/a (internal)\",\n",
    "    \"Max finite\": \"3.40e+38\",\n",
    "}\n",
    "\n",
    "# Insert TF32 right after FP32 if present; otherwise append.\n",
    "insert_at = None\n",
    "for i, r in enumerate(rows):\n",
    "    if r[\"Format\"].startswith(\"FP32\"):\n",
    "        insert_at = i + 1\n",
    "        break\n",
    "if insert_at is None:\n",
    "    rows.append(tf32_row)\n",
    "else:\n",
    "    rows.insert(insert_at, tf32_row)\n",
    "\n",
    "df_cheat = pd.DataFrame(rows).set_index(\"Format\")\n",
    "display(df_cheat)\n",
    "\n",
    "print()\n",
    "print(\"* TF32 is not a storage format. It is used internally by Tensor Cores on\")\n",
    "print(\"  Ampere+ GPUs for FP32 matmuls: FP32 range, but only 10 mantissa bits.\")\n",
    "print(\"  Your 'FP32 baseline' on Ampere+ may secretly be TF32 precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read this table\n",
    "\n",
    "Think of a floating-point format as a **measuring tape**:\n",
    "- **Exponent bits** determine the **length** of the tape (range: how big/small magnitudes you can reach).\n",
    "- **Mantissa bits** determine how **fine the tick marks** are (precision: how many significant digits you keep).\n",
    "\n",
    "| Format | Range | Precision | Training implication |\n",
    "|---|---|---|---|\n",
    "| **FP8** | Very narrow | Very low | Needs careful scaling (per-tensor/per-channel), mostly used with specialized kernels/hardware |\n",
    "| **FP16** | Narrow (5-bit exp) | Moderate (10-bit mantissa) | Underflow risk for gradients, overflow risk for activations → needs **loss scaling** |\n",
    "| **BF16** | Wide (8-bit exp, same as FP32) | Low (7-bit mantissa) | Rarely underflows → usually **no loss scaling** needed, but coarse rounding in reductions |\n",
    "| **FP32** | Wide | High | Stable baseline; slower and more memory |\n",
    "| **TF32** | Wide | Moderate (10-bit mantissa) | Compute-only on Ampere+ GPUs: FP32 matmuls may silently use TF32 for speed |\n",
    "| **FP64** | Very wide | Very high | Mostly for numeric reference/debugging (too slow for large training) |\n",
    "\n",
    "**Two immediate consequences:**\n",
    "1. FP16 has more mantissa bits than BF16 → **better precision per value**.\n",
    "2. BF16 has the same exponent width as FP32 → **dramatically better range** than FP16.\n",
    "\n",
    "So: FP16 fails first due to **range** (underflow/overflow). BF16 fails first due to **precision** (rounding/accumulation error). FP8 fails due to both unless extra care is taken.\n",
    "\n",
    "Autocast exists to route computations so that you get the performance of 16-bit compute without the worst numeric failure modes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visual bit layout: sign / exponent / mantissa for each format\n",
    "# This is the single most referenced diagram in floating-point explanations.\n",
    "\n",
    "formats_vis = [\n",
    "    (\"FP64 (double)\",     1, 11, 52, 64),\n",
    "    (\"FP32 (single)\",     1,  8, 23, 32),\n",
    "    (\"TF32 (tensor)*\",    1,  8, 10, 19),\n",
    "    (\"BF16 (brain float)\",1,  8,  7, 16),\n",
    "    (\"FP16 (IEEE half)\",  1,  5, 10, 16),\n",
    "    (\"FP8 (E5M2)\",        1,  5,  2,  8),\n",
    "    (\"FP8 (E4M3)\",        1,  4,  3,  8),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "y_positions = list(range(len(formats_vis)))[::-1]\n",
    "bar_height = 0.6\n",
    "max_bits = max(f[4] for f in formats_vis)\n",
    "\n",
    "for i, (name, s_bits, e_bits, m_bits, total) in enumerate(formats_vis):\n",
    "    y = y_positions[i]\n",
    "    # Scale bar width proportional to bit count (relative to max)\n",
    "    scale = 0.85  # fraction of plot width for the widest format\n",
    "    bit_width = scale / max_bits\n",
    "\n",
    "    # Draw sign bits\n",
    "    ax.barh(y, s_bits * bit_width, height=bar_height, left=0,\n",
    "            color=\"#e74c3c\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "    # Draw exponent bits\n",
    "    ax.barh(y, e_bits * bit_width, height=bar_height, left=s_bits * bit_width,\n",
    "            color=\"#3498db\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "    # Draw mantissa bits\n",
    "    ax.barh(y, m_bits * bit_width, height=bar_height, left=(s_bits + e_bits) * bit_width,\n",
    "            color=\"#2ecc71\", edgecolor=\"white\", linewidth=1.5, zorder=3)\n",
    "\n",
    "    # Labels inside bars\n",
    "    mid_s = s_bits * bit_width / 2\n",
    "    mid_e = (s_bits + e_bits / 2) * bit_width\n",
    "    mid_m = (s_bits + e_bits + m_bits / 2) * bit_width\n",
    "\n",
    "    fontsize = 8 if total >= 16 else 7\n",
    "    if s_bits >= 1:\n",
    "        ax.text(mid_s, y, f\"S\\n{s_bits}\", ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "    ax.text(mid_e, y, f\"Exp\\n{e_bits}\", ha=\"center\", va=\"center\", fontsize=fontsize, fontweight=\"bold\", color=\"white\")\n",
    "    if m_bits >= 2:\n",
    "        ax.text(mid_m, y, f\"Mantissa\\n{m_bits}\", ha=\"center\", va=\"center\", fontsize=fontsize, fontweight=\"bold\", color=\"white\")\n",
    "    else:\n",
    "        ax.text(mid_m, y, f\"M{m_bits}\", ha=\"center\", va=\"center\", fontsize=6, fontweight=\"bold\", color=\"white\")\n",
    "\n",
    "    # Format name and total bits on the left\n",
    "    ax.text(-0.02, y, f\"{name}  [{total} bits]\", ha=\"right\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlim(-0.55, max_bits * scale / max_bits + 0.05)\n",
    "ax.set_ylim(-0.5, len(formats_vis) - 0.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.set_title(\"Floating-Point Bit Layouts: where the bits go\", fontsize=13, fontweight=\"bold\", pad=15)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#e74c3c\", edgecolor=\"white\", label=\"Sign (1 bit)\"),\n",
    "    Patch(facecolor=\"#3498db\", edgecolor=\"white\", label=\"Exponent (range)\"),\n",
    "    Patch(facecolor=\"#2ecc71\", edgecolor=\"white\", label=\"Mantissa (precision)\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"  - BF16 has the SAME exponent width as FP32 (8 bits) → same range → no underflow issues\")\n",
    "print(\"  - FP16 has a NARROWER exponent (5 bits) but MORE mantissa than BF16 → better precision, worse range\")\n",
    "print(\"  - TF32 combines FP32's exponent with FP16's mantissa width (10 bits) → internal to Tensor Cores\")\n",
    "print(\"  - FP8 formats have tiny mantissa → need per-tensor scaling to be usable\")\n",
    "print()\n",
    "print(\"* TF32 is 19 bits internally but is NOT a storage format. Tensor Cores use it transparently for FP32 matmuls.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision / Autocast Regimes (PyTorch) — Cheat Sheet\n",
    "\n",
    "This is the \"optimizer table\" equivalent for AMP: a small set of regimes that cover most real training setups.\n",
    "\n",
    "**Rule of thumb (training):**\n",
    "- If you have CUDA + BF16 support → use **BF16 autocast** (usually no GradScaler).\n",
    "- Else if you have CUDA → use **FP16 autocast + GradScaler**.\n",
    "- Avoid `model.half()` / \"everything FP16\" for training unless you're deliberately doing it (it removes autocast's safety policy and makes optimizer math low precision).\n",
    "\n",
    "| Regime (common name) | What you set | What runs in 16-bit | What stays FP32 (by policy) | Loss scaling | Typical outcome |\n",
    "|---|---|---|---|---|---|\n",
    "| **FP32 baseline** | model params FP32, autocast OFF | nothing | everything | no | stable, slower |\n",
    "| **FP32 + TF32 matmuls** | Ampere+ default unless disabled | matmuls use **TF32** internally | everything else FP32 | no | stable + faster, but matmul precision is ~FP16 mantissa |\n",
    "| **AMP BF16 (recommended if supported)** | model params FP32, `autocast(dtype=bf16)` | matmuls / linears / convs | softmax / layernorm / losses / big reductions | no | stable + fast (range like FP32) |\n",
    "| **AMP FP16 + GradScaler** | model params FP32, `autocast(dtype=fp16)` + `GradScaler` | matmuls / linears / convs | softmax / layernorm / losses / big reductions | **yes** | stable + fast (but FP16 gradients need scaling) |\n",
    "| **Naive BF16** | model params BF16, autocast OFF | everything | almost nothing | no | often \"works\", but sensitive ops can drift (reductions/normalization) |\n",
    "| **Naive FP16** | model params FP16, autocast OFF | everything | almost nothing | maybe (manual) | frequently unstable (underflow/overflow + update stagnation) |\n",
    "\n",
    "**Key idea:** autocast is useful even if you *could* run everything in BF16/FP16 — it keeps the numerically sensitive operations in FP32."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Suggest an AMP recipe for THIS machine\n",
    "\n",
    "def recommend_amp_recipe(dev: torch.device):\n",
    "    rec = {\"device\": dev.type}\n",
    "    if dev.type == \"cuda\":\n",
    "        bf16_ok = torch.cuda.is_bf16_supported()\n",
    "        rec[\"bf16_supported\"] = bool(bf16_ok)\n",
    "        if bf16_ok:\n",
    "            rec[\"recommended_autocast_dtype\"] = \"bfloat16\"\n",
    "            rec[\"use_grad_scaler\"] = False\n",
    "            rec[\"why\"] = \"BF16 has FP32-like exponent range → underflow is rare.\"\n",
    "        else:\n",
    "            rec[\"recommended_autocast_dtype\"] = \"float16\"\n",
    "            rec[\"use_grad_scaler\"] = True\n",
    "            rec[\"why\"] = \"FP16 has narrow exponent range → GradScaler rescues gradients from underflow.\"\n",
    "        rec[\"note\"] = \"Keep model parameters in FP32; let autocast choose per-op dtypes.\"\n",
    "    elif dev.type == \"cpu\":\n",
    "        rec[\"recommended_autocast_dtype\"] = \"bfloat16\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"CPU autocast supports BF16; speedups vary by CPU/kernel support.\"\n",
    "        rec[\"note\"] = \"CPU demos here are mostly about numerics, not performance.\"\n",
    "    elif dev.type == \"mps\":\n",
    "        rec[\"recommended_autocast_dtype\"] = \"float16\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"MPS typically uses FP16 for reduced precision.\"\n",
    "        rec[\"note\"] = \"Operator coverage differs from CUDA; verify dtype behavior with the probes in Section 3.\"\n",
    "    else:\n",
    "        rec[\"recommended_autocast_dtype\"] = \"float32\"\n",
    "        rec[\"use_grad_scaler\"] = False\n",
    "        rec[\"why\"] = \"Unknown device type.\"\n",
    "        rec[\"note\"] = \"\"\n",
    "    return rec\n",
    "\n",
    "display(pd.DataFrame([recommend_amp_recipe(device)]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 — Theory\n",
    "\n",
    "The core trick of autocast is simple to state:\n",
    "\n",
    "> **Run the *right* operations in lower precision for speed/memory, while keeping *numerically sensitive* operations in FP32.**\n",
    "\n",
    "But to understand *why* this works (and when it doesn't), we need to understand what floating-point formats can and cannot represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Where floating-point lives during training (and why autocast exists)\n",
    "\n",
    "A single training step can be decomposed into:\n",
    "\n",
    "1. **Forward**: parameters + activations → logits\n",
    "2. **Loss**: logits + targets → scalar loss\n",
    "3. **Backward**: loss → gradients for parameters\n",
    "4. **Optimizer update**: parameters + gradients (+ optimizer state) → new parameters\n",
    "\n",
    "Different tensors have different numeric requirements:\n",
    "\n",
    "| Tensor | Typical AMP dtype | Why |\n",
    "|---|---|---|\n",
    "| Activations / matmul results | FP16/BF16 (where safe) | Saves memory + uses Tensor Cores |\n",
    "| Softmax / LayerNorm stats / reductions | FP32 | Protects against overflow + rounding accumulation |\n",
    "| Gradients | Often FP32 *storage* (even if compute is mixed) | Stable updates + compatibility with optimizers |\n",
    "| Parameters (\"master weights\") | FP32 | Prevents update stagnation |\n",
    "| Optimizer state (Adam moments) | FP32 | Long-horizon accumulation is precision-sensitive |\n",
    "\n",
    "**Autocast's job** is mostly about **(1) and (2)**: choose per-op dtypes during the forward pass.\n",
    "\n",
    "**GradScaler's job** is mostly about **(3)** when FP16 is involved: keep gradients from underflowing to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Floating-point is \"range + precision\", not just \"more bits = better\"\n",
    "\n",
    "A binary floating-point number is roughly:\n",
    "\n",
    "$$(-1)^{\\text{sign}} \\times (1.\\text{mantissa}) \\times 2^{\\text{exponent}}$$\n",
    "\n",
    "The bit budget is split across:\n",
    "\n",
    "- **Exponent bits** → *range* (how large/small magnitudes you can represent)\n",
    "- **Mantissa (fraction) bits** → *precision* (how many significant bits you keep)\n",
    "\n",
    "For deep learning training, the key question is not \"can I store 3.14159?\" but:\n",
    "\n",
    "- Can I represent **tiny gradients** without them becoming 0 (underflow)?\n",
    "- Can I represent **large activations** without them becoming `inf` (overflow)?\n",
    "- Can I sum many numbers without destroying meaning via rounding?\n",
    "\n",
    "These failure modes show up differently in FP16 and BF16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 IEEE 754 anatomy (sign, exponent bias, hidden bit)\n",
    "\n",
    "A *normalized* binary floating-point value is encoded as:\n",
    "- **sign bit** $s$ (0 = positive, 1 = negative)\n",
    "- **exponent field** $E$ (stored with a **bias** so we can represent negative exponents)\n",
    "- **mantissa / fraction field** $m$\n",
    "\n",
    "For normalized numbers:\n",
    "\n",
    "$$\\text{value} = (-1)^s \\times (1 + m) \\times 2^{(E - \\text{bias})}$$\n",
    "\n",
    "Key details:\n",
    "- The leading `1.` is **implicit** (the \"hidden bit\"), giving you 1 extra bit of effective precision for free.\n",
    "- Exponent all-zeros and all-ones are **reserved**: `E=0` → subnormals / zero; `E=all ones` → `inf` / `nan`.\n",
    "- The **bias** is $2^{(\\text{exp\\_bits} - 1)} - 1$. For FP32: $127$. For FP16: $15$. For BF16: $127$.\n",
    "\n",
    "Let's decode $\\pi$ in all three formats to see this concretely."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bit-level decoding of pi across FP32, FP16, BF16\n",
    "\n",
    "def bits_f32(x: float) -> str:\n",
    "    (u32,) = struct.unpack(\">I\", struct.pack(\">f\", float(x)))\n",
    "    return f\"{u32:032b}\"\n",
    "\n",
    "def bits_f16(x: float) -> str:\n",
    "    u16 = np.frombuffer(np.float16(x).tobytes(), dtype=np.uint16)[0]\n",
    "    return f\"{int(u16):016b}\"\n",
    "\n",
    "def bits_bf16(x: float) -> str:\n",
    "    t = torch.tensor(float(x), dtype=torch.bfloat16)\n",
    "    i16 = int(t.view(torch.int16).item()) & 0xFFFF\n",
    "    return f\"{i16:016b}\"\n",
    "\n",
    "def decode_float(bits: str, exp_bits: int, mant_bits: int, bias: int):\n",
    "    s = int(bits[0], 2)\n",
    "    E = int(bits[1:1+exp_bits], 2)\n",
    "    M_bits = bits[1+exp_bits:]\n",
    "    assert len(M_bits) == mant_bits\n",
    "\n",
    "    if E == 0:\n",
    "        exp = 1 - bias\n",
    "        mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "        val = ((-1)**s) * mant * (2**exp)\n",
    "        return \"subnormal/zero\", s, E, exp, mant, val\n",
    "\n",
    "    if E == (2**exp_bits - 1):\n",
    "        return \"inf/nan\", s, E, None, None, None\n",
    "\n",
    "    exp = E - bias\n",
    "    mant = sum(int(b) * (2 ** (-(i+1))) for i, b in enumerate(M_bits))\n",
    "    val = ((-1)**s) * (1.0 + mant) * (2**exp)\n",
    "    return \"normal\", s, E, exp, mant, val\n",
    "\n",
    "x = math.pi\n",
    "rows = []\n",
    "\n",
    "for name, get_bits, eb, mb, bias in [\n",
    "    (\"float32\", bits_f32, 8, 23, 127),\n",
    "    (\"float16\", bits_f16, 5, 10, 15),\n",
    "    (\"bfloat16\", bits_bf16, 8, 7, 127),\n",
    "]:\n",
    "    b = get_bits(x)\n",
    "    kind, s, E, exp, mant, val = decode_float(b, eb, mb, bias)\n",
    "    rows.append({\n",
    "        \"dtype\": name,\n",
    "        \"bits\": f\"{b[:1]}|{b[1:1+eb]}|{b[1+eb:]}\",\n",
    "        \"sign\": s, \"E(stored)\": E, \"exponent\": exp,\n",
    "        \"1+mantissa\": round(1 + mant, 8) if mant is not None else None,\n",
    "        \"decoded\": round(val, 10) if val is not None else None,\n",
    "        \"error vs pi\": f\"{abs(val - math.pi):.2e}\" if val is not None else None,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(f\"\\nTrue pi = {math.pi}\")\n",
    "print(\"Notice: BF16 and FP16 both decode to 3.140625, but via different bit patterns.\")\n",
    "print(\"FP16 has more mantissa bits (10) giving finer precision; BF16 has fewer (7) but same exponent range as FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Why FP32 → BF16 conversion is trivial (but FP32 → FP16 is not)\n",
    "\n",
    "Since BF16 shares the same 8-bit exponent as FP32, converting FP32 to BF16 is just **truncating** (or rounding) the bottom 16 mantissa bits. The exponent field doesn't change, so no value can overflow or underflow during conversion.\n",
    "\n",
    "Converting FP32 to FP16, on the other hand, requires **narrowing the exponent** from 8 bits to 5 bits. This means:\n",
    "- FP32 values with exponents outside FP16's range (below $2^{-14}$ or above $2^{15}$) become 0 or `inf` during conversion.\n",
    "- The conversion itself can destroy values even before you do any computation.\n",
    "\n",
    "This is one reason BF16 is considered a \"drop-in\" replacement for FP32 in many training scenarios, while FP16 requires extra infrastructure (loss scaling, careful range management)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FP32 → BF16 vs FP32 → FP16 conversion: what survives?\n",
    "\n",
    "test_values = [3.14159, 1e-6, 1e-10, 1e-30, 1e-38, 1e30, 65504.0, 65536.0, 1e38]\n",
    "\n",
    "rows = []\n",
    "for v in test_values:\n",
    "    fp32 = torch.tensor(v, dtype=torch.float32)\n",
    "    bf16 = fp32.to(torch.bfloat16)\n",
    "    fp16 = fp32.to(torch.float16)\n",
    "    rows.append({\n",
    "        \"FP32 value\": f\"{v:.2e}\",\n",
    "        \"→ BF16\": f\"{float(bf16):.4e}\",\n",
    "        \"BF16 survived?\": \"inf/0\" if not torch.isfinite(bf16) or float(bf16) == 0 and v != 0 else \"YES\",\n",
    "        \"BF16 rel error\": f\"{abs(float(bf16) - v) / (abs(v) + 1e-45):.2e}\" if torch.isfinite(bf16) and float(bf16) != 0 else \"-\",\n",
    "        \"→ FP16\": f\"{float(fp16):.4e}\",\n",
    "        \"FP16 survived?\": \"inf\" if torch.isinf(fp16) else (\"0 (underflow)\" if float(fp16) == 0 and v != 0 else \"YES\"),\n",
    "        \"FP16 rel error\": f\"{abs(float(fp16) - v) / (abs(v) + 1e-45):.2e}\" if torch.isfinite(fp16) and float(fp16) != 0 else \"-\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey takeaway:\")\n",
    "print(\"  BF16 preserves ALL magnitudes (same exponent range as FP32) — just loses some decimal precision.\")\n",
    "print(\"  FP16 DESTROYS values outside its narrow range: 1e-10 underflows to 0, 65536 overflows to inf.\")\n",
    "print(\"  This is why BF16 conversion is 'just truncate the mantissa' — safe and trivial.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Normal vs subnormal numbers (and \"flush-to-zero\")\n",
    "\n",
    "**Subnormals** (also called denormals) extend the representable range closer to 0 by giving up the implicit leading `1.`:\n",
    "\n",
    "$$\\text{subnormal value} = (-1)^s \\times (0.\\text{mantissa}) \\times 2^{(1 - \\text{bias})}$$\n",
    "\n",
    "They matter because **gradients can be very small**. But subnormals can be slow on some hardware, so many compute paths enable **FTZ/DAZ** (\"flush-to-zero\" / \"denormals-are-zero\"), which means extremely small values become exactly 0.\n",
    "\n",
    "**Practical lesson:** It is not enough to know the *spec* of a dtype. You also need to know what your hardware/kernel path does with subnormals.\n",
    "\n",
    "Let's probe whether the smallest subnormal survives on your device."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Subnormal survival probe\n",
    "\n",
    "def subnormal_survives(dtype, dev):\n",
    "    z = torch.tensor(0.0, dtype=dtype, device=dev)\n",
    "    o = torch.tensor(1.0, dtype=dtype, device=dev)\n",
    "    sub = torch.nextafter(z, o)\n",
    "    return {\n",
    "        \"dtype\": str(dtype), \"device\": dev.type,\n",
    "        \"nextafter(0,1)\": f\"{float(sub):.6e}\",\n",
    "        \"is_zero\": bool((sub == 0).item()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    try:\n",
    "        rows.append(subnormal_survives(dt, device))\n",
    "    except Exception as e:\n",
    "        rows.append({\"dtype\": str(dt), \"device\": device.type, \"error\": type(e).__name__})\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 The measuring tape analogy\n",
    "\n",
    "A good way to think about floating-point formats is as a **measuring tape**:\n",
    "- The **length** of the tape is determined by the exponent bits (range: how big/small you can measure).\n",
    "- The **fineness of the tick marks** is determined by the mantissa bits (precision: how closely you can read off a value).\n",
    "\n",
    "FP32 is a long tape with fine tick marks. BF16 is equally long but with coarser tick marks. FP16 is a much shorter tape with tick marks finer than BF16 but coarser than FP32.\n",
    "\n",
    "For training, **tape length (range) matters more than tick mark fineness (precision)** — because a gradient that falls *off the tape entirely* (underflow to zero) provides zero learning signal, while a gradient that lands *between tick marks* (rounding) still provides a useful approximate signal. SGD is inherently noisy; it tolerates imprecise gradients, but it cannot learn from absent ones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The measuring tape: visualize range and precision as tape length vs tick density\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 5), gridspec_kw={\"hspace\": 0.6})\n",
    "\n",
    "tape_configs = [\n",
    "    (\"FP32 (8-bit exp, 23-bit mantissa)\", torch.float32, \"#2ecc71\", -126, 127, 23),\n",
    "    (\"BF16 (8-bit exp, 7-bit mantissa)\",  torch.bfloat16, \"#3498db\", -126, 127, 7),\n",
    "    (\"FP16 (5-bit exp, 10-bit mantissa)\", torch.float16, \"#e74c3c\", -14, 15, 10),\n",
    "]\n",
    "\n",
    "for ax, (label, dt, color, exp_min, exp_max, mant_bits) in zip(axes, tape_configs):\n",
    "    # Draw the tape as a colored bar representing the exponent range\n",
    "    tape_left = exp_min\n",
    "    tape_right = exp_max\n",
    "    full_range = 127 - (-126)  # FP32 range for normalization\n",
    "\n",
    "    # Normalize to common axis\n",
    "    ax.barh(0, tape_right - tape_left, left=tape_left, height=0.4,\n",
    "            color=color, alpha=0.3, edgecolor=color, linewidth=2)\n",
    "\n",
    "    # Draw tick marks proportional to mantissa precision\n",
    "    # More mantissa bits = more ticks (denser)\n",
    "    n_ticks = min(2 ** mant_bits, 200)  # cap for visualization\n",
    "    tick_positions = np.linspace(tape_left, tape_right, n_ticks)\n",
    "    for tp in tick_positions:\n",
    "        ax.plot([tp, tp], [-0.15, 0.15], color=color, linewidth=0.3, alpha=0.6)\n",
    "\n",
    "    # Labels\n",
    "    ax.text(tape_left - 1, 0, label, ha=\"right\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "    ax.text((tape_left + tape_right) / 2, -0.35,\n",
    "            f\"Range: 2^{exp_min} to 2^{exp_max}  |  Precision: {2**mant_bits} levels per power-of-2\",\n",
    "            ha=\"center\", va=\"top\", fontsize=8, color=\"gray\")\n",
    "\n",
    "    ax.set_xlim(-140, 135)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.set_xlabel(\"exponent (log2 scale)\" if ax == axes[-1] else \"\")\n",
    "\n",
    "fig.suptitle(\"The Measuring Tape: Range (tape length) vs Precision (tick density)\", fontsize=12, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  FP32 and BF16 have the SAME tape length (range) — they can measure the same extremes.\")\n",
    "print(\"  FP16 has a MUCH SHORTER tape — values beyond 2^15 ≈ 65504 overflow to infinity,\")\n",
    "print(\"  and values below 2^-14 ≈ 6e-5 underflow to zero.\")\n",
    "print()\n",
    "print(\"  But BF16's tick marks are 8x coarser than FP16's (128 vs 1024 levels per interval).\")\n",
    "print(\"  For training, this tradeoff overwhelmingly favors BF16: coarse ticks = noise, short tape = death.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 ULP: spacing grows with magnitude\n",
    "\n",
    "A float format has *roughly constant relative precision* but *variable absolute precision*.\n",
    "\n",
    "- Near 1.0, FP16 spacing is ~$10^{-3}$.\n",
    "- Near 1024, FP16 spacing is ~$1$.\n",
    "\n",
    "This is the concrete reason \"tiny updates disappear\" when weights are stored in low precision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ULP vs magnitude for each dtype\n",
    "\n",
    "def ulp(x: torch.Tensor, dtype: torch.dtype):\n",
    "    x = x.to(dtype)\n",
    "    return (torch.nextafter(x, x * 2) - x).abs().to(torch.float32)\n",
    "\n",
    "ks = torch.arange(-10, 21, device=device)\n",
    "x = (2.0 ** ks).to(torch.float32)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, color in [(torch.float16, \"C0\"), (torch.bfloat16, \"C1\"), (torch.float32, \"C2\")]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    u = ulp(x, dt).cpu().numpy()\n",
    "    plt.plot(ks.cpu().numpy(), np.log2(u + 1e-45), marker=\"o\", markersize=4, label=str(dt), color=color)\n",
    "\n",
    "plt.title(\"ULP (spacing between adjacent floats) vs magnitude\")\n",
    "plt.xlabel(\"log2(|x|)\")\n",
    "plt.ylabel(\"log2(ULP(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 Number line: where representable floats actually live\n",
    "\n",
    "The spacing between representable numbers is *not uniform* — it depends on the magnitude. Near zero, floats are dense; as magnitude grows, they spread apart. And crucially, **different formats have different densities at every scale**.\n",
    "\n",
    "This visualization plots the actual representable numbers in each format within a small interval. Think of it as zooming into the \"measuring tape\" to see the tick marks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number line: representable floats in [1.0, 2.0) for each dtype\n",
    "# This interval is illuminating because ULP is constant within a power-of-2 interval\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 6), sharex=True)\n",
    "\n",
    "for ax, (dt, label, color) in zip(axes, [\n",
    "    (torch.float32, \"FP32 (23-bit mantissa: 8,388,608 values in [1,2))\", \"C2\"),\n",
    "    (torch.bfloat16, \"BF16 (7-bit mantissa: 128 values in [1,2))\", \"C1\"),\n",
    "    (torch.float16, \"FP16 (10-bit mantissa: 1,024 values in [1,2))\", \"C0\"),\n",
    "    (None, \"Comparison overlay\", \"k\"),\n",
    "]):\n",
    "    if dt is not None:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        two = torch.tensor(2.0, dtype=dt)\n",
    "        vals = [float(one)]\n",
    "        cur = one\n",
    "        while True:\n",
    "            cur = torch.nextafter(cur, two)\n",
    "            if float(cur) >= 2.0:\n",
    "                break\n",
    "            vals.append(float(cur))\n",
    "            if len(vals) > 2000:\n",
    "                break\n",
    "        vals = np.array(vals)\n",
    "        ax.eventplot([vals], lineoffsets=0, linelengths=0.6, colors=color, linewidths=0.5)\n",
    "        ax.set_ylabel(label, fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.text(1.0, 0.35, f\"{len(vals)} representable values\", fontsize=8, color=color)\n",
    "    else:\n",
    "        # Overlay: show a narrow window [1.0, 1.02] with all three\n",
    "        for dt2, c2, yoff in [(torch.float16, \"C0\", 0.3), (torch.bfloat16, \"C1\", 0.0), (torch.float32, \"C2\", -0.3)]:\n",
    "            one2 = torch.tensor(1.0, dtype=dt2)\n",
    "            limit = torch.tensor(1.02, dtype=dt2)\n",
    "            vs = [float(one2)]\n",
    "            cur2 = one2\n",
    "            for _ in range(200):\n",
    "                cur2 = torch.nextafter(cur2, limit)\n",
    "                if float(cur2) >= 1.02:\n",
    "                    break\n",
    "                vs.append(float(cur2))\n",
    "            vs = np.array(vs)\n",
    "            ax.eventplot([vs], lineoffsets=yoff, linelengths=0.25, colors=c2, linewidths=1.0)\n",
    "        ax.set_ylabel(\"Zoomed [1.0, 1.02]\", fontsize=8)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim(1.0, 1.02)\n",
    "        ax.legend([\"FP16\", \"BF16\", \"FP32\"], fontsize=7, loc=\"upper right\")\n",
    "\n",
    "axes[0].set_xlim(1.0, 2.0)\n",
    "for ax in axes[:3]:\n",
    "    ax.set_xlim(1.0, 2.0)\n",
    "axes[-1].set_xlim(1.0, 1.02)\n",
    "axes[-1].set_xlabel(\"value\")\n",
    "fig.suptitle(\"Representable floats in [1.0, 2.0): density depends on mantissa bits\", fontsize=11, y=1.01)\n",
    "plt.tight_layout();\n",
    "print(\"Key insight: BF16 has ~8x fewer representable values than FP16 in [1,2),\")\n",
    "print(\"but FP16 has ~8,192x fewer than FP32. This is the precision-range tradeoff made visible.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 The bit-level addition trap (why `1 + 1e-4 = 1` in FP16)\n",
    "\n",
    "This is the single most important numeric fact for understanding **weight update stagnation**.\n",
    "\n",
    "When adding two floating-point numbers, the hardware must **align exponents** by shifting the smaller number's mantissa to the right. If the shift pushes all significant bits past the mantissa width, the smaller number is effectively lost.\n",
    "\n",
    "**Concrete example (from the FP16 bit-level):**\n",
    "\n",
    "- `1.0` in FP16: exponent = $2^0$, mantissa = all zeros.\n",
    "- `1e-4` in FP16: exponent = $2^{-14}$, mantissa encodes ~1.639.\n",
    "- To add them, we must shift `1e-4`'s mantissa by **14 positions** to align with `1.0`'s exponent.\n",
    "- FP16 has only **10 mantissa bits**. After shifting 14 positions right, *all* significant bits fall off the edge.\n",
    "- Result: `1.0 + 1e-4 = 1.0` exactly.\n",
    "\n",
    "This is exactly what happens during training: if `learning_rate * gradient` is smaller than the ULP at the weight's magnitude, the weight **never changes**.\n",
    "\n",
    "FP16's epsilon is ~$9.77 \\times 10^{-4}$. Any update smaller than this relative to the weight magnitude is silently dropped."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate the addition trap across dtypes\n",
    "\n",
    "print(\"Does 1.0 + delta produce a value > 1.0?\\n\")\n",
    "\n",
    "deltas = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "rows = []\n",
    "for delta in deltas:\n",
    "    row = {\"delta\": f\"{delta:.0e}\"}\n",
    "    for name, dt in [(\"FP16\", torch.float16), (\"BF16\", torch.bfloat16), (\"FP32\", torch.float32)]:\n",
    "        one = torch.tensor(1.0, dtype=dt)\n",
    "        d = torch.tensor(delta, dtype=dt)\n",
    "        result = one + d\n",
    "        changed = float(result) != float(one)\n",
    "        row[name] = \"YES\" if changed else \"no (lost!)\"\n",
    "    rows.append(row)\n",
    "\n",
    "df_add = pd.DataFrame(rows)\n",
    "display(df_add)\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"- FP16 loses updates smaller than ~1e-3 relative to weight magnitude.\")\n",
    "print(\"- BF16 loses updates smaller than ~8e-3 (even coarser!).\")\n",
    "print(\"- FP32 loses updates smaller than ~1e-7.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights: typical lr*grad products\")\n",
    "print(\"are often 1e-5 to 1e-7, which FP16 and BF16 both silently discard.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bit-level mechanics: *why* `1.0 + 1e-4 = 1.0` in FP16\n",
    "\n",
    "The table above shows *that* small updates get lost. Let's see *why* at the bit level.\n",
    "\n",
    "When hardware adds two floats, it must **align their exponents** by right-shifting the smaller number's mantissa. If the shift exceeds the mantissa width, the smaller number's bits fall off completely.\n",
    "\n",
    "This is the exact same mechanism that causes **weight update stagnation** during training: `weight += lr * gradient` produces the same weight if `lr * gradient` is too small relative to the weight's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step-by-step bit alignment: why 1.0 + 1e-4 = 1.0 in FP16\n",
    "\n",
    "print(\"=== Bit-level addition in FP16: 1.0 + 1e-4 ===\\n\")\n",
    "\n",
    "# 1.0 in FP16\n",
    "one_f16 = np.float16(1.0)\n",
    "one_bits = f\"{int(np.frombuffer(one_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "print(f\"1.0 in FP16:   {one_bits[0]}|{one_bits[1:6]}|{one_bits[6:]}\")\n",
    "print(f\"               s| exp  | mantissa\")\n",
    "print(f\"               = (-1)^0 × 1.0000000000 × 2^(15 - 15) = 1.0\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 1e-4 in FP16\n",
    "small_f16 = np.float16(1e-4)\n",
    "small_bits = f\"{int(np.frombuffer(small_f16.tobytes(), dtype=np.uint16)[0]):016b}\"\n",
    "E_small = int(small_bits[1:6], 2)\n",
    "print(f\"1e-4 in FP16:  {small_bits[0]}|{small_bits[1:6]}|{small_bits[6:]}\")\n",
    "print(f\"               = (-1)^0 × 1.{small_bits[6:]} × 2^({E_small} - 15) = 2^{{{E_small - 15}}}\")\n",
    "print(f\"               ≈ {float(small_f16):.6e}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Addition step: align exponents ---\")\n",
    "print()\n",
    "shift = 15 - E_small\n",
    "print(f\"To add these, we align the smaller exponent ({E_small - 15}) to the larger (0).\")\n",
    "print(f\"This means shifting 1e-4's mantissa RIGHT by {shift} positions.\")\n",
    "print()\n",
    "print(f\"  1.0:     1.{'0' * 10}         (exponent = 0)\")\n",
    "print(f\"+ 1e-4:    0.{'0' * (shift - 1)}1{'?' * max(0, 10 - shift)}   (shifted {shift} positions right)\")\n",
    "print()\n",
    "print(f\"FP16 mantissa is only 10 bits wide.\")\n",
    "print(f\"After shifting right by {shift}, ALL significant bits of 1e-4 are\")\n",
    "print(f\"beyond the 10-bit mantissa boundary → they are discarded.\")\n",
    "print()\n",
    "print(f\"Result: 1.0 + 1e-4 = 1.0  (the small value vanished completely)\")\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "result = np.float16(1.0) + np.float16(1e-4)\n",
    "eps_f16 = np.finfo(np.float16).eps\n",
    "print(f\"Verification:  np.float16(1.0) + np.float16(1e-4) = {result}\")\n",
    "print(f\"FP16 epsilon:  {eps_f16:.4e}\")\n",
    "print(f\"1e-4 < eps?    {1e-4 < eps_f16}  → update is below FP16's resolution at magnitude 1.0\")\n",
    "print()\n",
    "print(\"Training implication: if weight ≈ 1.0 and lr × grad ≈ 1e-4,\")\n",
    "print(\"the weight NEVER changes in FP16. This is weight update stagnation.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 Epsilon meets training: where do real weight updates fall?\n",
    "\n",
    "The addition trap is not theoretical — it directly determines whether training succeeds. Let's connect the abstract epsilon values to concrete training scenarios.\n",
    "\n",
    "For a weight $w$ stored in a given dtype, the **minimum detectable update** is approximately $|w| \\times \\epsilon$. If $\\text{lr} \\times |\\text{grad}|$ is smaller than this, the weight never changes.\n",
    "\n",
    "Typical training setups use learning rates of $10^{-3}$ to $10^{-5}$, and gradient magnitudes often range from $10^{-2}$ to $10^{-6}$. Their product ($\\text{lr} \\times |\\text{grad}|$) is what the format must be able to represent *relative to the weight magnitude*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Where do typical lr × grad products fall relative to epsilon boundaries?\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Epsilon values (minimum detectable relative update)\n",
    "epsilons = {\n",
    "    \"FP16\": float(torch.finfo(torch.float16).eps),\n",
    "    \"BF16\": float(torch.finfo(torch.bfloat16).eps),\n",
    "    \"FP32\": float(torch.finfo(torch.float32).eps),\n",
    "}\n",
    "\n",
    "# Typical lr × |grad| magnitudes in real training\n",
    "typical_updates = {\n",
    "    \"lr=1e-3 × |g|=1e-1\\n(early training, large grads)\": 1e-4,\n",
    "    \"lr=1e-3 × |g|=1e-3\\n(mid training)\": 1e-6,\n",
    "    \"lr=1e-4 × |g|=1e-3\\n(fine-tuning)\": 1e-7,\n",
    "    \"lr=1e-4 × |g|=1e-5\\n(late training, small grads)\": 1e-9,\n",
    "    \"lr=1e-5 × |g|=1e-5\\n(LLM fine-tune)\": 1e-10,\n",
    "}\n",
    "\n",
    "y_pos = 0\n",
    "colors_eps = {\"FP16\": \"#e74c3c\", \"BF16\": \"#3498db\", \"FP32\": \"#2ecc71\"}\n",
    "\n",
    "# Draw epsilon boundaries as vertical lines\n",
    "for name, eps in epsilons.items():\n",
    "    ax.axvline(np.log10(eps), color=colors_eps[name], linewidth=3, alpha=0.7,\n",
    "               label=f\"{name} epsilon = {eps:.1e}\")\n",
    "\n",
    "# Shade the \"safe update\" zone (above all epsilons)\n",
    "ax.axvspan(np.log10(max(epsilons.values())), 0, alpha=0.05, color=\"green\")\n",
    "\n",
    "# Draw typical update magnitudes as horizontal markers\n",
    "y_updates = np.linspace(0.9, 0.1, len(typical_updates))\n",
    "for (label, mag), y in zip(typical_updates.items(), y_updates):\n",
    "    marker_color = \"green\"\n",
    "    if mag < epsilons[\"FP16\"]:\n",
    "        marker_color = \"red\"\n",
    "    elif mag < epsilons[\"BF16\"]:\n",
    "        marker_color = \"orange\"\n",
    "    ax.plot(np.log10(mag), y, \"D\", color=marker_color, markersize=10, zorder=5)\n",
    "    ax.text(np.log10(mag) + 0.15, y, label, fontsize=7, va=\"center\", color=marker_color)\n",
    "\n",
    "# Annotations\n",
    "ax.text(np.log10(epsilons[\"FP16\"]) - 0.1, 0.95,\n",
    "        \"← Updates here are LOST\\n    in FP16 (weight never changes)\",\n",
    "        fontsize=8, color=\"#e74c3c\", ha=\"right\", va=\"top\", fontstyle=\"italic\")\n",
    "ax.text(np.log10(epsilons[\"BF16\"]) - 0.1, 0.5,\n",
    "        \"← Also lost in BF16\",\n",
    "        fontsize=8, color=\"#3498db\", ha=\"right\", va=\"center\", fontstyle=\"italic\")\n",
    "\n",
    "ax.set_xlim(-12, 0.5)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "ax.set_xlabel(\"log10(lr × |gradient|)  — relative to weight magnitude of ~1.0\", fontsize=10)\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Where do real weight updates fall relative to format epsilon?\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Green diamonds: update is large enough for ALL formats\")\n",
    "print(\"  - Orange diamonds: update works in FP32 and FP16, but NOT in BF16\")\n",
    "print(\"  - Red diamonds: update only works in FP32\")\n",
    "print()\n",
    "print(\"This is why FP32 master weights are essential: the optimizer applies updates in FP32\")\n",
    "print(\"(where they're captured), then casts back to 16-bit for the next forward pass.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 FP16 vs BF16 vs FP32: the complete numeric comparison\n",
    "\n",
    "We generated the cheat sheet above. Here we dig deeper into what those numbers mean for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed format facts table\n",
    "\n",
    "def _ulp_at_one_v2(dtype):\n",
    "    try:\n",
    "        one = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(one, one + one) - one)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _smallest_subnormal_v2(dtype):\n",
    "    try:\n",
    "        z = torch.tensor(0.0, dtype=dtype)\n",
    "        o = torch.tensor(1.0, dtype=dtype)\n",
    "        return float(torch.nextafter(z, o))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dtype_row(name, dtype, exp_bits, mant_bits, exp_min, exp_max):\n",
    "    fi = torch.finfo(dtype)\n",
    "    ulp1 = _ulp_at_one_v2(dtype)\n",
    "    sub = _smallest_subnormal_v2(dtype)\n",
    "    return {\n",
    "        \"dtype\": name,\n",
    "        \"bits\": fi.bits,\n",
    "        \"exp_bits\": exp_bits,\n",
    "        \"mant_bits\": mant_bits,\n",
    "        \"precision_bits\": mant_bits + 1,\n",
    "        \"decimal_digits\": round((mant_bits + 1) * math.log10(2), 2),\n",
    "        \"exp_range\": f\"[{exp_min}, {exp_max}]\",\n",
    "        \"epsilon\": f\"{fi.eps:.2e}\",\n",
    "        \"ulp(1.0)\": f\"{ulp1:.2e}\" if ulp1 is not None else \"n/a\",\n",
    "        \"min_normal\": f\"{fi.tiny:.2e}\",\n",
    "        \"min_subnormal\": f\"{sub:.2e}\" if sub is not None else \"n/a\",\n",
    "        \"max_finite\": f\"{fi.max:.2e}\",\n",
    "    }\n",
    "\n",
    "dtype_info = pd.DataFrame([\n",
    "    *( [dtype_row(\"float8_e4m3fn\", torch.float8_e4m3fn, 4, 3, -6, 7)] if hasattr(torch, \"float8_e4m3fn\") else [] ),\n",
    "    *( [dtype_row(\"float8_e5m2\", torch.float8_e5m2, 5, 2, -14, 15)] if hasattr(torch, \"float8_e5m2\") else [] ),\n",
    "    dtype_row(\"float16\", torch.float16, 5, 10, -14, 15),\n",
    "    dtype_row(\"bfloat16\", torch.bfloat16, 8, 7, -126, 127),\n",
    "    dtype_row(\"float32\", torch.float32, 8, 23, -126, 127),\n",
    "    dtype_row(\"float64\", torch.float64, 11, 52, -1022, 1023),\n",
    "]).set_index(\"dtype\")\n",
    "\n",
    "display(dtype_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the table\n",
    "\n",
    "**Precision** (how fine the tick marks are):\n",
    "- FP64: ULP at 1.0 is ~2e-16. This is essentially \"reference precision\" for most deep learning numerics.\n",
    "- FP32: ULP at 1.0 is ~$1.2 \\times 10^{-7}$. Updates as small as $10^{-7}$ are captured.\n",
    "- FP16: ULP at 1.0 is ~$9.8 \\times 10^{-4}$. Updates smaller than $10^{-3}$ are lost.\n",
    "- BF16: ULP at 1.0 is ~$7.8 \\times 10^{-3}$. Updates smaller than $10^{-2}$ are lost. Even coarser than FP16!\n",
    "- FP8: ULP at 1.0 is huge. FP8 is *not* a drop-in training dtype without extra scaling strategies and specialized kernels.\n",
    "\n",
    "**Range** (how long the measuring tape is):\n",
    "- FP16: smallest normal is ~$6 \\times 10^{-5}$. Gradients below this become zero.\n",
    "- BF16: smallest normal is ~$1.2 \\times 10^{-38}$, same as FP32. Gradients essentially never underflow.\n",
    "- FP8: range depends on format (E4M3 vs E5M2), but is far smaller than FP16/BF16/FP32.\n",
    "- This is why **FP16 needs loss scaling** but **BF16 usually does not**.\n",
    "\n",
    "### 1.2.1 TF32 — the hidden precision mode\n",
    "\n",
    "On NVIDIA Ampere+ GPUs, FP32 matmuls can automatically use **TF32** internally:\n",
    "- Same 8-bit exponent as FP32 (full range)\n",
    "- But only 10 bits of mantissa (same as FP16 precision)\n",
    "- Transparent: your code says `float32`, but Tensor Cores use TF32 for speed\n",
    "\n",
    "This means your \"FP32 baseline\" on modern GPUs may actually be **TF32 precision** for matmuls. When comparing FP32 vs AMP, be aware of this hidden variable. You can control it with `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Why autocast targets matmul/linear first: FP32 accumulation\n",
    "\n",
    "Deep learning is dominated by large matrix multiplications (GEMMs): `Linear`, attention projections, and MLPs.\n",
    "\n",
    "On modern accelerators, these kernels typically:\n",
    "- **multiply** in FP16/BF16 (or TF32/FP8, depending on mode)\n",
    "- **accumulate** partial sums in **FP32**\n",
    "\n",
    "This is a sweet spot:\n",
    "- massive speedup (Tensor Cores / specialized units)\n",
    "- much better numeric behavior than \"pure FP16 accumulation\"\n",
    "\n",
    "Autocast heavily leans on this: it prefers to cast matmul-like ops down because they are both *fast* and comparatively *stable* (relative to softmax, layernorm, exp/log, large reductions)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Matmul accuracy across dtypes (vs FP64 reference)\n",
    "\n",
    "M = 256 if device.type != \"cuda\" else 512\n",
    "a = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "b = torch.randn(M, M, device=device, dtype=torch.float32)\n",
    "\n",
    "ref = (a.double() @ b.double()).float()\n",
    "\n",
    "def _err_stats(c, ref):\n",
    "    c = c.float()\n",
    "    abs_err = (c - ref).abs()\n",
    "    rel_err = abs_err / (ref.abs() + 1e-6)\n",
    "    return abs_err, rel_err\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    try:\n",
    "        c_dt = a.to(dt) @ b.to(dt)\n",
    "        abs_err, rel_err = _err_stats(c_dt, ref)\n",
    "        rows.append({\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": str(c_dt.dtype).replace(\"torch.\", \"\"),\n",
    "            \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "            \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "            \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "            \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "            \"note\": \"\",\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": \"-\",\n",
    "            \"max_abs_err\": \"-\",\n",
    "            \"mean_abs_err\": \"-\",\n",
    "            \"max_rel_err\": \"-\",\n",
    "            \"mean_rel_err\": \"-\",\n",
    "            \"note\": f\"matmul failed ({type(e).__name__})\",\n",
    "        })\n",
    "\n",
    "# FP32: on CUDA this may be strict FP32 or TF32 depending on allow_tf32.\n",
    "if supports_dtype_on_device(torch.float32, device):\n",
    "    if device.type == \"cuda\":\n",
    "        orig_tf32 = torch.backends.cuda.matmul.allow_tf32\n",
    "        try:\n",
    "            for allow in [False, True]:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = allow\n",
    "                c = a @ b\n",
    "                abs_err, rel_err = _err_stats(c, ref)\n",
    "                rows.append({\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"matmul_mode\": \"strict_fp32\" if not allow else \"tf32_allowed\",\n",
    "                    \"matmul_output_dtype\": str(c.dtype).replace(\"torch.\", \"\"),\n",
    "                    \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "                    \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "                    \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "                    \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "                    \"note\": \"\",\n",
    "                })\n",
    "        finally:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = orig_tf32\n",
    "    else:\n",
    "        c = a @ b\n",
    "        abs_err, rel_err = _err_stats(c, ref)\n",
    "        rows.append({\n",
    "            \"dtype\": \"float32\",\n",
    "            \"matmul_mode\": \"native\",\n",
    "            \"matmul_output_dtype\": str(c.dtype).replace(\"torch.\", \"\"),\n",
    "            \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "            \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "            \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "            \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "            \"note\": \"\",\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- Errors come from (1) input rounding to dt and (2) output rounding back to dt.\")\n",
    "print(\"- On CUDA, FP16/BF16 matmuls usually accumulate in FP32 internally, which helps stability.\")\n",
    "print(\"- On Ampere+ CUDA GPUs, float32 matmuls may run in TF32 mode when allow_tf32=True (10 mantissa bits).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 The three numeric disasters that show up during training\n",
    "\n",
    "### (A) Underflow — values become 0\n",
    "\n",
    "- Common in **gradients**, especially late in training or in deep nets with tiny signals.\n",
    "- Most harmful in **FP16** due to narrow exponent range (5 bits → min normal $\\approx 6 \\times 10^{-5}$).\n",
    "- BF16 has the same exponent range as FP32, so underflow is rare.\n",
    "\n",
    "### (B) Overflow — values become `inf`\n",
    "\n",
    "- Common in **activations** (exponentials, attention logits) or badly-initialized models.\n",
    "- FP16 max is only ~65,504. Easy to exceed.\n",
    "- BF16 max is ~$3.4 \\times 10^{38}$, same as FP32.\n",
    "\n",
    "### (C) Accumulation / cancellation error\n",
    "\n",
    "Even when values are in range, precision limits corrupt sums and products:\n",
    "- Adding many small numbers to a large accumulator can lose the small contributions (same mechanism as the `1 + 1e-4` trap).\n",
    "- For reductions (layernorm statistics, softmax normalization, large sums), frameworks often keep accumulation in FP32.\n",
    "\n",
    "**Autocast** is partly about preventing A/B (range disasters), and partly about routing sensitive reductions so C doesn't destroy training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0 Accumulation error, made concrete\n",
    "\n",
    "**Accumulation error** is just \"rounding happens at every add/multiply, and it compounds\".\n",
    "\n",
    "The simplest microscope is:\n",
    "\n",
    "> start at a value (like 1.0), add a tiny increment many times, and see when the increment stops \"counting\".\n",
    "\n",
    "This is the same mechanism behind:\n",
    "- weight-update stagnation (updates below ULP get rounded away)\n",
    "- instability in reductions (sums/means/variances done in low precision)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Accumulation microscope: 1.0 + N * delta, computed sequentially in different dtypes\n",
    "\n",
    "N = 20000\n",
    "base = 1.0\n",
    "delta = 1e-3\n",
    "\n",
    "steps_i = torch.arange(N + 1, device=device, dtype=torch.int64)\n",
    "expected64 = base + delta * steps_i.double()\n",
    "\n",
    "rows = []\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "every = max(1, (N + 1) // 800)\n",
    "\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32, torch.float64]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    deltas = torch.full((N,), delta, device=device, dtype=dt)\n",
    "    start = torch.tensor([base], device=device, dtype=dt)\n",
    "    cs = torch.cat([start, deltas]).cumsum(0)\n",
    "    err = (cs.double() - expected64).cpu().numpy()\n",
    "\n",
    "    label = str(dt).replace(\"torch.\", \"\")\n",
    "    plt.plot(steps_i.cpu().numpy()[::every], err[::every], label=label, alpha=0.85)\n",
    "\n",
    "    final = float(cs[-1].cpu())\n",
    "    rows.append({\n",
    "        \"dtype\": label,\n",
    "        \"final\": f\"{final:.6f}\",\n",
    "        \"expected\": f\"{float(expected64[-1].cpu()):.6f}\",\n",
    "        \"abs_err\": f\"{abs(final - float(expected64[-1].cpu())):.3e}\",\n",
    "    })\n",
    "\n",
    "plt.axhline(0.0, color=\"k\", lw=1, alpha=0.3)\n",
    "plt.title(\"Accumulation error: sequentially adding delta many times\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cumsum(dtype) - reference (float64 arithmetic)\")\n",
    "plt.legend()\n",
    "plt.tight_layout();\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- If the error curve flattens, your increments stopped affecting the accumulator.\")\n",
    "print(\"- This is why AMP promotes certain reductions (like sum/prod and norm stats) to FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LayerNorm-like statistics are especially sensitive:\n",
    "# mean/var of (large offset + small noise) is a classic cancellation problem.\n",
    "\n",
    "M = 200_000\n",
    "x = (torch.randn(M, device=device, dtype=torch.float32) * 0.1) + 1000.0\n",
    "\n",
    "ref_mu = float(x.double().mean())\n",
    "ref_var = float(((x.double() - ref_mu) ** 2).mean())\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    xd = x.to(dt)\n",
    "    mu = xd.mean()\n",
    "    var = ((xd - mu) ** 2).mean()\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"mean\": f\"{float(mu):.6f}\",\n",
    "        \"var\": f\"{float(var):.6e}\",\n",
    "        \"mean_abs_err\": f\"{abs(float(mu) - ref_mu):.3e}\",\n",
    "        \"var_rel_err\": f\"{abs(float(var) - ref_var) / (ref_var + 1e-30):.3e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(\"\\nThis is a simplified version of why LayerNorm/Softmax reductions are often forced to FP32 under autocast.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0a Why floating-point addition is NOT associative\n",
    "\n",
    "In real-number math, addition is **associative**: $(a + b) + c = a + (b + c)$. In floating-point, this identity breaks because **rounding happens after every operation**.\n",
    "\n",
    "This has a direct practical consequence: **the order in which you sum values affects the result**. When autocast forces reductions to FP32, one reason is this: the same sum computed in FP16 vs FP32 gives different answers because rounding error accumulates differently with fewer mantissa bits.\n",
    "\n",
    "This also means **nondeterministic reduction order** (common in parallel GPU kernels) can cause run-to-run variance in low precision. Knowing this helps debug \"my loss is slightly different every run\" issues."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Non-associativity of floating-point addition\n",
    "\n",
    "# Choose three values where grouping matters\n",
    "a_val, b_val, c_val = 1.0, 1e-4, -1.0\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32, torch.float64]:\n",
    "    a = torch.tensor(a_val, dtype=dt)\n",
    "    b = torch.tensor(b_val, dtype=dt)\n",
    "    c = torch.tensor(c_val, dtype=dt)\n",
    "\n",
    "    lhs = (a + b) + c      # (1 + 1e-4) + (-1)\n",
    "    rhs = a + (b + c)       # 1 + (1e-4 + (-1))\n",
    "\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"(a+b)+c\": f\"{float(lhs):.8e}\",\n",
    "        \"a+(b+c)\": f\"{float(rhs):.8e}\",\n",
    "        \"equal?\": \"YES\" if float(lhs) == float(rhs) else \"NO\",\n",
    "        \"abs_diff\": f\"{abs(float(lhs) - float(rhs)):.2e}\",\n",
    "        \"explanation\": \"\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"  (a+b)+c: first computes 1.0 + 1e-4. In FP16, this is just 1.0 (addition trap).\")\n",
    "print(\"           Then 1.0 + (-1.0) = 0.0. The 1e-4 is completely lost.\")\n",
    "print(\"  a+(b+c): first computes 1e-4 + (-1.0) = -0.9999. No precision loss here.\")\n",
    "print(\"           Then 1.0 + (-0.9999) = 1e-4 (approximately). The value survives!\")\n",
    "print()\n",
    "print(\"Practical lesson: summation order matters in low precision.\")\n",
    "print(\"This is one reason parallel GPU reductions can give different results than sequential ones,\")\n",
    "print(\"and why autocast promotes large reductions to FP32 where the effect is negligible.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0b Catastrophic cancellation: when subtraction destroys information\n",
    "\n",
    "**Catastrophic cancellation** occurs when you subtract two nearly-equal numbers. The leading significant digits cancel, leaving only the noisy trailing digits. In low precision, there are fewer trailing digits to begin with, so the result can be almost entirely noise.\n",
    "\n",
    "This is exactly what happens in **variance computation**: $\\text{Var}(X) = E[X^2] - (E[X])^2$. If the mean is large relative to the standard deviation, both $E[X^2]$ and $(E[X])^2$ are large and nearly equal. The subtraction amplifies the rounding error in each term.\n",
    "\n",
    "This is the mechanical reason **LayerNorm and BatchNorm statistics need FP32** under autocast.\n",
    "\n",
    "The \"safe\" formula avoids the cancellation: $\\text{Var}(X) = E[(X - E[X])^2]$ -- subtract the mean *before* squaring, so you never form two large nearly-equal numbers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Catastrophic cancellation in variance computation\n",
    "\n",
    "def variance_naive(x):\n",
    "    # Var(X) = E[X^2] - E[X]^2 -- catastrophic cancellation when E[X] >> std(X)\n",
    "    return (x ** 2).mean() - x.mean() ** 2\n",
    "\n",
    "def variance_safe(x):\n",
    "    # Var(X) = E[(X - E[X])^2] -- avoids large-number subtraction\n",
    "    return ((x - x.mean()) ** 2).mean()\n",
    "\n",
    "# Test with offset data: mean ≈ 10000, std ≈ 0.1\n",
    "set_seed(0)\n",
    "x_fp32 = (torch.randn(50_000, device=device) * 0.1 + 10_000.0).float()\n",
    "ref_var = float(variance_safe(x_fp32.double()))\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    x = x_fp32.to(dt)\n",
    "    v_naive = float(variance_naive(x))\n",
    "    v_safe = float(variance_safe(x))\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"naive_var (E[X²]-E[X]²)\": f\"{v_naive:.6e}\",\n",
    "        \"safe_var (E[(X-μ)²])\": f\"{v_safe:.6e}\",\n",
    "        \"reference (FP64)\": f\"{ref_var:.6e}\",\n",
    "        \"naive_rel_err\": f\"{abs(v_naive - ref_var) / (ref_var + 1e-30):.2e}\",\n",
    "        \"safe_rel_err\": f\"{abs(v_safe - ref_var) / (ref_var + 1e-30):.2e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"  The naive formula (E[X²] - E[X]²) catastrophically cancels in FP16/BF16 because\")\n",
    "print(\"  E[X²] ≈ 1e8 and E[X]² ≈ 1e8, but their difference is only ~0.01.\")\n",
    "print(\"  Subtracting two huge nearly-equal numbers amplifies the rounding error.\")\n",
    "print()\n",
    "print(\"  The safe formula (E[(X-μ)²]) subtracts the mean FIRST, so the squared values\")\n",
    "print(\"  are small (~0.01) and there's no cancellation.\")\n",
    "print()\n",
    "print(\"  PyTorch's LayerNorm/BatchNorm use the safe formula internally — but even so,\")\n",
    "print(\"  autocast runs them in FP32 because the intermediate accumulations still benefit\")\n",
    "print(\"  from higher precision.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0c Kahan summation: the compensated-sum fix\n",
    "\n",
    "If naive accumulation loses precision, can we do better without just using FP32?\n",
    "\n",
    "**Kahan summation** (compensated summation) tracks a separate \"error compensation\" variable that captures the rounding error from each addition and feeds it back into the next step. It's like an accountant who keeps a running tally of all the pennies that got rounded off.\n",
    "\n",
    "```\n",
    "running_sum = 0.0\n",
    "compensation = 0.0        # tracks accumulated rounding error\n",
    "\n",
    "for x in values:\n",
    "    y = x - compensation   # add back what was lost last time\n",
    "    t = running_sum + y    # this addition might round\n",
    "    compensation = (t - running_sum) - y   # what got lost this time\n",
    "    running_sum = t\n",
    "```\n",
    "\n",
    "Some high-performance kernels internally use compensated summation when operating in low precision. Understanding it helps you appreciate what \"accumulate in FP32\" really means: it's the brute-force version of the same idea (just use more bits instead of being clever about rounding)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Kahan summation vs naive summation in low precision\n",
    "\n",
    "def kahan_sum(values, dtype):\n",
    "    # Compensated summation in the given dtype.\n",
    "    s = torch.tensor(0.0, dtype=dtype)\n",
    "    c = torch.tensor(0.0, dtype=dtype)   # compensation for lost low-order bits\n",
    "    for v in values:\n",
    "        y = v.to(dtype) - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y     # captures the rounding error\n",
    "        s = t\n",
    "    return float(s)\n",
    "\n",
    "def naive_sum(values, dtype):\n",
    "    # Simple sequential sum in the given dtype.\n",
    "    s = torch.tensor(0.0, dtype=dtype)\n",
    "    for v in values:\n",
    "        s = s + v.to(dtype)\n",
    "    return float(s)\n",
    "\n",
    "# Sum 10,000 small values: 1e-3 each → expected total = 10.0\n",
    "N = 10_000\n",
    "values = [torch.tensor(1e-3)] * N\n",
    "expected = 10.0\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    n_sum = naive_sum(values, dt)\n",
    "    k_sum = kahan_sum(values, dt)\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt).replace(\"torch.\", \"\"),\n",
    "        \"naive_sum\": f\"{n_sum:.6f}\",\n",
    "        \"kahan_sum\": f\"{k_sum:.6f}\",\n",
    "        \"expected\": f\"{expected:.6f}\",\n",
    "        \"naive_err\": f\"{abs(n_sum - expected):.4e}\",\n",
    "        \"kahan_err\": f\"{abs(k_sum - expected):.4e}\",\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nKahan summation recovers much of the precision lost by naive accumulation.\")\n",
    "print(\"In practice, 'accumulate in FP32' (what Tensor Cores and autocast do) achieves\")\n",
    "print(\"the same goal with less complexity: you just use a wider accumulator register.\")\n",
    "print(\"But Kahan summation shows that the problem IS solvable in low precision —\")\n",
    "print(\"it's just not worth the extra operations when FP32 accumulators are available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Where does exp() overflow by dtype?\n",
    "\n",
    "x = torch.linspace(-20, 20, 400, device=device)\n",
    "\n",
    "def safe_exp(x, dtype):\n",
    "    return torch.exp(x.to(dtype)).to(torch.float32).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt, label in [(torch.float32, \"FP32\"), (torch.bfloat16, \"BF16\")]:\n",
    "    y = safe_exp(x, dt)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y, 1e-30, 1e30)), label=label)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    y16 = safe_exp(x, torch.float16)\n",
    "    plt.plot(x.cpu().numpy(), np.log10(np.clip(y16, 1e-30, 1e30)), label=\"FP16\")\n",
    "\n",
    "plt.axhline(np.log10(65504), color=\"r\", linestyle=\"--\", alpha=0.5, label=\"FP16 max (65504)\")\n",
    "plt.title(\"log10(exp(x)) computed in different dtypes — FP16 overflows early\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(exp(x))\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Loss functions are \"log-sum-exp machines\" (and dtype matters)\n",
    "\n",
    "Many deep learning losses contain exponentials and logs. A classic example:\n",
    "\n",
    "$$\\log(1 + e^x) \\quad \\text{(softplus)}$$\n",
    "\n",
    "- The naive formula overflows quickly in FP16 (for $x > 11$, $e^x > 65504$).\n",
    "- Stable implementations (e.g., `F.softplus`) avoid overflow by rewriting the expression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softplus across dtypes\n",
    "\n",
    "def naive_softplus(x):\n",
    "    return torch.log1p(torch.exp(x))\n",
    "\n",
    "x = torch.linspace(-80, 80, 2000, device=device)\n",
    "ref = F.softplus(x.double()).float()  # high-precision reference\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    y_naive = naive_softplus(x.to(dt)).float()\n",
    "    y_stable = F.softplus(x.to(dt)).float()\n",
    "    err_naive = (y_naive - ref).abs().cpu().numpy()\n",
    "    err_stable = (y_stable - ref).abs().cpu().numpy()\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_naive + 1e-12), label=f\"naive {dt}\")\n",
    "    plt.plot(x.cpu().numpy(), np.log10(err_stable + 1e-12), ls=\"--\", label=f\"stable {dt}\")\n",
    "\n",
    "plt.title(\"Softplus error: naive vs stable implementation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log10(|error|) vs FP64 reference\")\n",
    "plt.legend(ncols=2)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Softmax: the overflow trap and the stability rewrite\n",
    "\n",
    "Softmax is everywhere in transformers (attention). Naive softmax:\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "This can overflow in low precision because $e^x$ explodes quickly. The stable rewrite subtracts the max:\n",
    "\n",
    "$$\\text{softmax}(x) = \\text{softmax}(x - \\max(x))$$\n",
    "\n",
    "PyTorch's `F.softmax` uses a stable implementation. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Naive vs stable softmax\n",
    "\n",
    "def naive_softmax(x, dim=-1):\n",
    "    ex = torch.exp(x)\n",
    "    return ex / ex.sum(dim=dim, keepdim=True)\n",
    "\n",
    "logits = torch.tensor([0.0, 20.0, 40.0, 80.0], device=device)\n",
    "ref = F.softmax(logits.double(), dim=0).float()\n",
    "\n",
    "rows = []\n",
    "for dt in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "    if device.type == \"cpu\" and dt is torch.float16:\n",
    "        continue\n",
    "    x = logits.to(dt)\n",
    "    try:\n",
    "        naive = naive_softmax(x, dim=0).float()\n",
    "        naive_ok = bool(torch.isfinite(naive).all().item())\n",
    "    except Exception:\n",
    "        naive = torch.full_like(ref, float(\"nan\"))\n",
    "        naive_ok = False\n",
    "    stable = F.softmax(x, dim=0).float()\n",
    "    stable_ok = bool(torch.isfinite(stable).all().item())\n",
    "    rows.append({\n",
    "        \"dtype\": str(dt),\n",
    "        \"naive_finite\": naive_ok,\n",
    "        \"stable_finite\": stable_ok,\n",
    "        \"max_abs_err(stable vs ref)\": f\"{float((stable - ref).abs().max()):.2e}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 What AMP actually is\n",
    "\n",
    "In PyTorch, AMP is two complementary tools:\n",
    "\n",
    "1. **`autocast`** (forward + loss)\n",
    "   - A context manager that applies a **per-operation dtype policy**.\n",
    "   - It *temporarily* casts inputs/weights for each operation. It does **not** permanently change model parameters.\n",
    "   - Matmuls/linear → lower precision. Softmax/layernorm/losses → FP32.\n",
    "\n",
    "2. **`GradScaler`** (backward + optimizer step)\n",
    "   - Primarily for **FP16 training** (BF16 usually doesn't need it).\n",
    "   - Multiplies loss by a scale factor $S$ before backward → gradients are $S\\times$ larger → fewer underflow to zero.\n",
    "   - Before optimizer step, divides gradients by $S$. If overflow is detected (`inf`/`nan`), skips the step and reduces $S$.\n",
    "\n",
    "**Clean mental model:**\n",
    "- `autocast` protects you from **bad forward dtypes**.\n",
    "- `GradScaler` protects you from **bad backward magnitudes** (FP16 gradient underflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The canonical AMP training loop (four conceptual changes)\n",
    "\n",
    "Start with FP32 training:\n",
    "```python\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "logits = model(x)\n",
    "loss = loss_fn(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "AMP adds four things:\n",
    "```python\n",
    "scaler = GradScaler()\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "with autocast(device_type=\"cuda\", dtype=torch.float16):   # 1. wrap forward + loss\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "scaler.scale(loss).backward()    # 2. scale loss before backward\n",
    "scaler.step(optimizer)           # 3. unscale + check for inf/nan + step\n",
    "scaler.update()                  # 4. adjust scale factor\n",
    "```\n",
    "\n",
    "That's the \"small code change\" people talk about. But the *reason* it works is the theory above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Master weights and optimizer state (why \"just casting the model\" is not the same)\n",
    "\n",
    "There are **three** numeric objects in training that matter:\n",
    "\n",
    "1. **Parameters** (weights) — used in forward/backward\n",
    "2. **Gradients** — produced by backward\n",
    "3. **Optimizer state** (e.g., Adam's first and second moment estimates $m_t, v_t$) — long-lived accumulators\n",
    "\n",
    "A classic mixed precision recipe:\n",
    "- Keep a **master copy of weights in FP32**.\n",
    "- Do forward/backward in FP16/BF16 where safe.\n",
    "- Maintain optimizer state (Adam moments) in FP32.\n",
    "\n",
    "**Why FP32 master weights?**\n",
    "\n",
    "Because 16-bit formats have coarse spacing. A small update $\\Delta w = \\text{lr} \\times \\text{grad}$ can be *below the ULP* at the magnitude of $w$, so the weight never changes. We showed this in the `1 + 1e-4` demo above.\n",
    "\n",
    "Over many steps, these tiny updates *accumulate* in FP32 and eventually become large enough to appear in the 16-bit copy. This is the key insight from the Micikevicius et al. (2017) paper.\n",
    "\n",
    "**Why FP32 optimizer state?**\n",
    "\n",
    "Adam's moments are exponential moving averages. They accumulate information over the entire training run. Even small rounding errors compound over thousands of steps. Keeping moments in FP32 prevents this drift.\n",
    "\n",
    "**Memory implication:**\n",
    "\n",
    "Mixed precision doesn't eliminate FP32 — it just limits FP32 to parameters + optimizer state (which is fixed-size), while saving on activations (which scale with batch size and sequence length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Autocast is an operator policy (not a global cast)\n",
    "\n",
    "Autocast does **not** \"turn the whole model into FP16\". Instead it applies a per-operation policy:\n",
    "\n",
    "| Policy | Operations | Rationale |\n",
    "|---|---|---|\n",
    "| **Lower precision** (FP16/BF16) | `linear`, `matmul`, `mm`, `bmm`, convolutions | Compute-bound → Tensor Core speedup |\n",
    "| **Force FP32** | `softmax`, `layer_norm`, `log_softmax`, `mse_loss`, `cross_entropy`, `sum`, `prod`, `exp`, `log`, ... | Numerically sensitive: overflow/underflow/accumulation risk |\n",
    "| **Promote to widest** | Binary ops when inputs differ | If one input is FP32, the op runs in FP32 |\n",
    "| **Pass-through** | `relu`, `dropout`, `max`, `min`, `mean`, ... | Element-wise, no numeric risk; output matches input dtype |\n",
    "\n",
    "The exact policy is PyTorch-version-dependent. In Section 3 we will probe it empirically on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 summary\n",
    "\n",
    "| Concept | Key takeaway |\n",
    "|---|---|\n",
    "| **FP16** | Better precision than BF16, but narrow range → needs loss scaling and careful op policies |\n",
    "| **BF16** | FP32-like range → often trains without scaling, but coarse precision → reductions need care |\n",
    "| **TF32** | Your \"FP32 baseline\" on Ampere+ GPUs may secretly be TF32 (10-bit mantissa) for matmuls |\n",
    "| **AMP** | `autocast` (forward op policy) + `GradScaler` (backward magnitude control) |\n",
    "| **Master weights** | Keep FP32 copy for updates; prevents stagnation from ULP rounding |\n",
    "| **Optimizer state** | Keep in FP32; long-horizon accumulation is precision-sensitive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 — What the literature says\n",
    "\n",
    "This section is intentionally *written*: the point is to build a paper-and-doc-driven mental model that you can carry into real training code.\n",
    "\n",
    "No experiments here — only explanations. Think of this section as: \"what each source contributes, and how it maps to PyTorch AMP.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 A reading map\n",
    "\n",
    "| Source | What it gives you | Maps to |\n",
    "|---|---|---|\n",
    "| Gupta et al. (2015) | Why limited precision can still train; stochastic rounding + scaling intuition | Section 1.3 (rounding/accumulation), Section 3 debugging |\n",
    "| Micikevicius et al. (2017) | The canonical mixed precision recipe (master weights + loss scaling) | Sections 1.4–1.6, scaling experiments |\n",
    "| Kalamkar et al. (2019) | BF16 bit-level analysis, proof that BF16 avoids FP16's underflow | Section 1.2, BF16 training runs |\n",
    "| NVIDIA mixed precision guidance | Engineering intuition + failure modes | Underflow/overflow + \"sensitive ops in FP32\" |\n",
    "| PyTorch `torch.amp` docs | The actual API + gotchas | `autocast` + `GradScaler` loops in Section 3 |\n",
    "| PyTorch autocast op reference | *The* per-op policy | Probed empirically in Section 3.2 |\n",
    "| Rajbhandari et al. (2020) — ZeRO | Memory breakdown of mixed-precision training at scale | Optimizer state precision, Section 2.6 |\n",
    "| Distributed training docs (FSDP/ZeRO/DeepSpeed) | Where dtypes live in large systems | Section 2.7 |\n",
    "| FP8 literature (e.g., NVIDIA) | Why FP8 needs scaling (E4M3 vs E5M2) + where it fits vs AMP | Section 1 tables, Section 3 practical guidance |\n",
    "| Dettmers et al. (8-bit optimizers) | Optimizer-state precision as the next bottleneck after AMP | Section 2.6 memory discussion |\n",
    "\n",
    "If you only read one thing: read the Micikevicius paper, then read the PyTorch autocast op reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Micikevicius et al. (2017): *Mixed Precision Training*\n",
    "\n",
    "> arXiv:1710.03740 — ICLR 2018\n",
    "\n",
    "This is the foundational paper for everything in this notebook. It introduced the three-part recipe that all modern AMP implementations are based on.\n",
    "\n",
    "**The problem:** FP16 arithmetic is fast (2–8$\\times$ throughput on Tensor Cores) and memory-efficient (half the bytes), but naively training in FP16 breaks. Models either diverge, produce NaN losses, or silently stagnate. The paper identifies three distinct failure modes:\n",
    "\n",
    "1. **Weight update stagnation.** When a weight $w$ is stored in FP16, the update $\\Delta w = \\eta \\cdot g$ can be smaller than the ULP (unit in the last place) at $|w|$. The update is rounded away and the weight never changes. The paper's solution: maintain an FP32 \"master copy\" of all parameters. Forward and backward compute use the FP16 cast, but the actual parameter update happens in FP32 where the precision is sufficient to register small changes. The updated FP32 value is then cast back to FP16 for the next forward pass.\n",
    "\n",
    "2. **Gradient underflow.** Even with FP32 master weights, gradients themselves are computed in FP16 during the backward pass. FP16's smallest normalized number is approximately $6 \\times 10^{-5}$. The paper's analysis of gradient histograms across several production models (image classifiers, speech models, generative models) shows that a significant fraction of gradient values fall below this threshold. Once they underflow to zero, the update signal is lost. The solution: **loss scaling**. Multiply the scalar loss by a large constant $S$ before calling `backward()`. Since backpropagation is a chain of multiplications, every gradient in the network is scaled up by $S$. After backward, divide gradients by $S$ before the optimizer step. If $S$ is chosen well, gradients that would have underflowed are now safely within FP16's representable range, and the division by $S$ restores correct magnitudes. The paper demonstrates that a fixed $S$ works for many models, but notes that $S$ too large can cause gradient *overflow*. This motivates **dynamic loss scaling**: start with a large $S$, and if `inf`/`nan` gradients are detected, skip the update and reduce $S$. If training is stable, periodically increase $S$ to maximize headroom.\n",
    "\n",
    "3. **Accumulation error in reductions.** Large dot products and reductions (e.g., summing thousands of values) accumulate rounding errors that compound in FP16. The paper recommends performing these accumulations in FP32, even when the operands are FP16. Modern Tensor Cores implement this: they accept FP16 inputs but accumulate in FP32.\n",
    "\n",
    "**What to remember:** AMP is not a single trick. It is the *combination* of (a) per-op precision policies, (b) loss scaling, and (c) FP32 master weights/optimizer state. Removing any one of these can cause training to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kalamkar et al. (2019): *A Study of BFLOAT16 for Deep Learning Training*\n",
    "\n",
    "> arXiv:1905.12322\n",
    "\n",
    "This paper provides the empirical and theoretical justification for BF16 as a training format.\n",
    "\n",
    "**The key insight** is architectural: BF16 uses **8 exponent bits** (same as FP32), giving it the same dynamic range — roughly $10^{-38}$ to $10^{38}$. This means BF16 can represent the same extreme magnitudes as FP32. The gradient underflow problem that plagues FP16 (5-bit exponent → range only $10^{-5}$ to $6.5 \\times 10^4$) essentially does not exist for BF16.\n",
    "\n",
    "**The tradeoff:** BF16 sacrifices mantissa bits (7 vs FP16's 10 vs FP32's 23). This means worse *precision* per individual value. But the paper demonstrates empirically that deep neural networks are remarkably robust to precision loss — they are much more sensitive to *range* loss. Training with BF16 across a variety of models (ResNets, Transformers, LSTMs) converges to the same final accuracy as FP32, often without any loss scaling at all.\n",
    "\n",
    "**Conversion simplicity:** Since BF16 and FP32 share the same exponent field, conversion is trivial: truncate (or round) the bottom 16 mantissa bits. FP32 → FP16 conversion is harder because the exponent field must also be narrowed, risking overflow or underflow in the conversion itself.\n",
    "\n",
    "**Practical implication for AMP:** BF16 autocast is often a \"drop-in\" replacement that doesn't require a `GradScaler`. This is explicitly confirmed by modern frameworks: DeepSpeed documentation states, \"Training with bfloat16 does not require loss scaling.\"\n",
    "\n",
    "**When BF16 can still struggle:** Precision-sensitive accumulations (layernorm statistics, attention score normalization, large batch reductions) can still degrade with BF16's 7-bit mantissa. This is why autocast routes these operations to FP32 even in BF16 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NVIDIA mixed precision guidance (the engineering perspective)\n",
    "\n",
    "NVIDIA's developer blog and documentation provide the practitioner's view of mixed precision. The key engineering decomposition:\n",
    "\n",
    "1. **Tensor Core compute** wants FP16/BF16 inputs. Modern GPUs (Volta+) have dedicated matrix-multiply-accumulate units that operate on 16-bit inputs with FP32 accumulation. These deliver 2–8$\\times$ the throughput of FP32 CUDA cores for matmuls, which dominate deep learning compute.\n",
    "\n",
    "2. **Dimension alignment matters.** Tensor Cores process tiles of fixed size (typically 8 or 16 elements). If your tensor dimensions aren't multiples of 8, you either waste hardware or fall back to slower code paths. This is a practical detail that affects whether you actually see the theoretical speedup.\n",
    "\n",
    "3. **Memory bandwidth matters as much as compute.** 16-bit formats halve the bytes transferred between GPU memory (HBM) and compute cores (SMs). For memory-bandwidth-bound models (which many transformer models are at inference time), this alone can yield significant speedup even without Tensor Core benefits.\n",
    "\n",
    "4. **Some ops are numerically sensitive.** NVIDIA's guidance identifies the same ops that PyTorch's autocast routes to FP32: exponentials, logs, softmax, normalization statistics, and large reductions. The rationale is the same: overflow/underflow risk and accumulation error.\n",
    "\n",
    "The practical outcome is the \"few lines of code\" AMP recipe. But the engineering reason it works is the numeric analysis in the theory section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BF16: \"FP32 range, fewer bits of precision\"\n",
    "\n",
    "BF16 was originally designed for Google's TPUs. The design goal was explicit: make it possible to train neural networks in 16-bit formats *without* the underflow problems of FP16.\n",
    "\n",
    "**The design decision:** Given 16 bits, how should you split them between exponent and mantissa?\n",
    "\n",
    "- FP16 (IEEE): 5 exponent + 10 mantissa → good precision, but range only $[6 \\times 10^{-5}, 6.5 \\times 10^4]$.\n",
    "- BF16 (Google/Intel): 8 exponent + 7 mantissa → FP32-like range $[10^{-38}, 3.4 \\times 10^{38}]$, coarser precision.\n",
    "\n",
    "For training, range wins. Gradients span many orders of magnitude, and losing any of them to underflow is worse than representing them imprecisely. Networks are fundamentally robust to noise (they're trained with stochastic gradient descent after all), but they cannot learn from zero gradients.\n",
    "\n",
    "**In practice, for many transformer trainings on modern GPUs:**\n",
    "- BF16 autocast is \"drop-in\" without loss scaling.\n",
    "- FP16 autocast typically needs a `GradScaler`.\n",
    "- Both still benefit from FP32 master weights and FP32 optimizer state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 PyTorch AMP docs: the operator policy is the decoder ring\n",
    "\n",
    "A common mistake is to treat autocast like a global switch (\"my model is now FP16\"). But autocast is really a **per-operation dispatch table**:\n",
    "\n",
    "- Some ops are eligible for lower precision (FP16/BF16). These are the compute-heavy ops where Tensor Cores give speedup.\n",
    "- Some ops are forced to FP32. These are the numerically sensitive ops where lower precision would cause training failures.\n",
    "- Some ops promote to the widest input type. These are binary operations where mixed-dtype inputs would be ambiguous.\n",
    "- Unlisted ops run in whatever dtype they receive (pass-through).\n",
    "\n",
    "**The key documentation page** is the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). It lists every CUDA op and its autocast behavior. This is the authoritative answer to \"why did this op run in FP32?\" — faster and more reliable than any blog post.\n",
    "\n",
    "**A common debugging pattern:** when AMP training fails, the first thing to check is which ops are running in which dtype. The dtype hooks we build in Section 3 make this visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Rajbhandari et al. (2020): ZeRO and optimizer state precision\n",
    "\n",
    "> arXiv:1910.02054 — *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models*\n",
    "\n",
    "While ZeRO is primarily about distributed training, its memory analysis (Section 2) is essential for understanding where mixed precision fits.\n",
    "\n",
    "**The memory breakdown for a model with $\\Psi$ parameters trained with Adam in mixed precision:**\n",
    "\n",
    "| Component | Dtype | Bytes per parameter |\n",
    "|---|---|---|\n",
    "| FP16 parameters (forward/backward) | FP16 | 2 |\n",
    "| FP16 gradients | FP16 | 2 |\n",
    "| FP32 master weights | FP32 | 4 |\n",
    "| FP32 Adam first moment ($m$) | FP32 | 4 |\n",
    "| FP32 Adam second moment ($v$) | FP32 | 4 |\n",
    "| **Total** | | **16 bytes/param** |\n",
    "\n",
    "For a 7B parameter model: $7 \\times 10^9 \\times 16 = 112$ GB just for parameters + optimizer state — before any activations.\n",
    "\n",
    "**Key insight:** Mixed precision doesn't eliminate FP32 from the system. It moves FP32 to where it's needed (optimizer state, master weights) and uses 16-bit where it's safe (activations, compute). The memory savings come primarily from activations (which scale with batch size and sequence length), not from the fixed-size parameter/optimizer memory.\n",
    "\n",
    "**Practical takeaway:** \"AMP\" in a distributed stack is not only about autocast during forward/backward. It is about **where each tensor lives and in what dtype** across the entire training loop: parameters, gradients, optimizer state, communication buffers, and activation checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 LLM training stacks (FSDP / ZeRO / DeepSpeed): where precision choices multiply\n",
    "\n",
    "At LLM scale, training systems shard parameters, gradients, and optimizer state across multiple GPUs. Mixed precision gets more complicated because dtype decisions affect:\n",
    "\n",
    "- **Parameter storage:** \"working\" params in BF16/FP16 for forward/backward; FP32 master copy for updates.\n",
    "- **Gradient communication:** All-reduce operations can accumulate rounding errors. Some stacks reduce in FP32 for stability, others use BF16 + gradient compression.\n",
    "- **Optimizer state:** Typically FP32 (Adam moments need high precision over long training). Some frameworks offer FP16 optimizer states as a memory optimization, but this trades stability for memory.\n",
    "- **Activation checkpointing:** Recomputed activations use the same autocast policy as the original forward pass, but the checkpointing mechanism itself must preserve dtypes correctly.\n",
    "\n",
    "**FSDP mixed precision** (PyTorch): lets you separately control `param_dtype` (for sharded parameters), `reduce_dtype` (for gradient all-reduce), and `buffer_dtype`. FP16 + scaler or BF16 without scaler.\n",
    "\n",
    "**DeepSpeed ZeRO:** explicit config knobs. `fp16.loss_scale = 0` enables dynamic loss scaling; `bf16.enabled = true` with no loss scaling.\n",
    "\n",
    "**The practical lesson:** When debugging AMP issues in distributed training, the dtype of every tensor transfer (parameter broadcast, gradient reduce, optimizer state scatter/gather) is a potential source of numeric problems. This is well beyond the scope of a single `autocast` context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 How to think about the autocast policy (conceptual categories)\n",
    "\n",
    "Autocast decisions fall into a few conceptual buckets. These aren't arbitrary — each traces back to a specific numeric risk:\n",
    "\n",
    "**1. Lower precision eligible (matmul-like ops)**\n",
    "\n",
    "These are the ops where Tensor Cores provide speedup and numeric risk is low. The key property: these operations multiply pairs of values and accumulate in FP32 (on Tensor Cores), so the inputs can be 16-bit without precision loss in the output.\n",
    "\n",
    "**2. Force FP32 (numerically sensitive ops)**\n",
    "\n",
    "Two sub-categories:\n",
    "- **Overflow/underflow risk:** `exp`, `log`, `softmax`, `log_softmax`. These can produce extreme magnitudes.\n",
    "- **Accumulation risk:** `sum`, `prod`, `layer_norm`, `batch_norm`, `cross_entropy`, `mse_loss`. These reduce many values and small rounding errors compound.\n",
    "\n",
    "**3. Promote to widest (binary ops with mixed inputs)**\n",
    "\n",
    "If you add a BF16 tensor to an FP32 tensor, the result is FP32. This prevents silent precision loss when autocast and non-autocast regions interact.\n",
    "\n",
    "**4. Pass-through (element-wise, no numeric risk)**\n",
    "\n",
    "`relu`, `sigmoid`, `tanh`, `dropout`, `max`, `min`, `mean`. These just transform individual values without extreme magnitudes or accumulation. Whatever goes in comes out.\n",
    "\n",
    "**Practitioner's two rules:**\n",
    "- If an op creates very large/small magnitudes ($e^x$, $\\log x$, softmax), it needs FP32.\n",
    "- If an op reduces many values ($\\sum$, variance, cross-entropy), it needs FP32 accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Dynamic loss scaling: what `GradScaler` is doing\n",
    "\n",
    "Loss scaling multiplies the scalar loss by a factor $S$ before calling `backward()`.\n",
    "\n",
    "Since backpropagation is linear (gradients are proportional to the loss), every gradient in the network is multiplied by $S$. This shifts the entire gradient distribution toward larger magnitudes, rescuing values that would have underflowed to zero in FP16.\n",
    "\n",
    "**The protocol:**\n",
    "\n",
    "1. Compute loss normally under autocast.\n",
    "2. Multiply loss by $S$ and call `backward()`.\n",
    "3. Before the optimizer step, **unscale** all gradients by dividing by $S$.\n",
    "4. Check for `inf`/`nan` in gradients:\n",
    "   - If found: **skip** the optimizer step, reduce $S$ (usually halve it).\n",
    "   - If not found: proceed with the step. Periodically increase $S$ (e.g., double it every $N$ successful steps) to maximize headroom.\n",
    "\n",
    "**Why it's safe:** scaling and unscaling cancel out exactly if no overflow occurs. The optimizer sees the same gradients it would have without scaling.\n",
    "\n",
    "**Critical detail:** gradient clipping must happen **after** unscaling. PyTorch's `scaler.unscale_(optimizer)` does the unscale explicitly; `scaler.step(optimizer)` then checks for inf/nan before calling `optimizer.step()`.\n",
    "\n",
    "**Why BF16 doesn't need this:** BF16 has the same exponent range as FP32. Gradients that would be $10^{-10}$ in FP32 are also representable in BF16 (though with less precision). No underflow → no need for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Gupta et al. (2015): *Deep Learning with Limited Numerical Precision*\n",
    "\n",
    "This earlier line of work is worth knowing because it frames low-precision training as a **numerical analysis problem**, not a hardware trick.\n",
    "\n",
    "**The core idea:** training can succeed even when weights/activations/gradients are represented in low precision, *if you control rounding and scaling*.\n",
    "\n",
    "Key takeaways often cited from this family of results:\n",
    "- **Rounding is the enemy.** Deterministic rounding can systematically bias updates. Stochastic rounding can remove that bias at the cost of noise.\n",
    "- **Scaling matters.** Keeping values in a representable dynamic range is the difference between \"noisy training\" and \"dead training\" (underflow to zeros).\n",
    "- **Noise tolerance is real.** SGD is already noisy; in many regimes, extra quantization noise is tolerable *as long as the signal is not destroyed*.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- Autocast is a modern, practical version of \"do low precision where it's safe\".\n",
    "- Loss scaling is a specialized scaling strategy focused on preserving *gradient* signal in FP16.\n",
    "- The accumulation demos in Section 1 (tiny increments and LayerNorm stats) are the same failure modes this literature is trying to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1 Stochastic rounding: why it helps low-precision training\n",
    "\n",
    "One key insight from the Gupta line of work is that **rounding mode matters**.\n",
    "\n",
    "Standard IEEE 754 rounding is **deterministic** (round-to-nearest, ties-to-even). This creates a systematic problem: if a value consistently falls just below a representable grid point, it *always* rounds down. Over many training steps, this introduces a persistent bias — the value drifts away from where it \"should\" be.\n",
    "\n",
    "**Stochastic rounding** instead rounds probabilistically:\n",
    "- If a value $x$ falls between grid points $x_\\text{lo}$ and $x_\\text{hi}$:\n",
    "  - Round to $x_\\text{hi}$ with probability $(x - x_\\text{lo}) / (x_\\text{hi} - x_\\text{lo})$\n",
    "  - Round to $x_\\text{lo}$ otherwise\n",
    "\n",
    "The expected value of the rounded result is $x$ itself — it's an **unbiased** estimator. Over many steps, small updates that would be systematically rounded away under deterministic rounding instead *accumulate on average*.\n",
    "\n",
    "**Connection to AMP:** Modern autocast doesn't use stochastic rounding (it uses standard IEEE rounding). Instead, it achieves a similar effect by keeping accumulations and updates in FP32, where the deterministic rounding error is small enough not to matter. But in FP8 and other extreme low-precision formats, stochastic rounding is actively used in some training frameworks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stochastic rounding demo: accumulate tiny updates in FP16\n",
    "\n",
    "def stochastic_round_fp16(x_fp32):\n",
    "    # Round FP32 value to FP16 with stochastic rounding.\n",
    "    x_lo = torch.tensor(float(x_fp32), dtype=torch.float16).float()\n",
    "    x_hi = torch.nextafter(torch.tensor(float(x_fp32), dtype=torch.float16),\n",
    "                           torch.tensor(float(\"inf\"), dtype=torch.float16)).float()\n",
    "    if float(x_hi) == float(x_lo):\n",
    "        return torch.tensor(float(x_lo), dtype=torch.float16)\n",
    "    # Probability of rounding up\n",
    "    p_up = (x_fp32 - x_lo) / (x_hi - x_lo + 1e-30)\n",
    "    p_up = float(p_up.clamp(0, 1))\n",
    "    if random.random() < p_up:\n",
    "        return torch.tensor(float(x_hi), dtype=torch.float16)\n",
    "    else:\n",
    "        return torch.tensor(float(x_lo), dtype=torch.float16)\n",
    "\n",
    "# Accumulate 1.0 + delta * N with delta below FP16 epsilon\n",
    "delta = 1e-4\n",
    "N = 5000\n",
    "expected = 1.0 + delta * N  # 1.5\n",
    "\n",
    "# Deterministic rounding (standard)\n",
    "w_det = torch.tensor(1.0, dtype=torch.float16)\n",
    "for _ in range(N):\n",
    "    w_det = (w_det.float() + delta).half()  # deterministic round\n",
    "\n",
    "# Stochastic rounding\n",
    "set_seed(42)\n",
    "w_stoch = torch.tensor(1.0, dtype=torch.float16)\n",
    "for _ in range(N):\n",
    "    w_stoch = stochastic_round_fp16(w_stoch.float() + delta)\n",
    "\n",
    "# FP32 reference\n",
    "w_fp32 = torch.tensor(1.0, dtype=torch.float32)\n",
    "for _ in range(N):\n",
    "    w_fp32 = w_fp32 + delta\n",
    "\n",
    "print(f\"Accumulating {N} updates of delta={delta} starting from 1.0\")\n",
    "print(f\"Expected result: {expected}\")\n",
    "print(f\"  FP16 deterministic: {float(w_det):.6f}  (error: {abs(float(w_det) - expected):.4e})\")\n",
    "print(f\"  FP16 stochastic:    {float(w_stoch):.6f}  (error: {abs(float(w_stoch) - expected):.4e})\")\n",
    "print(f\"  FP32 deterministic: {float(w_fp32):.6f}  (error: {abs(float(w_fp32) - expected):.4e})\")\n",
    "print()\n",
    "print(\"With deterministic rounding, the delta is below FP16's ULP at 1.0 (~1e-3),\")\n",
    "print(\"so it gets rounded away EVERY time. The weight never moves.\")\n",
    "print(\"With stochastic rounding, each update has a small probability of 'counting',\")\n",
    "print(\"so the weight drifts toward the correct value over many steps.\")\n",
    "print()\n",
    "print(\"Modern AMP avoids this issue by doing updates in FP32 (where delta > ULP),\")\n",
    "print(\"but stochastic rounding remains relevant for FP8 training and research.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 FP8 for deep learning: why it exists and why it's not just \"AMP but smaller\"\n",
    "\n",
    "FP8 is attractive because it can further reduce memory bandwidth and increase tensor-core throughput compared to FP16/BF16.\n",
    "\n",
    "But FP8 is fundamentally different from FP16/BF16:\n",
    "- the representable grid is *much* coarser\n",
    "- range depends strongly on the FP8 variant (commonly summarized as **E4M3** vs **E5M2**)\n",
    "- most practical FP8 training systems rely on **explicit scaling** (per-tensor/per-channel) and carefully chosen accumulation dtypes\n",
    "\n",
    "**The mental model:** FP8 compute works when you pair it with an explicit scale factor that keeps values near the \"sweet spot\" of the format. This makes FP8 closer to \"learned quantization with dynamic scaling\" than to the drop-in nature of BF16 autocast.\n",
    "\n",
    "**How it maps to AMP/autocast:**\n",
    "- AMP is primarily about FP16/BF16 (and TF32) policies inside a framework like PyTorch.\n",
    "- FP8 usually requires a specialized kernel stack (often beyond the default `torch.amp.autocast`) that manages scaling metadata alongside tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 8-bit optimizer-state work: the next bottleneck after AMP\n",
    "\n",
    "Classic AMP reduces activation memory and speeds up matmuls, but for large models the **optimizer state** becomes a dominant fixed cost (ZeRO's 16 bytes/parameter for Adam mixed precision).\n",
    "\n",
    "This motivates a separate line of work: compressing the **optimizer state** (and sometimes gradients) to 8-bit representations while preserving training quality.\n",
    "\n",
    "**How it relates to autocast:**\n",
    "- Autocast is about *compute dtype choice per op* during forward/loss (and indirectly backward).\n",
    "- 8-bit optimizers are about *storage precision* and *update math* for long-lived optimizer tensors.\n",
    "\n",
    "They are complementary: you can use AMP for activations/compute and (in some stacks) use 8-bit optimizer states to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 summary\n",
    "\n",
    "| Paper / Source | Key contribution |\n",
    "|---|---|\n",
    "| **Gupta et al.** | Limited precision can work when rounding/scaling are designed, not ignored |\n",
    "| **Micikevicius et al.** | The three-part recipe: per-op policies + loss scaling + FP32 master weights |\n",
    "| **Kalamkar et al.** | BF16's 8-bit exponent matches FP32 → no underflow → no loss scaling needed |\n",
    "| **NVIDIA guidance** | Engineering view: Tensor Cores, dimension alignment, bandwidth savings |\n",
    "| **PyTorch AMP docs** | The per-op dispatch table (the \"decoder ring\" for debugging) |\n",
    "| **ZeRO (Rajbhandari et al.)** | Memory breakdown: 16 bytes/param with Adam mixed precision |\n",
    "| **Distributed stacks** | Precision applies to params, grads, optimizer state, and communication separately |\n",
    "| **FP8 literature** | FP8 needs explicit scaling + specialized kernels; not a drop-in autocast dtype |\n",
    "| **8-bit optimizers** | Optimizer-state precision is the next memory target after AMP |\n",
    "\n",
    "**The single most useful reference for debugging:** the [PyTorch Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference). Bookmark it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 — Practicalities (experiments + graphs)\n",
    "\n",
    "This section is where we turn everything into measurements.\n",
    "\n",
    "**Principles:**\n",
    "- Prefer experiments that are **small, fast, and explain a single idea**.\n",
    "- Log everything you might need for debugging (loss, grad norms, scaler scale, step time, NaN/inf).\n",
    "- Make results comparable across dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Controlling confounders (TF32, randomness, and fair comparisons)\n",
    "\n",
    "When comparing FP32 vs AMP, you can accidentally compare the wrong thing:\n",
    "\n",
    "1. **TF32 on Ampere+:** Many FP32 matmuls use TF32 internally (10-bit mantissa). Your \"FP32 baseline\" may not be strict FP32 precision.\n",
    "2. **Randomness:** Dropout, data sampling, and nondeterministic kernels introduce run-to-run variance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if device.type == \"cuda\":\n",
    "    print(\"TF32 matmul:\", torch.backends.cuda.matmul.allow_tf32)\n",
    "    print(\"TF32 cuDNN:\", torch.backends.cudnn.allow_tf32)\n",
    "\n",
    "    # Uncomment to disable TF32 for strict FP32 comparisons:\n",
    "    # torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    # torch.backends.cudnn.allow_tf32 = False\n",
    "else:\n",
    "    print(\"CUDA not available; TF32 not applicable\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building mixed precision from scratch (the progressive implementation)\n",
    "\n",
    "This is the most important experiment in the notebook. Instead of jumping straight to `torch.amp`, we will:\n",
    "\n",
    "1. Train a simple model in **FP32** (baseline).\n",
    "2. Try **naive FP16** (just cast everything to half) — watch it fail or stagnate.\n",
    "3. Add **FP32 master weights** — fix the stagnation.\n",
    "4. Add **manual loss scaling** — fix gradient underflow.\n",
    "5. Replace everything with **PyTorch AMP** — the clean two-line version.\n",
    "\n",
    "By building each piece manually, the \"magic\" of AMP becomes completely transparent.\n",
    "\n",
    "We'll use a small MLP on a synthetic regression task to keep things fast and focused. The model is intentionally simple — the numeric effects are the same at any scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Progressive mixed-precision: shared setup\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Synthetic dataset: regression\n",
    "N_SAMPLES = 4096\n",
    "INPUT_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "X_data = torch.randn(N_SAMPLES, INPUT_DIM, device=device)\n",
    "# Target: a noisy linear function (so the model CAN learn it)\n",
    "true_w = torch.randn(INPUT_DIM, OUTPUT_DIM, device=device) * 0.01\n",
    "Y_data = X_data @ true_w + torch.randn(N_SAMPLES, OUTPUT_DIM, device=device) * 0.01\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "PROG_STEPS = 300\n",
    "\n",
    "def get_batches(steps):\n",
    "    for _ in range(steps):\n",
    "        idx = torch.randint(0, N_SAMPLES, (BATCH_SIZE,))\n",
    "        yield X_data[idx], Y_data[idx]\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def grad_stats(model):\n",
    "    # Return fraction of zero gradients and median |grad|.\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().float().flatten())\n",
    "    if not grads:\n",
    "        return 0.0, 0.0\n",
    "    g = torch.cat(grads)\n",
    "    zero_frac = float((g == 0).float().mean())\n",
    "    median_abs = float(g.abs().median())\n",
    "    return zero_frac, median_abs\n",
    "\n",
    "print(f\"Dataset: {N_SAMPLES} samples, input_dim={INPUT_DIM}\")\n",
    "print(f\"Model: MLP {INPUT_DIM} -> {HIDDEN_DIM} -> {HIDDEN_DIM} -> {OUTPUT_DIM}\")\n",
    "print(f\"Training: {PROG_STEPS} steps, batch_size={BATCH_SIZE}, lr={LR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Step 0: FP32 baseline\n",
    "\n",
    "Everything in FP32. This is our reference for correct training behavior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 0: FP32 baseline\n",
    "set_seed(42)\n",
    "model_fp32 = SimpleMLP().to(device).float()\n",
    "opt_fp32 = torch.optim.Adam(model_fp32.parameters(), lr=LR)\n",
    "\n",
    "log_fp32 = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": []}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_fp32.zero_grad(set_to_none=True)\n",
    "    pred = model_fp32(xb)\n",
    "    loss = F.mse_loss(pred, yb)\n",
    "    loss.backward()\n",
    "    zf, mg = grad_stats(model_fp32)\n",
    "    opt_fp32.step()\n",
    "    log_fp32[\"loss\"].append(float(loss))\n",
    "    log_fp32[\"zero_grad_frac\"].append(zf)\n",
    "    log_fp32[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"FP32 baseline: final loss = {log_fp32['loss'][-1]:.6f}\")\n",
    "print(f\"  Zero-grad fraction: {log_fp32['zero_grad_frac'][-1]:.4f}\")\n",
    "print(f\"  Median |grad|: {log_fp32['median_grad'][-1]:.2e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Step 1: Naive FP16 (just cast everything to half)\n",
    "\n",
    "The simplest approach: `model.half()` and cast inputs to FP16. No master weights, no scaling, no autocast.\n",
    "\n",
    "This is what the Micikevicius paper warns against. Expect problems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1: Naive FP16\n",
    "log_fp16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_fp16_naive[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping naive FP16 demo: CPU/MPS fp16 matmuls are often unsupported or unrepresentative.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_fp16_naive = SimpleMLP().to(device).half()\n",
    "    opt_fp16_naive = torch.optim.Adam(model_fp16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_fp16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_fp16_naive(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_fp16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_fp16_naive)\n",
    "        opt_fp16_naive.step()\n",
    "        log_fp16_naive[\"loss\"].append(float(loss))\n",
    "        log_fp16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_fp16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive FP16: status = {log_fp16_naive['status']}\")\n",
    "    if log_fp16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_fp16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_fp16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_fp16_naive['median_grad'][-1]:.2e}\")\n",
    "    else:\n",
    "        print(\"  Training failed immediately.\")\n",
    "\n",
    "    print()\n",
    "    if log_fp16_naive[\"status\"] != \"ok\" or (log_fp16_naive[\"loss\"] and log_fp16_naive[\"loss\"][-1] > log_fp32[\"loss\"][-1] * 5):\n",
    "        print(\"As expected, naive FP16 is problematic. The combination of:\")\n",
    "        print(\"  1. Weight update stagnation (updates below FP16 ULP)\")\n",
    "        print(\"  2. Gradient underflow (small gradients become zero)\")\n",
    "        print(\"makes training unstable or ineffective.\")\n",
    "    else:\n",
    "        print(\"Naive FP16 happened to work for this model (it sometimes does for simple/small models).\")\n",
    "        print(\"This does NOT mean it's safe in general.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2b Step 1b: Naive BF16 (the same cast-everything approach, but with BF16)\n",
    "\n",
    "Now do the *exact same thing* but with BF16 instead of FP16. This is the key comparison that shows **why BF16's range matters more than FP16's precision** for training stability.\n",
    "\n",
    "Same code, same model, same learning rate — just a different 16-bit format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1b: Naive BF16\n",
    "log_bf16_naive = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if not supports_dtype_on_device(torch.bfloat16, device):\n",
    "    log_bf16_naive[\"status\"] = \"skipped_no_bf16\"\n",
    "    print(\"Skipping naive BF16 demo: BF16 not supported on this device.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_bf16_naive = SimpleMLP().to(device).to(torch.bfloat16)\n",
    "    opt_bf16_naive = torch.optim.Adam(model_bf16_naive.parameters(), lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_bf16_naive.zero_grad(set_to_none=True)\n",
    "        pred = model_bf16_naive(xb.to(torch.bfloat16))\n",
    "        loss = F.mse_loss(pred, yb.to(torch.bfloat16))\n",
    "        if not torch.isfinite(loss):\n",
    "            log_bf16_naive[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "        zf, mg = grad_stats(model_bf16_naive)\n",
    "        opt_bf16_naive.step()\n",
    "        log_bf16_naive[\"loss\"].append(float(loss))\n",
    "        log_bf16_naive[\"zero_grad_frac\"].append(zf)\n",
    "        log_bf16_naive[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"Naive BF16: status = {log_bf16_naive['status']}\")\n",
    "    if log_bf16_naive[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_bf16_naive['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_bf16_naive['zero_grad_frac'][-1]:.4f}\")\n",
    "        print(f\"  Median |grad|: {log_bf16_naive['median_grad'][-1]:.2e}\")\n",
    "\n",
    "    print()\n",
    "    if log_bf16_naive[\"status\"] == \"ok\" and log_bf16_naive[\"loss\"]:\n",
    "        if log_bf16_naive[\"loss\"][-1] < log_fp32[\"loss\"][-1] * 2:\n",
    "            print(\"BF16 naive training WORKS! This is the key insight:\")\n",
    "            print(\"  - BF16 has the same exponent range as FP32 (8-bit exponent)\")\n",
    "            print(\"  - Gradients don't underflow, so training progresses even without loss scaling\")\n",
    "            print(\"  - The coarser precision (7-bit mantissa) introduces noise, but SGD tolerates noise\")\n",
    "            print(\"  - Compare this to FP16 naive above: same approach, different outcome, entirely due to range.\")\n",
    "        else:\n",
    "            print(\"BF16 naive converged but to a higher loss — precision may have limited final accuracy.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Step 2: Add FP32 master weights\n",
    "\n",
    "The first fix from the Micikevicius paper: keep an FP32 copy of parameters for the optimizer update.\n",
    "\n",
    "**The flow:**\n",
    "1. Forward pass uses FP16 parameters (for speed/memory).\n",
    "2. Backward produces FP16 gradients.\n",
    "3. Copy FP16 gradients to FP32 master parameters.\n",
    "4. Optimizer updates FP32 master weights.\n",
    "5. Copy updated FP32 weights back to FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 2: FP16 + FP32 master weights\n",
    "log_master = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_master[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 master-weights demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_master = SimpleMLP().to(device).half()  # FP16 for forward/backward\n",
    "\n",
    "    # Create FP32 master copy\n",
    "    master_params = [p.detach().clone().float().requires_grad_(True) for p in model_master.parameters()]\n",
    "    opt_master = torch.optim.Adam(master_params, lr=LR)\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_master.zero_grad(set_to_none=True)\n",
    "        model_master.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward in FP16\n",
    "        pred = model_master(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_master[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "        loss.backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master params\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float()\n",
    "\n",
    "        zf, mg = grad_stats(model_master)\n",
    "\n",
    "        # Update FP32 master weights\n",
    "        opt_master.step()\n",
    "\n",
    "        # Copy FP32 master -> FP16 model\n",
    "        for mp, p16 in zip(master_params, model_master.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_master[\"loss\"].append(float(loss))\n",
    "        log_master[\"zero_grad_frac\"].append(zf)\n",
    "        log_master[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights: status = {log_master['status']}\")\n",
    "    if log_master[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_master['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_master['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Step 3: Add loss scaling\n",
    "\n",
    "The second fix: multiply the loss by a scale factor $S$ before backward, then unscale gradients before the optimizer step. This prevents small gradients from underflowing to zero in FP16."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: FP16 + FP32 master weights + loss scaling\n",
    "log_scaled = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\"}\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    log_scaled[\"status\"] = \"skipped_no_cuda\"\n",
    "    print(\"Skipping FP16 loss-scaling demo: intended for CUDA FP16 behavior.\")\n",
    "else:\n",
    "    set_seed(42)\n",
    "    model_scaled = SimpleMLP().to(device).half()\n",
    "    master_params_s = [p.detach().clone().float().requires_grad_(True) for p in model_scaled.parameters()]\n",
    "    opt_scaled = torch.optim.Adam(master_params_s, lr=LR)\n",
    "\n",
    "    LOSS_SCALE = 2**13  # 8192 — a common starting point\n",
    "\n",
    "    for xb, yb in get_batches(PROG_STEPS):\n",
    "        opt_scaled.zero_grad(set_to_none=True)\n",
    "        model_scaled.zero_grad(set_to_none=True)\n",
    "\n",
    "        pred = model_scaled(xb.half())\n",
    "        loss = F.mse_loss(pred, yb.half())\n",
    "        if not torch.isfinite(loss):\n",
    "            log_scaled[\"status\"] = \"non_finite_loss\"\n",
    "            break\n",
    "\n",
    "        # Scale loss before backward\n",
    "        (loss * LOSS_SCALE).backward()\n",
    "\n",
    "        # Copy FP16 grads -> FP32 master, then UNSCALE\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            if p16.grad is not None:\n",
    "                mp.grad = p16.grad.float() / LOSS_SCALE\n",
    "\n",
    "        zf, mg = grad_stats(model_scaled)\n",
    "\n",
    "        opt_scaled.step()\n",
    "        for mp, p16 in zip(master_params_s, model_scaled.parameters()):\n",
    "            p16.data.copy_(mp.data.half())\n",
    "\n",
    "        log_scaled[\"loss\"].append(float(loss))\n",
    "        log_scaled[\"zero_grad_frac\"].append(zf)\n",
    "        log_scaled[\"median_grad\"].append(mg)\n",
    "\n",
    "    print(f\"FP16 + master weights + scaling: status = {log_scaled['status']}\")\n",
    "    if log_scaled[\"loss\"]:\n",
    "        print(f\"  Final loss = {log_scaled['loss'][-1]:.6f}\")\n",
    "        print(f\"  Zero-grad fraction: {log_scaled['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Step 4: PyTorch AMP (the clean version)\n",
    "\n",
    "Now replace all the manual work with PyTorch's `autocast` + `GradScaler`. Two extra lines of code.\n",
    "\n",
    "Note: autocast handles per-op dtype policies automatically. The model stays in FP32, and autocast temporarily casts operations during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: PyTorch AMP\n",
    "set_seed(42)\n",
    "model_amp = SimpleMLP().to(device).float()  # FP32 — autocast handles casting\n",
    "opt_amp = torch.optim.Adam(model_amp.parameters(), lr=LR)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "elif device.type == \"cpu\":\n",
    "    amp_dtype = torch.bfloat16\n",
    "    use_scaler = False\n",
    "else:  # mps (experimental AMP coverage)\n",
    "    amp_dtype = torch.float16\n",
    "    use_scaler = False\n",
    "\n",
    "scaler = GradScaler(enabled=(use_scaler and device.type == \"cuda\")) if use_scaler else None\n",
    "\n",
    "log_amp = {\"loss\": [], \"zero_grad_frac\": [], \"median_grad\": [], \"status\": \"ok\", \"dtype\": str(amp_dtype)}\n",
    "\n",
    "for xb, yb in get_batches(PROG_STEPS):\n",
    "    opt_amp.zero_grad(set_to_none=True)\n",
    "\n",
    "    with amp_autocast(device, amp_dtype, enabled=True):\n",
    "        pred = model_amp(xb)\n",
    "        loss = F.mse_loss(pred, yb)\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "        log_amp[\"status\"] = \"non_finite_loss\"\n",
    "        break\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt_amp)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        opt_amp.step()\n",
    "\n",
    "    zf, mg = grad_stats(model_amp)\n",
    "    log_amp[\"loss\"].append(float(loss))\n",
    "    log_amp[\"zero_grad_frac\"].append(zf)\n",
    "    log_amp[\"median_grad\"].append(mg)\n",
    "\n",
    "print(f\"PyTorch AMP ({log_amp['dtype']}): status = {log_amp['status']}\")\n",
    "if log_amp[\"loss\"]:\n",
    "    print(f\"  Final loss = {log_amp['loss'][-1]:.6f}\")\n",
    "    print(f\"  Zero-grad fraction: {log_amp['zero_grad_frac'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare all progressive approaches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "configs = [\n",
    "    (\"FP32 baseline\", log_fp32, \"C0\"),\n",
    "    (\"Naive FP16\", log_fp16_naive, \"C3\"),\n",
    "    (\"Naive BF16\", log_bf16_naive, \"C5\"),\n",
    "    (\"FP16 + master wts\", log_master, \"C1\"),\n",
    "    (\"FP16 + master + scaling\", log_scaled, \"C4\"),\n",
    "    (f\"PyTorch AMP ({log_amp['dtype']})\", log_amp, \"C2\"),\n",
    "]\n",
    "\n",
    "# Loss curves\n",
    "for name, log, color in configs:\n",
    "    if log[\"loss\"]:\n",
    "        axes[0].plot(log[\"loss\"], label=name, color=color, alpha=0.8)\n",
    "axes[0].set_title(\"Training loss\")\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"MSE loss\")\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "# Zero gradient fraction\n",
    "for name, log, color in configs:\n",
    "    if log[\"zero_grad_frac\"]:\n",
    "        axes[1].plot(log[\"zero_grad_frac\"], label=name, color=color, alpha=0.8)\n",
    "axes[1].set_title(\"Fraction of zero gradients\")\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"fraction\")\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# Median gradient magnitude\n",
    "for name, log, color in configs:\n",
    "    if log[\"median_grad\"]:\n",
    "        axes[2].plot(log[\"median_grad\"], label=name, color=color, alpha=0.8)\n",
    "axes[2].set_title(\"Median |gradient|\")\n",
    "axes[2].set_xlabel(\"step\")\n",
    "axes[2].set_ylabel(\"|grad|\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Progressive Mixed Precision: from naive FP16 to PyTorch AMP\", fontsize=12, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 What to observe\n",
    "\n",
    "**Loss curves:**\n",
    "- FP32 baseline should decrease smoothly.\n",
    "- Naive FP16 may diverge, stagnate, or produce NaN.\n",
    "- **Naive BF16 often trains successfully** — this is the dramatic demonstration of range vs precision. Same \"cast everything to 16-bit\" approach, but BF16's 8-bit exponent prevents the gradient underflow that kills FP16.\n",
    "- Adding master weights to FP16 typically helps convergence.\n",
    "- Adding loss scaling to FP16 reduces the fraction of zero gradients.\n",
    "- PyTorch AMP should match or beat the manual implementations.\n",
    "\n",
    "**Zero gradient fraction:**\n",
    "- In naive FP16, many gradients may be exactly zero (underflow).\n",
    "- In naive BF16, almost no gradients are zero — the exponent range is wide enough.\n",
    "- Loss scaling shifts the FP16 distribution, rescuing underflowed gradients.\n",
    "\n",
    "**Median gradient magnitude:**\n",
    "- Shows the \"signal strength\" available to the optimizer.\n",
    "- If it drops to zero, the model stops learning.\n",
    "- Compare FP16 vs BF16: even with coarser mantissa, BF16 preserves gradient *signal*.\n",
    "\n",
    "**The key lesson from this progressive build-up:**\n",
    "Naive FP16 fails. BF16 naive often works. But both benefit from FP32 master weights and proper AMP policies. The Micikevicius paper's three-part recipe (per-op policy + loss scaling + master weights) handles FP16 correctly. BF16 simplifies the picture by removing the need for loss scaling, but master weights and per-op policies still help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build an \"autocast operator policy table\" from your local PyTorch\n",
    "\n",
    "Instead of trusting a static table from the internet, we can probe your exact PyTorch version:\n",
    "1. Run each op with autocast **disabled** → observe output dtype.\n",
    "2. Run each op with autocast **enabled** → observe output dtype.\n",
    "3. Compare to see which ops autocast changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Enhanced operator policy probe\n",
    "\n",
    "def probe_ops(dev, amp_dtype):\n",
    "    a32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    b32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x128 = torch.randn(2, 128, device=dev, dtype=torch.float32)\n",
    "    x_big = torch.randn(2, 20000, device=dev, dtype=torch.float32)\n",
    "    w32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    bias32 = torch.randn(128, device=dev, dtype=torch.float32)\n",
    "    target32 = torch.randn(128, 128, device=dev, dtype=torch.float32)\n",
    "    x16 = x_big.to(amp_dtype)\n",
    "\n",
    "    ops = {\n",
    "        # Matmul-like (expect lower precision)\n",
    "        \"matmul (a @ b)\": lambda: a32 @ b32,\n",
    "        \"F.linear\": lambda: F.linear(a32, w32, bias32),\n",
    "        # Numerically sensitive (expect FP32)\n",
    "        \"F.softmax\": lambda: F.softmax(x128, dim=-1),\n",
    "        \"F.layer_norm\": lambda: F.layer_norm(x128, [x128.size(-1)]),\n",
    "        \"F.cross_entropy\": lambda: F.cross_entropy(a32[:10], torch.randint(0, 128, (10,), device=dev)),\n",
    "        \"F.mse_loss\": lambda: F.mse_loss(a32, target32),\n",
    "        \"torch.exp\": lambda: torch.exp(x128),\n",
    "        \"torch.log\": lambda: torch.log(x128.abs() + 1e-6),\n",
    "        \"torch.sum\": lambda: x_big.sum(),\n",
    "        \"torch.prod\": lambda: x_big[:, :10].prod(),\n",
    "        # Pass-through (expect input dtype)\n",
    "        \"F.relu\": lambda: F.relu(x_big),\n",
    "        \"torch.max\": lambda: x_big.max(),\n",
    "        \"torch.min\": lambda: x_big.min(),\n",
    "        \"torch.mean\": lambda: x_big.mean(),\n",
    "        \"F.dropout\": lambda: F.dropout(x_big, p=0.0, training=True),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for name, fn in ops.items():\n",
    "        # Without autocast\n",
    "        y_no = fn()\n",
    "        dt_no = str(y_no.dtype) if isinstance(y_no, torch.Tensor) else \"n/a\"\n",
    "        # With autocast (FP32 inputs)\n",
    "        with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "            y_ac = fn()\n",
    "        dt_ac = str(y_ac.dtype) if isinstance(y_ac, torch.Tensor) else \"n/a\"\n",
    "\n",
    "        # With autocast (16-bit inputs, where applicable)\n",
    "        dt_ac16 = \"\"\n",
    "        try:\n",
    "            # Replace x32 temporarily\n",
    "            ops_16 = {\n",
    "                \"F.relu\": lambda: F.relu(x16),\n",
    "                \"torch.max\": lambda: x16.max(),\n",
    "                \"torch.min\": lambda: x16.min(),\n",
    "                \"torch.mean\": lambda: x16.mean(),\n",
    "                \"torch.sum\": lambda: x16.sum(),\n",
    "            }\n",
    "            if name in ops_16:\n",
    "                with amp_autocast(dev, amp_dtype, enabled=True):\n",
    "                    y16 = ops_16[name]()\n",
    "                dt_ac16 = str(y16.dtype)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        changed = dt_no != dt_ac\n",
    "        rows.append({\n",
    "            \"op\": name,\n",
    "            \"no_autocast\": dt_no,\n",
    "            f\"autocast({amp_dtype})_fp32_input\": dt_ac,\n",
    "            \"16bit_input\": dt_ac16 if dt_ac16 else \"-\",\n",
    "            \"policy\": \"LOWER PREC\" if \"float16\" in dt_ac or \"bfloat16\" in dt_ac else (\"FP32\" if changed or dt_ac == \"torch.float32\" else \"pass-through\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "for amp_dt in [torch.float16, torch.bfloat16]:\n",
    "    if device.type == \"cuda\" and amp_dt is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n",
    "        continue\n",
    "    if device.type == \"cpu\" and amp_dt is torch.float16:\n",
    "        continue\n",
    "    if not supports_dtype_on_device(amp_dt, device):\n",
    "        continue\n",
    "    print(f\"\\n=== Autocast policy with amp_dtype={amp_dt} ===\")\n",
    "    display(probe_ops(device, amp_dt))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 The \"sum vs mean\" mystery\n",
    "\n",
    "Under autocast, `sum` produces FP32 output but `mean` stays in the input dtype. Why?\n",
    "\n",
    "**`sum`** is a reduction that accumulates values. Summing 20,000 BF16 values can easily exceed the representable range (BF16 max ~ $3.4 \\times 10^{38}$, but even FP16 max is only ~65,504). More critically, the rounding errors from adding many small values compound. PyTorch hardcodes `sum` to promote to FP32.\n",
    "\n",
    "**`mean`** inherently divides by $N$, keeping the output bounded between the min and max of the input. It cannot overflow by accumulation. PyTorch treats it as a pass-through — whatever dtype goes in, the same comes out.\n",
    "\n",
    "**`prod`** is similar to `sum` but even more extreme: multiplying many values can grow (or shrink) astronomically. Also forced to FP32.\n",
    "\n",
    "Let's verify this directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The sum vs mean mystery — direct probe\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    dtype_16bit = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    x16 = torch.randn(2, 20000, device=device, dtype=dtype_16bit)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype_16bit):\n",
    "        results = {\n",
    "            \"max\": x16.max().dtype,\n",
    "            \"min\": x16.min().dtype,\n",
    "            \"sum\": x16.sum().dtype,\n",
    "            \"mean\": x16.mean().dtype,\n",
    "            \"prod\": x16.prod().dtype,\n",
    "            \"exp\": x16.exp().dtype,\n",
    "            \"log\": x16.abs().log().dtype,\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for op, dt in results.items():\n",
    "        rows.append({\n",
    "            \"op\": op,\n",
    "            \"output_dtype\": str(dt),\n",
    "            \"promoted_to_fp32\": \"YES\" if dt == torch.float32 else \"no\",\n",
    "            \"reason\": {\n",
    "                \"max\": \"element-wise selection, no accumulation\",\n",
    "                \"min\": \"element-wise selection, no accumulation\",\n",
    "                \"sum\": \"ACCUMULATION: rounding errors compound over N values\",\n",
    "                \"mean\": \"bounded output (divides by N), no overflow risk\",\n",
    "                \"prod\": \"ACCUMULATION: multiplicative explosion/collapse\",\n",
    "                \"exp\": \"can produce extreme magnitudes (overflow risk)\",\n",
    "                \"log\": \"can produce extreme magnitudes (underflow risk)\",\n",
    "            }.get(op, \"\"),\n",
    "        })\n",
    "\n",
    "    display(pd.DataFrame(rows))\n",
    "else:\n",
    "    print(\"Run on CUDA to see the sum vs mean mystery in action.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Watch dtype flow through a transformer (4 configurations)\n",
    "\n",
    "This is the \"visceral\" version of the operator policy: instead of probing individual ops, we observe dtypes flowing through a real transformer model.\n",
    "\n",
    "We'll test all four combinations from the toy example (source1):\n",
    "\n",
    "| Config | Model params | Autocast | What to observe |\n",
    "|---|---|---|---|\n",
    "| A | FP32 | OFF | Everything FP32 (baseline) |\n",
    "| B | FP16/BF16 | OFF | Everything 16-bit (no policy) |\n",
    "| C | FP16/BF16 | ON | LayerNorm→FP32, Linear→16-bit, residuals→16-bit |\n",
    "| D | FP32 | ON | Linear→16-bit even with FP32 weights! LayerNorm stays FP32, residuals→FP32 (promotion) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tiny transformer model for dtype tracing\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        att = att.masked_fill(mask, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer=2, n_embd=128, n_heads=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(n_embd, n_heads, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"TinyGPT defined: 2-layer transformer for dtype tracing and training experiments.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dtype hooks + 4-configuration trace\n",
    "\n",
    "def install_dtype_hooks(model, watch=(nn.Linear, nn.LayerNorm, nn.Embedding)):\n",
    "    hooks, records = [], []\n",
    "    def make_hook(name):\n",
    "        def hook(m, inp, out):\n",
    "            def dt(x):\n",
    "                return str(x.dtype) if isinstance(x, torch.Tensor) else type(x).__name__\n",
    "            indt = dt(inp[0]) if isinstance(inp, (tuple, list)) and inp else dt(inp)\n",
    "            oudt = dt(out) if not isinstance(out, (tuple, list)) else dt(out[0])\n",
    "            records.append({\"module\": name, \"type\": type(m).__name__, \"in_dtype\": indt, \"out_dtype\": oudt})\n",
    "        return hook\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, watch):\n",
    "            hooks.append(m.register_forward_hook(make_hook(name)))\n",
    "    return hooks, records\n",
    "\n",
    "VOCAB = 128\n",
    "BLOCK = 64\n",
    "idx = torch.randint(0, VOCAB, (2, BLOCK), device=device)\n",
    "\n",
    "dtype_16 = torch.bfloat16\n",
    "if device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    dtype_16 = torch.float16\n",
    "elif device.type == \"mps\":\n",
    "    dtype_16 = torch.float16\n",
    "\n",
    "configs = [\n",
    "    (\"A: params=FP32, autocast=OFF\", torch.float32, False),\n",
    "    (\"B: params=16bit, autocast=OFF\", dtype_16, False),\n",
    "    (\"C: params=16bit, autocast=ON\",  dtype_16, True),\n",
    "    (\"D: params=FP32, autocast=ON\",   torch.float32, True),\n",
    "]\n",
    "\n",
    "for title, param_dt, use_ac in configs:\n",
    "    model = TinyGPT(VOCAB, BLOCK).to(device).to(param_dt)\n",
    "    hooks, rec = install_dtype_hooks(model)\n",
    "    ctx = amp_autocast(device, dtype_16, enabled=use_ac)\n",
    "    with torch.inference_mode(), ctx:\n",
    "        _ = model(idx)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    df = pd.DataFrame(rec)\n",
    "    df[\"count\"] = 1\n",
    "    summary = df.groupby([\"type\", \"in_dtype\", \"out_dtype\"], as_index=False)[\"count\"].sum()\n",
    "    summary = summary.sort_values([\"type\", \"in_dtype\", \"out_dtype\"])\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    display(summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 What the traces reveal\n",
    "\n",
    "**Config A (FP32, no autocast):** Everything is FP32. Baseline.\n",
    "\n",
    "**Config B (16-bit, no autocast):** Everything is 16-bit. No per-op policy. LayerNorm runs in 16-bit (risky for accumulation). Softmax runs in 16-bit (risky for overflow).\n",
    "\n",
    "**Config C (16-bit params, autocast ON):**\n",
    "- LayerNorm: input is 16-bit → **output is FP32** (autocast forces FP32 for normalization)\n",
    "- Linear after LayerNorm: **input is FP32 → output is 16-bit** (autocast casts FP32 weights/inputs to 16-bit for the matmul)\n",
    "- This is the key insight: autocast doesn't just \"use 16-bit everywhere.\" It routes different ops to different precisions.\n",
    "\n",
    "**Config D (FP32 params, autocast ON):**\n",
    "- Linear: **FP32 input → 16-bit output** (autocast temporarily casts FP32 weights to 16-bit!)\n",
    "- LayerNorm: FP32 → FP32 (stays in FP32, as it should)\n",
    "- Residual adds: 16-bit + FP32 → **FP32** (dtype promotion)\n",
    "- This is the \"standard\" AMP configuration: model stays in FP32, autocast handles per-op casting.\n",
    "\n",
    "```\n",
    "Config D data flow (FP32 params + autocast):\n",
    "\n",
    "[int64 tokens]\n",
    "       |\n",
    "  +----+---------------------+\n",
    "  |                          |\n",
    "[embed_tok] int64->fp32    [embed_pos] int64->fp32\n",
    "  |                          |\n",
    "  +-------- sum (fp32+fp32->fp32) --------+\n",
    "                                          |\n",
    "                                        [LN1] fp32->fp32\n",
    "                                          |\n",
    "                      +---------+---------+---------+\n",
    "                      |         |                   |\n",
    "                   [q_proj]  [k_proj]           [v_proj]\n",
    "                    fp32->bf16  fp32->bf16      fp32->bf16\n",
    "                      \\         |                 /\n",
    "                       \\        |                /\n",
    "                        +----[attn + softmax]----+   (bf16 compute)\n",
    "                                          |\n",
    "                                    [out_proj] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "                                          |\n",
    "                                        [LN2] fp32->fp32\n",
    "                                          |\n",
    "                                       [fc1] fp32->bf16\n",
    "                                          |\n",
    "                                       [fc2] bf16->bf16\n",
    "                                          |\n",
    "                (residual add: bf16 + fp32 -> fp32)   <- promotion!\n",
    "```\n",
    "\n",
    "The promotion at residual connections is a key feature: it prevents precision loss from accumulating through the network's residual stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Per-layer precision sensitivity: which parts of the transformer hurt most?\n",
    "\n",
    "The dtype hooks above show *what* dtype each layer uses. But a deeper question is: **which layers are most affected by the precision change?**\n",
    "\n",
    "We'll feed the same input through our TinyGPT in FP32 vs under autocast, and measure the per-module output error. This tells you which operations are precision-sensitive and why the autocast policy protects certain ops."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Per-layer output error: FP32 vs autocast\n",
    "\n",
    "import copy\n",
    "\n",
    "set_seed(0)\n",
    "sens_model = TinyGPT(VOCAB, BLOCK, n_layer=2, n_embd=128, n_heads=4, dropout=0.0).to(device).float()\n",
    "\n",
    "# Collect per-module outputs under FP32 and autocast\n",
    "def collect_outputs(model, idx, use_autocast_flag, amp_dt):\n",
    "    outputs = {}\n",
    "    hooks = []\n",
    "    def make_hook(name):\n",
    "        def hook(m, inp, out):\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                outputs[name] = out.detach().float().cpu().clone()\n",
    "            elif isinstance(out, (tuple, list)) and len(out) > 0 and isinstance(out[0], torch.Tensor):\n",
    "                outputs[name] = out[0].detach().float().cpu().clone()\n",
    "        return hook\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if name:  # skip root\n",
    "            hooks.append(m.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        ctx = amp_autocast(device, amp_dt, enabled=use_autocast_flag)\n",
    "        with ctx:\n",
    "            _ = model(idx)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    return outputs\n",
    "\n",
    "idx_sens = torch.randint(0, VOCAB, (2, BLOCK), device=device)\n",
    "\n",
    "out_fp32 = collect_outputs(sens_model, idx_sens, False, None)\n",
    "\n",
    "dtype_16 = torch.bfloat16\n",
    "if device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    dtype_16 = torch.float16\n",
    "elif device.type == \"mps\":\n",
    "    dtype_16 = torch.float16\n",
    "\n",
    "out_ac = collect_outputs(sens_model, idx_sens, True, dtype_16)\n",
    "\n",
    "# Compare\n",
    "rows = []\n",
    "for name in sorted(out_fp32.keys()):\n",
    "    if name not in out_ac:\n",
    "        continue\n",
    "    o32 = out_fp32[name]\n",
    "    oac = out_ac[name]\n",
    "    if o32.shape != oac.shape:\n",
    "        continue\n",
    "    abs_err = (oac - o32).abs()\n",
    "    rel_err = abs_err / (o32.abs() + 1e-8)\n",
    "    rows.append({\n",
    "        \"module\": name,\n",
    "        \"max_abs_err\": f\"{float(abs_err.max()):.2e}\",\n",
    "        \"mean_abs_err\": f\"{float(abs_err.mean()):.2e}\",\n",
    "        \"mean_rel_err\": f\"{float(rel_err.mean()):.2e}\",\n",
    "        \"max_rel_err\": f\"{float(rel_err.max()):.2e}\",\n",
    "    })\n",
    "\n",
    "df_sens = pd.DataFrame(rows)\n",
    "\n",
    "# Sort by mean_rel_err descending to show most sensitive layers first\n",
    "df_sens[\"_sort\"] = df_sens[\"mean_rel_err\"].apply(lambda x: float(x))\n",
    "df_sens = df_sens.sort_values(\"_sort\", ascending=False).drop(columns=[\"_sort\"])\n",
    "\n",
    "print(f\"Per-module output error: FP32 vs autocast({dtype_16})\")\n",
    "print(f\"{'='*70}\")\n",
    "display(df_sens)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Layers with HIGHER error are more precision-sensitive.\")\n",
    "print(\"  - LayerNorm, softmax, and final head outputs tend to show larger errors\")\n",
    "print(\"    because they involve reductions and normalization.\")\n",
    "print(\"  - Linear layers often show small errors because Tensor Cores accumulate in FP32.\")\n",
    "print(\"  - This explains why autocast's policy protects normalization and loss ops in FP32.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize per-layer sensitivity as a bar chart\n",
    "\n",
    "if len(rows) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, max(4, len(rows) * 0.3)))\n",
    "\n",
    "    names_plot = df_sens[\"module\"].tolist()\n",
    "    errors_plot = [float(x) for x in df_sens[\"mean_rel_err\"].tolist()]\n",
    "\n",
    "    # Color by module type\n",
    "    colors_plot = []\n",
    "    for n in names_plot:\n",
    "        if \"ln\" in n.lower() or \"norm\" in n.lower():\n",
    "            colors_plot.append(\"#e74c3c\")  # red for normalization\n",
    "        elif \"attn\" in n.lower() or \"qkv\" in n.lower() or \"proj\" in n.lower():\n",
    "            colors_plot.append(\"#3498db\")  # blue for attention\n",
    "        elif \"mlp\" in n.lower() or \"fc\" in n.lower():\n",
    "            colors_plot.append(\"#2ecc71\")  # green for MLP\n",
    "        elif \"head\" in n.lower():\n",
    "            colors_plot.append(\"#9b59b6\")  # purple for output head\n",
    "        else:\n",
    "            colors_plot.append(\"#95a5a6\")  # gray for other\n",
    "\n",
    "    y_pos = range(len(names_plot))\n",
    "    ax.barh(y_pos, errors_plot, color=colors_plot, alpha=0.8, edgecolor=\"white\")\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(names_plot, fontsize=7)\n",
    "    ax.set_xlabel(\"Mean relative error (autocast vs FP32)\")\n",
    "    ax.set_title(\"Per-layer precision sensitivity: which modules are most affected by autocast?\", fontsize=11)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_el = [\n",
    "        Patch(color=\"#e74c3c\", label=\"Normalization (most sensitive)\"),\n",
    "        Patch(color=\"#3498db\", label=\"Attention\"),\n",
    "        Patch(color=\"#2ecc71\", label=\"MLP/Linear\"),\n",
    "        Patch(color=\"#9b59b6\", label=\"Output head\"),\n",
    "        Patch(color=\"#95a5a6\", label=\"Other\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_el, loc=\"lower right\", fontsize=8)\n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gradient underflow and why loss scaling works\n",
    "\n",
    "We'll do a controlled experiment:\n",
    "1. Create a synthetic gradient distribution spanning many orders of magnitude.\n",
    "2. Cast it to FP16 and count how many values become exactly 0.\n",
    "3. Apply a scale factor $S$, cast, then unscale.\n",
    "\n",
    "This shows the core mechanism of loss scaling without needing a full training run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient underflow + rescue via scaling\n",
    "\n",
    "N = 200_000\n",
    "log10_mag = torch.empty(N).uniform_(-12, 0)  # 1e-12 to 1\n",
    "sign = torch.randint(0, 2, (N,)) * 2 - 1\n",
    "synthetic = (10 ** log10_mag) * sign\n",
    "synthetic = synthetic.to(torch.float32)\n",
    "\n",
    "rows = []\n",
    "for S_label, S in [(\"unscaled (S=1)\", 1), (\"S=2^10 (1024)\", 2**10), (\"S=2^13 (8192)\", 2**13), (\"S=2^16 (65536)\", 2**16)]:\n",
    "    scaled = synthetic * S\n",
    "    for dt in [torch.float16, torch.bfloat16]:\n",
    "        g = scaled.to(dt)\n",
    "        zeros = float((g == 0).float().mean())\n",
    "        infs = float(torch.isinf(g).float().mean())\n",
    "        rows.append({\n",
    "            \"scaling\": S_label,\n",
    "            \"dtype\": str(dt),\n",
    "            \"zero_frac\": f\"{zeros:.3f}\",\n",
    "            \"inf_frac\": f\"{infs:.4f}\",\n",
    "            \"preserved_frac\": f\"{1 - zeros - infs:.3f}\",\n",
    "        })\n",
    "\n",
    "pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize gradient distribution vs FP16 thresholds\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal = float(fi16.tiny)\n",
    "min_sub = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "vals = synthetic.abs().cpu().numpy()\n",
    "axes[0].hist(np.log10(vals + 1e-30), bins=200, alpha=0.7, color=\"steelblue\")\n",
    "axes[0].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=f\"FP16 min normal ({min_normal:.1e})\")\n",
    "axes[0].axvline(np.log10(min_sub), color=\"m\", ls=\":\", label=f\"FP16 min subnormal ({min_sub:.1e})\")\n",
    "axes[0].set_title(\"Unscaled |grad| distribution\")\n",
    "axes[0].set_xlabel(\"log10(|grad|)\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# After scaling by 2^13\n",
    "scaled_vals = (synthetic * 2**13).abs().cpu().numpy()\n",
    "axes[1].hist(np.log10(scaled_vals + 1e-30), bins=200, alpha=0.7, color=\"darkorange\")\n",
    "axes[1].axvline(np.log10(min_normal), color=\"r\", ls=\"--\", label=\"FP16 min normal\")\n",
    "axes[1].axvline(np.log10(float(fi16.max)), color=\"darkred\", ls=\"-.\", label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "axes[1].set_title(\"Scaled by 2^13: distribution shifts right\")\n",
    "axes[1].set_xlabel(\"log10(|grad| * S)\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Loss scaling shifts gradients into FP16's representable range\", fontsize=11, y=1.02)\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Underflow in a real backward pass\n",
    "\n",
    "Now we show the actual training failure mode with a tiny FP16 network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Real backward underflow demo\n",
    "\n",
    "def tiny_backward(use_scaling, scale=2**13):\n",
    "    model = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1)).to(device).half()\n",
    "    x = (torch.randn(256, 128, device=device) * 1e-3).half()\n",
    "    y = (torch.randn(256, 1, device=device) * 1e-3).half()\n",
    "    pred = model(x)\n",
    "    loss = ((pred - y)**2).mean()\n",
    "    if use_scaling:\n",
    "        (loss * scale).backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.div_(scale)\n",
    "    else:\n",
    "        loss.backward()\n",
    "    grads = torch.cat([p.grad.flatten().abs().float() for p in model.parameters() if p.grad is not None])\n",
    "    return float(loss), float((grads == 0).float().mean()), float(grads.median()), grads\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    loss0, z0, med0, g0 = tiny_backward(use_scaling=False)\n",
    "    loss1, z1, med1, g1 = tiny_backward(use_scaling=True)\n",
    "    display(pd.DataFrame([\n",
    "        {\"setting\": \"FP16, no scaling\", \"loss\": f\"{loss0:.6f}\", \"zero_grad_frac\": f\"{z0:.3f}\", \"median|grad|\": f\"{med0:.2e}\"},\n",
    "        {\"setting\": \"FP16, scaled+unscaled\", \"loss\": f\"{loss1:.6f}\", \"zero_grad_frac\": f\"{z1:.3f}\", \"median|grad|\": f\"{med1:.2e}\"},\n",
    "    ]))\n",
    "else:\n",
    "    print(\"Run on CUDA for the FP16 backward demo.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 The Micikevicius gradient histogram: where do real gradients live?\n",
    "\n",
    "The Micikevicius paper's most famous figure shows a histogram of gradient magnitudes during FP32 training, overlaid with FP16's representable range. Let's reproduce this analysis with our model.\n",
    "\n",
    "This visualization answers the question: *What fraction of the training signal would you lose by switching to FP16 or BF16?*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradient histogram analysis (Micikevicius-style)\n",
    "\n",
    "def collect_gradient_histogram(model_class, device, dtype=torch.float32, steps=20):\n",
    "    # Collect all gradient values from several training steps.\n",
    "    set_seed(42)\n",
    "    model = model_class().to(device).to(dtype)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    all_grads = []\n",
    "\n",
    "    for xb, yb in get_batches(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        pred = model(xb.to(dtype))\n",
    "        loss = F.mse_loss(pred, yb.to(dtype))\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                all_grads.append(p.grad.detach().float().flatten().cpu())\n",
    "        opt.step()\n",
    "\n",
    "    return torch.cat(all_grads)\n",
    "\n",
    "grads_fp32 = collect_gradient_histogram(SimpleMLP, device, torch.float32, steps=30)\n",
    "nonzero_grads = grads_fp32[grads_fp32 != 0]\n",
    "log_abs_grads = torch.log10(nonzero_grads.abs()).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: gradient histogram with FP16 thresholds\n",
    "ax = axes[0]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "min_normal_16 = float(fi16.tiny)\n",
    "min_sub_16 = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n",
    "\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=2, label=f\"FP16 min normal ({min_normal_16:.1e})\")\n",
    "ax.axvline(np.log10(min_sub_16), color=\"darkred\", ls=\":\", lw=1.5, label=f\"FP16 min subnormal ({min_sub_16:.1e})\")\n",
    "ax.axvline(np.log10(float(fi16.max)), color=\"orange\", ls=\"-.\", lw=1.5, label=f\"FP16 max ({fi16.max:.0f})\")\n",
    "\n",
    "# Shade the underflow zone\n",
    "ax.axvspan(ax.get_xlim()[0], np.log10(min_sub_16), alpha=0.15, color=\"red\", label=\"FP16 underflow zone\")\n",
    "\n",
    "ax.set_title(\"FP32 gradient magnitudes vs FP16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "# Right: same but with BF16 thresholds\n",
    "ax = axes[1]\n",
    "ax.hist(log_abs_grads, bins=200, alpha=0.7, color=\"steelblue\", edgecolor=\"none\", density=True)\n",
    "\n",
    "fi_bf16 = torch.finfo(torch.bfloat16)\n",
    "min_normal_bf16 = float(fi_bf16.tiny)\n",
    "\n",
    "ax.axvline(np.log10(min_normal_bf16), color=\"green\", ls=\"--\", lw=2, label=f\"BF16 min normal ({min_normal_bf16:.1e})\")\n",
    "ax.axvline(np.log10(min_normal_16), color=\"red\", ls=\"--\", lw=1.5, alpha=0.5, label=f\"FP16 min normal (for comparison)\")\n",
    "\n",
    "ax.set_title(\"Same gradients vs BF16 representable range\", fontsize=10)\n",
    "ax.set_xlabel(\"log10(|gradient|)\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.legend(fontsize=7, loc=\"upper left\")\n",
    "\n",
    "fig.suptitle(\"Micikevicius-style analysis: gradient magnitude distribution\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Quantify\n",
    "fp16_underflow_frac = float((nonzero_grads.abs() < min_normal_16).float().mean())\n",
    "bf16_underflow_frac = float((nonzero_grads.abs() < min_normal_bf16).float().mean())\n",
    "print(f\"\\nGradients collected: {len(nonzero_grads):,}\")\n",
    "print(f\"Fraction that would underflow in FP16: {fp16_underflow_frac:.4f} ({fp16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"Fraction that would underflow in BF16: {bf16_underflow_frac:.4f} ({bf16_underflow_frac*100:.2f}%)\")\n",
    "print(f\"\\nThis is why FP16 needs loss scaling and BF16 usually doesn't.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Dynamic loss scaling (GradScaler) in action\n",
    "\n",
    "Loss scaling has two failure modes:\n",
    "\n",
    "- **Scale too small** → doesn't rescue underflow (gradients still become 0 in FP16).\n",
    "- **Scale too large** → causes overflow (gradients become `inf`/`nan`).\n",
    "\n",
    "`GradScaler` automates this tradeoff: it tries to keep the scale as large as possible *without* overflow.\n",
    "\n",
    "In this demo we intentionally start with an absurdly large scale to trigger overflows, and watch GradScaler back off and skip optimizer steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# GradScaler overflow + step skipping demo (CUDA only)\n",
    "\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Run on CUDA to see GradScaler dynamically adjust scale and skip steps.\")\n",
    "else:\n",
    "    set_seed(0)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    model = nn.Linear(256, 256, bias=False).to(device).float()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Large inputs to create large (but finite) gradients.\n",
    "    x = (torch.randn(256, 256, device=device) * 5000).float()\n",
    "\n",
    "    try:\n",
    "        scaler = GradScaler(\n",
    "            init_scale=2**16,\n",
    "            growth_factor=2.0,\n",
    "            backoff_factor=0.5,\n",
    "            growth_interval=2,\n",
    "            enabled=True,\n",
    "        )\n",
    "    except TypeError:\n",
    "        scaler = GradScaler(enabled=True)\n",
    "        print(\"[warn] GradScaler init args not supported on this version; using defaults.\")\n",
    "\n",
    "    logs = []\n",
    "    for step in range(12):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scale_before = float(scaler.get_scale()) if hasattr(scaler, \"get_scale\") else float(\"nan\")\n",
    "\n",
    "        with amp_autocast(device, torch.float16, enabled=True):\n",
    "            y = model(x)  # FP16 matmul compute under autocast\n",
    "            # Force loss computation in FP32 to avoid forward overflow; we want overflow from scaling.\n",
    "            loss = (y.float() ** 2).mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale so we can inspect gradients in their true magnitude (and clip here if desired).\n",
    "        scaler.unscale_(opt)\n",
    "        grads = [p.grad for p in model.parameters() if p.grad is not None]\n",
    "        found_inf = bool(any((~torch.isfinite(g)).any().item() for g in grads))\n",
    "        max_abs_grad = float(torch.stack([g.detach().abs().max() for g in grads]).max())\n",
    "\n",
    "        w_before = model.weight.detach().float().clone()\n",
    "        scaler.step(opt)     # skipped if found_inf=True\n",
    "        scaler.update()\n",
    "        w_after = model.weight.detach().float()\n",
    "\n",
    "        scale_after = float(scaler.get_scale()) if hasattr(scaler, \"get_scale\") else float(\"nan\")\n",
    "        step_ran = bool((w_after - w_before).abs().max().item() != 0.0)\n",
    "\n",
    "        logs.append({\n",
    "            \"step\": step,\n",
    "            \"loss(fp32)\": float(loss.detach().cpu()),\n",
    "            \"scale_before\": scale_before,\n",
    "            \"found_inf\": found_inf,\n",
    "            \"optimizer_step_ran\": step_ran,\n",
    "            \"max_abs_grad(after_unscale)\": max_abs_grad,\n",
    "            \"scale_after\": scale_after,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(logs)\n",
    "    display(df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    ax.plot(df[\"step\"], df[\"scale_before\"], marker=\"o\", label=\"scale_before\")\n",
    "    ax.plot(df[\"step\"], df[\"scale_after\"], marker=\"o\", ls=\"--\", label=\"scale_after\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.set_ylabel(\"scale (log)\")\n",
    "    ax.set_title(\"GradScaler: backoff on overflow + step skipping\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[\"step\"], df[\"found_inf\"].astype(int), color=\"red\", alpha=0.3, lw=2, label=\"found_inf\")\n",
    "    ax2.set_ylabel(\"found_inf (0/1)\")\n",
    "    plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Weight update stagnation (why FP32 master weights matter)\n",
    "\n",
    "Even if you avoid underflow, you can lose learning signal if weight updates are **below the ULP** of the weight's dtype.\n",
    "\n",
    "If $w \\approx 1$ in FP16, the ULP is ~$10^{-3}$. Any update $\\Delta w < 10^{-3}$ is rounded away."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weight stagnation demo\n",
    "\n",
    "def apply_updates(dtype, w0=1.0, delta=1e-5, steps=2000):\n",
    "    w = torch.tensor(w0, dtype=dtype)\n",
    "    changed = 0\n",
    "    for _ in range(steps):\n",
    "        w_new = w - torch.tensor(delta, dtype=dtype)\n",
    "        changed += int(w_new.item() != w.item())\n",
    "        w = w_new\n",
    "    return {\n",
    "        \"dtype\": str(dtype),\n",
    "        \"w0\": w0,\n",
    "        \"delta\": f\"{delta:.0e}\",\n",
    "        \"steps\": steps,\n",
    "        \"steps_where_w_changed\": changed,\n",
    "        \"final_w\": f\"{float(w):.6f}\",\n",
    "        \"expected_final\": f\"{w0 - delta * steps:.6f}\",\n",
    "        \"ulp_at_1.0\": f\"{_ulp_at_one(dtype):.2e}\",\n",
    "    }\n",
    "\n",
    "rows = [apply_updates(dt) for dt in [torch.float16, torch.bfloat16, torch.float32]]\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"\\nFP16: delta=1e-5 is below ULP at 1.0 (~1e-3). Weight NEVER changes.\")\n",
    "print(\"BF16: delta=1e-5 is below ULP at 1.0 (~8e-3). Weight NEVER changes.\")\n",
    "print(\"FP32: delta=1e-5 is above ULP at 1.0 (~1e-7). Weight changes every step.\")\n",
    "print(\"\\nThis is why optimizers need FP32 master weights.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 The main event: train a tiny causal LM under different precision regimes\n",
    "\n",
    "We'll train TinyGPT on a character-level next-token prediction task using an in-notebook corpus.\n",
    "\n",
    "**Why character-level?** No external downloads, stable, deterministic, and still exercises the transformer mechanics that matter for autocast (attention, layernorm, softmax, embeddings).\n",
    "\n",
    "**We will compare (device-dependent):**\n",
    "- **Always:** FP32 baseline\n",
    "- **CUDA:** naive FP16/BF16 (cast-everything baselines) + AMP FP16 (with GradScaler) + AMP BF16 (if supported)\n",
    "- **CPU:** BF16 autocast (numerics-focused; speedups vary by CPU/kernel support)\n",
    "- **MPS:** FP16 autocast (+ BF16 autocast if your MPS backend supports it)\n",
    "\n",
    "Before the full training suite, we'll also do:\n",
    "- A **single-batch numerical drift** comparison: FP32 vs autocast (loss + gradient deltas).\n",
    "- A qualitative **text-generation sanity check** from each successfully trained regime.\n",
    "\n",
    "**We will log:**\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Gradient norm + exact-zero gradient fraction (underflow proxy)\n",
    "- Step time + throughput (tokens/s)\n",
    "- CUDA peak memory (if CUDA)\n",
    "- GradScaler scale (for FP16 AMP)\n",
    "\n",
    "Implementation detail: we keep the tokenized corpus on the selected device and sample batches with vectorized indexing, so the step-time graphs are not dominated by Python loops or host→device copies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tiny corpus + character-level tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Use the repo's reference material as training text (no external downloads).\n",
    "# Falls back to a small built-in corpus if the files aren't present.\n",
    "FALLBACK_LINES = [\n",
    "    \"Autocast is not a global cast: it is an operator policy that routes each operation to the right precision.\",\n",
    "    \"Matmuls/linears are the primary AMP targets because they map cleanly to Tensor Cores with FP32 accumulation.\",\n",
    "    \"Softmax, layer normalization, exp/log, and many reductions are numerically sensitive and often run in FP32.\",\n",
    "    \"FP16 has narrow exponent range → gradient underflow; BF16 keeps FP32 exponent range → underflow is rare.\",\n",
    "    \"GradScaler implements dynamic loss scaling: scale loss → backward → unscale grads → step → update scale.\",\n",
    "]\n",
    "\n",
    "def _read_text(path: str) -> str | None:\n",
    "    try:\n",
    "        return Path(path).read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "sources_used = []\n",
    "parts = []\n",
    "for p in [\"sources/source4.md\", \"sources/source5.md\"]:\n",
    "    t = _read_text(p)\n",
    "    if t:\n",
    "        parts.append(t)\n",
    "        sources_used.append(p)\n",
    "\n",
    "raw = \"\\n\".join(FALLBACK_LINES).strip()\n",
    "if parts:\n",
    "    raw = raw + \"\\n\\n\" + \"\\n\\n\".join(parts)\n",
    "\n",
    "# Minimal cleanup: remove code fences (if any) and normalize whitespace.\n",
    "raw = raw.replace(\"\\r\\n\", \"\\n\")\n",
    "raw = re.sub(r\"```.*?```\", \"\", raw, flags=re.S)\n",
    "raw = re.sub(r\"[ \\t]+\", \" \", raw)\n",
    "raw = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw)\n",
    "corpus = raw.strip()\n",
    "\n",
    "# Keep runtime predictable: cap corpus size and repeat if too small.\n",
    "TARGET_CHARS = 120_000\n",
    "if len(corpus) < TARGET_CHARS:\n",
    "    repeats = (TARGET_CHARS // max(len(corpus), 1)) + 1\n",
    "    corpus = ((corpus + \"\\n\") * repeats)[:TARGET_CHARS]\n",
    "else:\n",
    "    corpus = corpus[:TARGET_CHARS]\n",
    "\n",
    "chars = sorted(set(corpus))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode_tokens(ids):\n",
    "    return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "data = torch.tensor(encode(corpus), dtype=torch.long).to(device)\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_data):,}, Val tokens: {len(val_data):,}\")\n",
    "print(f\"Corpus sources used: {sources_used if sources_used else ['fallback_only']}\")\n",
    "print(f\"Sample: {decode_tokens(train_data[:80].tolist())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Batch sampling + evaluation\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    # Vectorized slicing (no Python loops) and stays on `device`.\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    max_start = src.size(0) - block_size - 1\n",
    "    ix = torch.randint(0, max_start, (batch_size,), device=src.device)\n",
    "    offsets = torch.arange(block_size, device=src.device).unsqueeze(0)\n",
    "    x = src[ix.unsqueeze(1) + offsets]\n",
    "    y = src[ix.unsqueeze(1) + offsets + 1]\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size, iters=20, use_autocast=False, amp_dtype=None):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            with amp_autocast(device, amp_dtype, enabled=use_autocast):\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "            losses.append(float(loss))\n",
    "        out[split] = float(np.mean(losses))\n",
    "    model.train()\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.0 A microscope view: what autocast changes numerically (one fixed batch)\n",
    "\n",
    "Before we look at full training curves, let's make autocast *concrete*:\n",
    "\n",
    "- Take the **same** model initialization and the **same** batch.\n",
    "- Compute loss + gradients in strict **FP32**.\n",
    "- Compute loss + gradients under **autocast** (FP16/BF16) with **FP32 parameters** (the usual AMP setup).\n",
    "- Quantify the deltas.\n",
    "\n",
    "This is the most direct way to understand \"mixed precision\": it introduces **small, structured numerical error** in specific parts of the forward/backward graph. The goal of AMP is not \"no error\" — it's \"bounded error that doesn't break training, in exchange for speed/memory gains\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Single-batch numerical drift: FP32 vs autocast\n",
    "import copy\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "# A tiny fixed batch to make this fast and deterministic.\n",
    "DRIFT_BS = 4\n",
    "DRIFT_BLOCK = 64\n",
    "x_drift, y_drift = get_batch(\"train\", DRIFT_BS, DRIFT_BLOCK)\n",
    "\n",
    "base = TinyGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=DRIFT_BLOCK,\n",
    "    n_layer=2,\n",
    "    n_embd=128,\n",
    "    n_heads=4,\n",
    "    dropout=0.0,\n",
    ").to(device).float()\n",
    "\n",
    "def loss_and_grads(model, x, y, use_autocast=False, amp_dtype=None):\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    with amp_autocast(device, amp_dtype, enabled=use_autocast):\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "    loss.backward()\n",
    "    grads = {n: p.grad.detach().float().cpu().clone() for n, p in model.named_parameters() if p.grad is not None}\n",
    "    return float(loss.detach().cpu()), grads\n",
    "\n",
    "loss_fp32, grads_fp32 = loss_and_grads(base, x_drift, y_drift, use_autocast=False, amp_dtype=None)\n",
    "\n",
    "def compare_grads(grads_ref, grads_test):\n",
    "    names = sorted(grads_ref.keys())\n",
    "    v0 = torch.cat([grads_ref[n].flatten() for n in names])\n",
    "    v1 = torch.cat([grads_test[n].flatten() for n in names])\n",
    "    rel_l2 = float((v1 - v0).norm() / (v0.norm() + 1e-12))\n",
    "    cos = float(F.cosine_similarity(v0, v1, dim=0))\n",
    "\n",
    "    per_param = []\n",
    "    for n in names:\n",
    "        g0 = grads_ref[n]\n",
    "        g1 = grads_test[n]\n",
    "        denom = float(g0.norm()) + 1e-12\n",
    "        per_param.append({\n",
    "            \"param\": n,\n",
    "            \"ref_norm\": float(g0.norm()),\n",
    "            \"rel_l2\": float((g1 - g0).norm() / denom),\n",
    "            \"max_abs_diff\": float((g1 - g0).abs().max()),\n",
    "        })\n",
    "    df = pd.DataFrame(per_param)\n",
    "    summary = {\n",
    "        \"grad_rel_l2\": rel_l2,\n",
    "        \"grad_cosine\": cos,\n",
    "        \"param_rel_l2_median\": float(df[\"rel_l2\"].median()),\n",
    "        \"param_rel_l2_p90\": float(df[\"rel_l2\"].quantile(0.90)),\n",
    "        \"param_rel_l2_max\": float(df[\"rel_l2\"].max()),\n",
    "    }\n",
    "    return df, summary\n",
    "\n",
    "candidates = []\n",
    "for dt in [torch.float16, torch.bfloat16]:\n",
    "    if dt is torch.float16 and device.type == \"cpu\":\n",
    "        continue\n",
    "    if device.type == \"cuda\" and dt is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n",
    "        continue\n",
    "    if not supports_dtype_on_device(dt, device):\n",
    "        continue\n",
    "    candidates.append(dt)\n",
    "\n",
    "rows = []\n",
    "per_param_dfs = {}\n",
    "\n",
    "print(f\"Reference FP32 loss: {loss_fp32:.6f}\")\n",
    "\n",
    "for amp_dt in candidates:\n",
    "    name = str(amp_dt).replace(\"torch.\", \"\")\n",
    "    try:\n",
    "        m = copy.deepcopy(base)\n",
    "        loss_amp, grads_amp = loss_and_grads(m, x_drift, y_drift, use_autocast=True, amp_dtype=amp_dt)\n",
    "        df_param, s = compare_grads(grads_fp32, grads_amp)\n",
    "        per_param_dfs[name] = df_param\n",
    "        rows.append({\n",
    "            \"amp_dtype\": name,\n",
    "            \"status\": \"ok\",\n",
    "            \"loss_amp\": loss_amp,\n",
    "            \"loss_abs_diff\": abs(loss_amp - loss_fp32),\n",
    "            \"loss_rel_diff\": abs(loss_amp - loss_fp32) / max(abs(loss_fp32), 1e-12),\n",
    "            **s,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"amp_dtype\": name,\n",
    "            \"status\": f\"failed: {type(e).__name__}\",\n",
    "            \"loss_amp\": float(\"nan\"),\n",
    "            \"loss_abs_diff\": float(\"nan\"),\n",
    "            \"loss_rel_diff\": float(\"nan\"),\n",
    "            \"grad_rel_l2\": float(\"nan\"),\n",
    "            \"grad_cosine\": float(\"nan\"),\n",
    "            \"param_rel_l2_median\": float(\"nan\"),\n",
    "            \"param_rel_l2_p90\": float(\"nan\"),\n",
    "            \"param_rel_l2_max\": float(\"nan\"),\n",
    "        })\n",
    "        print(f\"[warn] autocast drift compare failed for {amp_dt}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n",
    "\n",
    "for amp_name, df_param in per_param_dfs.items():\n",
    "    print(f\"\\nTop gradient-relative-error parameters for autocast({amp_name}) vs FP32:\")\n",
    "    display(df_param.sort_values('rel_l2', ascending=False).head(10))\n",
    "\n",
    "if per_param_dfs:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for amp_name, df_param in per_param_dfs.items():\n",
    "        plt.hist(np.log10(df_param[\"rel_l2\"].to_numpy() + 1e-20), bins=40, alpha=0.5, label=amp_name)\n",
    "    plt.title(\"Per-parameter gradient relative L2 error (autocast vs FP32)\")\n",
    "    plt.xlabel(\"log10(rel_l2 + 1e-20)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training infrastructure\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    name: str\n",
    "    steps: int = 200\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 64\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    use_autocast: bool = False\n",
    "    amp_dtype: torch.dtype = None\n",
    "    use_grad_scaler: bool = False\n",
    "    param_dtype: torch.dtype = torch.float32\n",
    "    eval_interval: int = 50\n",
    "    eval_iters: int = 10\n",
    "\n",
    "def global_grad_norm(model):\n",
    "    total_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_sq += float(p.grad.detach().float().norm())**2\n",
    "    return math.sqrt(total_sq)\n",
    "\n",
    "def global_zero_grad_frac(model):\n",
    "    zeros = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g = p.grad.detach()\n",
    "        zeros += int((g == 0).sum())\n",
    "        total += g.numel()\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "def train_one(cfg):\n",
    "    set_seed(42)\n",
    "    model = TinyGPT(\n",
    "        vocab_size=vocab_size, block_size=cfg.block_size,\n",
    "        n_layer=2, n_embd=128, n_heads=4, dropout=0.0,\n",
    "    ).to(device).to(cfg.param_dtype)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scaler = GradScaler(enabled=True) if (cfg.use_grad_scaler and device.type == \"cuda\") else None\n",
    "\n",
    "    logs = {\n",
    "        \"step\": [], \"train_loss\": [], \"grad_norm\": [], \"zero_grad_frac\": [],\n",
    "        \"step_time_ms\": [], \"tokens_per_s\": [], \"scale\": [],\n",
    "        \"cuda_mem_mb\": [], \"val_step\": [], \"val_loss\": [],\n",
    "    }\n",
    "    status = \"ok\"\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    tokens_per_step = cfg.batch_size * cfg.block_size\n",
    "\n",
    "    for step in range(cfg.steps):\n",
    "        # GPU kernels are async; synchronize so step_time_ms reflects actual compute.\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        x, y = get_batch(\"train\", cfg.batch_size, cfg.block_size)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with amp_autocast(device, cfg.amp_dtype, enabled=cfg.use_autocast):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            status = \"non_finite_loss\"\n",
    "            break\n",
    "\n",
    "        if scaler is None:\n",
    "            loss.backward()\n",
    "            grad_norm = global_grad_norm(model)\n",
    "            zero_frac = global_zero_grad_frac(model)\n",
    "            optimizer.step()\n",
    "            scale_val = float(\"nan\")\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = global_grad_norm(model)\n",
    "            zero_frac = global_zero_grad_frac(model)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scale_val = float(scaler.get_scale())\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        dt = max(time.perf_counter() - t0, 1e-12)\n",
    "        logs[\"step\"].append(step)\n",
    "        logs[\"train_loss\"].append(float(loss))\n",
    "        logs[\"grad_norm\"].append(float(grad_norm))\n",
    "        logs[\"zero_grad_frac\"].append(float(zero_frac))\n",
    "        logs[\"step_time_ms\"].append(dt * 1000)\n",
    "        logs[\"tokens_per_s\"].append(tokens_per_step / dt)\n",
    "        logs[\"scale\"].append(scale_val)\n",
    "        logs[\"cuda_mem_mb\"].append(torch.cuda.max_memory_allocated() / 1024**2 if device.type == \"cuda\" else float(\"nan\"))\n",
    "\n",
    "        if cfg.eval_interval and (step % cfg.eval_interval == 0 or step == cfg.steps - 1):\n",
    "            try:\n",
    "                ev = estimate_loss(\n",
    "                    model,\n",
    "                    cfg.block_size,\n",
    "                    cfg.batch_size,\n",
    "                    cfg.eval_iters,\n",
    "                    use_autocast=cfg.use_autocast,\n",
    "                    amp_dtype=cfg.amp_dtype,\n",
    "                )\n",
    "                logs[\"val_step\"].append(step)\n",
    "                logs[\"val_loss\"].append(ev[\"val\"])\n",
    "            except Exception:\n",
    "                logs[\"val_step\"].append(step)\n",
    "                logs[\"val_loss\"].append(float(\"nan\"))\n",
    "\n",
    "    # Save final weights for downstream analysis (e.g., text generation).\n",
    "    # Store as FP32 on CPU for portability and to avoid holding multiple GPU copies.\n",
    "    state_dict_fp32 = {k: v.detach().float().cpu() for k, v in model.state_dict().items()}\n",
    "    return {\"config\": cfg, \"status\": status, \"logs\": logs, \"state_dict\": state_dict_fp32}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define experiment suite\n",
    "\n",
    "FAST_DEV_RUN = (device.type != \"cuda\")\n",
    "BASE_STEPS = 60 if FAST_DEV_RUN else 300\n",
    "\n",
    "suite = [\n",
    "    TrainConfig(name=\"fp32\", steps=BASE_STEPS, param_dtype=torch.float32),\n",
    "]\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"fp16_naive\", steps=BASE_STEPS,\n",
    "        param_dtype=torch.float16,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"bf16_naive\", steps=BASE_STEPS,\n",
    "            param_dtype=torch.bfloat16,\n",
    "        ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16_no_scaler\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=False, param_dtype=torch.float32,\n",
    "    ))\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_fp16\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.float16,\n",
    "        use_grad_scaler=True, param_dtype=torch.float32,\n",
    "    ))\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_bf16\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "            param_dtype=torch.float32,\n",
    "        ))\n",
    "elif device.type == \"cpu\":\n",
    "    suite.append(TrainConfig(\n",
    "        name=\"amp_bf16_cpu\", steps=BASE_STEPS,\n",
    "        use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "        param_dtype=torch.float32,\n",
    "    ))\n",
    "elif device.type == \"mps\":\n",
    "    if supports_dtype_on_device(torch.float16, device):\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_fp16_mps\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.float16,\n",
    "            use_grad_scaler=False, param_dtype=torch.float32,\n",
    "        ))\n",
    "    if supports_dtype_on_device(torch.bfloat16, device):\n",
    "        suite.append(TrainConfig(\n",
    "            name=\"amp_bf16_mps\", steps=BASE_STEPS,\n",
    "            use_autocast=True, amp_dtype=torch.bfloat16,\n",
    "            use_grad_scaler=False, param_dtype=torch.float32,\n",
    "        ))\n",
    "\n",
    "print(\"Planned experiments:\")\n",
    "for cfg in suite:\n",
    "    print(f\"  {cfg.name}: params={cfg.param_dtype}, autocast={cfg.use_autocast} {cfg.amp_dtype}, scaler={cfg.use_grad_scaler}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run all experiments\n",
    "\n",
    "def _empty_logs():\n",
    "    return {\n",
    "        \"step\": [], \"train_loss\": [], \"grad_norm\": [], \"zero_grad_frac\": [],\n",
    "        \"step_time_ms\": [], \"tokens_per_s\": [], \"scale\": [],\n",
    "        \"cuda_mem_mb\": [], \"val_step\": [], \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "def _safe_ppl(loss):\n",
    "    try:\n",
    "        return float(math.exp(float(loss)))\n",
    "    except OverflowError:\n",
    "        return float(\"inf\")\n",
    "\n",
    "results = []\n",
    "for cfg in suite:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running: {cfg.name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    try:\n",
    "        res = train_one(cfg)\n",
    "        print(f\"  Status: {res['status']}, Steps: {len(res['logs']['step'])}\")\n",
    "        if res['logs']['train_loss']:\n",
    "            print(f\"  Final train loss: {res['logs']['train_loss'][-1]:.4f}\")\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        print(f\"  Exception: {type(e).__name__}: {e}\")\n",
    "        results.append({\n",
    "            \"config\": cfg,\n",
    "            \"status\": f\"exception: {type(e).__name__}\",\n",
    "            \"logs\": _empty_logs(),\n",
    "            \"state_dict\": None,\n",
    "        })\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for r in results:\n",
    "    cfg, logs = r[\"config\"], r[\"logs\"]\n",
    "    n = len(logs[\"step\"])\n",
    "    final_train_loss = logs[\"train_loss\"][-1] if n else None\n",
    "    final_val_loss = logs[\"val_loss\"][-1] if logs[\"val_loss\"] else None\n",
    "    summary_rows.append({\n",
    "        \"name\": cfg.name,\n",
    "        \"status\": r[\"status\"],\n",
    "        \"steps\": n,\n",
    "        \"final_train_loss\": f\"{final_train_loss:.4f}\" if final_train_loss is not None else \"n/a\",\n",
    "        \"final_train_ppl\": f\"{_safe_ppl(final_train_loss):.2f}\" if final_train_loss is not None else \"n/a\",\n",
    "        \"final_val_loss\": f\"{final_val_loss:.4f}\" if final_val_loss is not None else \"n/a\",\n",
    "        \"final_val_ppl\": f\"{_safe_ppl(final_val_loss):.2f}\" if final_val_loss is not None else \"n/a\",\n",
    "        \"mean_step_ms\": f\"{np.mean(logs['step_time_ms']):.1f}\" if n else \"n/a\",\n",
    "        \"mean_tok/s\": f\"{np.mean(logs['tokens_per_s']):.0f}\" if n else \"n/a\",\n",
    "        \"peak_cuda_MB\": f\"{np.nanmax(logs['cuda_mem_mb']):.1f}\" if device.type == \"cuda\" and n else \"n/a\",\n",
    "    })\n",
    "\n",
    "print(\"\\n\\n=== Summary ===\")\n",
    "display(pd.DataFrame(summary_rows))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Training + Validation loss curves (annotated)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "color_map = {\n",
    "    \"fp32\": \"C0\", \"fp16_naive\": \"C3\", \"bf16_naive\": \"C5\",\n",
    "    \"amp_fp16_no_scaler\": \"C6\", \"amp_fp16\": \"C4\", \"amp_bf16\": \"C1\",\n",
    "    \"amp_bf16_cpu\": \"C1\",\n",
    "    \"amp_fp16_mps\": \"C4\", \"amp_bf16_mps\": \"C1\",\n",
    "}\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    suffix = f\" ({r['status']})\" if r[\"status\"] != \"ok\" else \"\"\n",
    "    color = color_map.get(name, \"C7\")\n",
    "    ls = \"--\" if \"naive\" in name else \"-\"\n",
    "    if logs[\"train_loss\"]:\n",
    "        axes[0].plot(logs[\"step\"], logs[\"train_loss\"], label=f\"{name}{suffix}\",\n",
    "                     alpha=0.85, color=color, linestyle=ls, linewidth=1.5 if \"naive\" not in name else 1.0)\n",
    "    if logs[\"val_loss\"]:\n",
    "        axes[1].plot(logs[\"val_step\"], logs[\"val_loss\"], marker=\"o\", ms=5, label=f\"{name}{suffix}\",\n",
    "                     alpha=0.85, color=color, linestyle=ls)\n",
    "\n",
    "axes[0].set_title(\"Training loss vs step\", fontsize=11)\n",
    "axes[0].set_xlabel(\"step\")\n",
    "axes[0].set_ylabel(\"cross-entropy loss\")\n",
    "axes[0].legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "axes[1].set_title(\"Validation loss (periodic eval)\", fontsize=11)\n",
    "axes[1].set_xlabel(\"step\")\n",
    "axes[1].set_ylabel(\"cross-entropy loss\")\n",
    "axes[1].legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "fig.suptitle(\"TinyGPT Training: How precision regime affects convergence\", fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"What to look for:\")\n",
    "print(\"  - FP32 (solid blue): smooth reference curve\")\n",
    "print(\"  - Naive FP16 (dashed red): may stagnate, diverge, or NaN\")\n",
    "print(\"  - Naive BF16 (dashed olive): often converges — BF16 range prevents gradient death\")\n",
    "print(\"  - AMP variants (solid): should track FP32 closely, showing that AMP preserves quality\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Step time + throughput\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    axes[0].plot(logs[\"step\"], logs[\"step_time_ms\"], label=name, alpha=0.7)\n",
    "    axes[1].plot(logs[\"step\"], logs[\"tokens_per_s\"], label=name, alpha=0.7)\n",
    "\n",
    "axes[0].set_title(\"Step time (ms)\")\n",
    "axes[0].set_xlabel(\"step\"); axes[0].set_ylabel(\"ms\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"Throughput (tokens/s)\")\n",
    "axes[1].set_xlabel(\"step\"); axes[1].set_ylabel(\"tokens/s\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Gradient norms across precision regimes\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    color = color_map.get(name, \"C7\")\n",
    "    ls = \"--\" if \"naive\" in name else \"-\"\n",
    "    plt.plot(logs[\"step\"], logs[\"grad_norm\"], label=name, alpha=0.75, color=color, linestyle=ls)\n",
    "\n",
    "# Add FP16 min normal threshold line\n",
    "fi16 = torch.finfo(torch.float16)\n",
    "plt.axhline(float(fi16.tiny), color=\"red\", ls=\":\", alpha=0.4, label=f\"FP16 min normal ({fi16.tiny:.1e})\")\n",
    "\n",
    "plt.title(\"Gradient L2 norm vs step (after unscaling)\", fontsize=11)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"||grad||_2\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"If gradients fall below the FP16 min normal line, they underflow to zero in FP16.\")\n",
    "print(\"BF16 gradients essentially never underflow (BF16 min normal ≈ 1.2e-38).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: Exact-zero gradient fraction (a proxy for underflow / dead signal)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for r in results:\n",
    "    name = r[\"config\"].name\n",
    "    logs = r[\"logs\"]\n",
    "    if not logs[\"step\"]:\n",
    "        continue\n",
    "    if \"zero_grad_frac\" not in logs:\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], logs[\"zero_grad_frac\"], label=name, alpha=0.75)\n",
    "\n",
    "plt.title(\"Fraction of gradients that are exactly 0\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"zero_grad_frac\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: GradScaler dynamic scale (FP16 AMP only)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plotted = False\n",
    "for r in results:\n",
    "    if not r[\"config\"].use_grad_scaler:\n",
    "        continue\n",
    "    logs = r[\"logs\"]\n",
    "    scale = np.array(logs[\"scale\"], dtype=np.float64)\n",
    "    if len(scale) == 0 or np.all(np.isnan(scale)):\n",
    "        continue\n",
    "    plt.plot(logs[\"step\"], scale, label=r[\"config\"].name)\n",
    "    plotted = True\n",
    "\n",
    "if plotted:\n",
    "    plt.title(\"GradScaler dynamic scale factor\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"scale\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No GradScaler data to plot (did amp_fp16 run?)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot: CUDA peak memory\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    for r in results:\n",
    "        logs = r[\"logs\"]\n",
    "        if not logs[\"step\"]:\n",
    "            continue\n",
    "        ax.plot(logs[\"step\"], logs[\"cuda_mem_mb\"], label=r[\"config\"].name, alpha=0.7)\n",
    "    ax.set_title(\"Peak CUDA memory allocated (MB)\")\n",
    "    ax.set_xlabel(\"step\"); ax.set_ylabel(\"MB\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"CUDA not available; skipping memory plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary bar charts: compare all experiments at a glance\n",
    "\n",
    "completed = [r for r in results if r[\"status\"] == \"ok\" and r[\"logs\"][\"train_loss\"]]\n",
    "\n",
    "if len(completed) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    names = [r[\"config\"].name for r in completed]\n",
    "    x_pos = np.arange(len(names))\n",
    "\n",
    "    # 1. Final training loss\n",
    "    final_losses = [r[\"logs\"][\"train_loss\"][-1] for r in completed]\n",
    "    colors = [color_map.get(r[\"config\"].name, \"C7\") for r in completed]\n",
    "\n",
    "    axes[0].bar(x_pos, final_losses, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[0].set_ylabel(\"Final train loss\")\n",
    "    axes[0].set_title(\"Final Training Loss (lower is better)\")\n",
    "\n",
    "    # 2. Mean step time\n",
    "    mean_times = [np.mean(r[\"logs\"][\"step_time_ms\"]) for r in completed]\n",
    "    axes[1].bar(x_pos, mean_times, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "    axes[1].set_ylabel(\"Mean step time (ms)\")\n",
    "    axes[1].set_title(\"Training Speed (lower is better)\")\n",
    "\n",
    "    # 3. Peak CUDA memory\n",
    "    if device.type == \"cuda\":\n",
    "        peak_mem = [np.nanmax(r[\"logs\"][\"cuda_mem_mb\"]) for r in completed]\n",
    "        axes[2].bar(x_pos, peak_mem, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "        axes[2].set_xticks(x_pos)\n",
    "        axes[2].set_xticklabels(names, rotation=35, ha=\"right\", fontsize=8)\n",
    "        axes[2].set_ylabel(\"Peak memory (MB)\")\n",
    "        axes[2].set_title(\"Peak GPU Memory (lower is better)\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"CUDA memory\\nnot available\", transform=axes[2].transAxes,\n",
    "                     ha=\"center\", va=\"center\", fontsize=12, color=\"gray\")\n",
    "        axes[2].set_title(\"Peak GPU Memory\")\n",
    "\n",
    "    fig.suptitle(\"Experiment Summary: Loss, Speed, and Memory across Precision Regimes\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"Need at least 2 completed experiments for summary charts.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Sanity check: generate text from each trained model\n",
    "\n",
    "Loss curves are the real metric, but they're abstract. Since we're training a tiny **causal language model**, we can do a qualitative sanity check: generate a short continuation from a fixed prompt for each successfully trained regime.\n",
    "\n",
    "Notes:\n",
    "- This is **not** a rigorous evaluation — it's a learning aid.\n",
    "- We run generation in **FP32** for comparability (we're comparing the *trained weights*, not inference autocast)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text generation samples (one prompt per precision regime)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)[:, -1, :] / max(float(temperature), 1e-8)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, int(top_k))\n",
    "            logits = logits.clone()\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "def safe_encode(s: str):\n",
    "    return [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "PROMPT = \"Autocast is \"\n",
    "NEW_TOKENS = 240\n",
    "TEMPERATURE = 0.9\n",
    "TOP_K = 20\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "printed = 0\n",
    "for r in results:\n",
    "    if r[\"status\"] != \"ok\":\n",
    "        continue\n",
    "    state = r.get(\"state_dict\", None)\n",
    "    if state is None:\n",
    "        continue\n",
    "    cfg = r[\"config\"]\n",
    "    m = TinyGPT(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=cfg.block_size,\n",
    "        n_layer=2,\n",
    "        n_embd=128,\n",
    "        n_heads=4,\n",
    "        dropout=0.0,\n",
    "    ).to(device).float()\n",
    "    m.load_state_dict(state)\n",
    "\n",
    "    idx0 = torch.tensor([safe_encode(PROMPT)], dtype=torch.long, device=device)\n",
    "    out = generate(m, idx0, NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "    text = decode_tokens(out[0].tolist())\n",
    "    print(f\"\\n=== {cfg.name} ===\\n{text}\")\n",
    "    printed += 1\n",
    "\n",
    "if printed == 0:\n",
    "    print(\"No successful runs to sample from.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Memory breakdown comparison (bytes per parameter)\n",
    "\n",
    "For LLM-scale training, it helps to separate memory into:\n",
    "\n",
    "- **Fixed-size memory** (scales with number of parameters): parameters + gradients + optimizer state (+ optional master weights).\n",
    "- **Activation memory** (scales with batch size × sequence length × layers): intermediate tensors saved for backward.\n",
    "\n",
    "AMP/autocast primarily helps with **activation memory** (and speed). With AdamW, the fixed-size **bytes/parameter** are often **~16 bytes/param** in both FP32 and \"standard AMP\" because optimizer state dominates.\n",
    "\n",
    "We'll compute the fixed-size accounting for a few common patterns and show what that looks like for our TinyGPT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fixed-size memory breakdown analysis (parameters + grads + optimizer state)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model_tmp = TinyGPT(vocab_size=vocab_size, block_size=64, n_layer=2, n_embd=128, n_heads=4)\n",
    "n_params = count_params(model_tmp)\n",
    "del model_tmp\n",
    "\n",
    "rows = []\n",
    "configs = [\n",
    "    # AdamW has 2 FP32 moment buffers by default: m and v (8 bytes/param).\n",
    "    {\n",
    "        \"config\": \"FP32 AdamW (baseline)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Params/grads/Adam moments all FP32\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"PyTorch AMP FP16 (compute)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Typical AMP: params/grads/state FP32; matmuls run in FP16 under autocast\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"PyTorch AMP BF16 (compute)\",\n",
    "        \"param_B\": 4, \"master_B\": 0, \"grad_B\": 4, \"opt_B\": 8,\n",
    "        \"notes\": \"Typical AMP: params/grads/state FP32; matmuls run in BF16 under autocast\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"Naive FP16 AdamW (NOT recommended)\",\n",
    "        \"param_B\": 2, \"master_B\": 0, \"grad_B\": 2, \"opt_B\": 4,\n",
    "        \"notes\": \"Small fixed memory, but numerically fragile (underflow/overflow + low-precision optimizer state)\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"Naive BF16 AdamW (sometimes works)\",\n",
    "        \"param_B\": 2, \"master_B\": 0, \"grad_B\": 2, \"opt_B\": 4,\n",
    "        \"notes\": \"Often trains, but sensitive reductions/normalizations can drift without an FP32 policy\",\n",
    "    },\n",
    "    {\n",
    "        \"config\": \"FP16 params + FP32 master + FP32 AdamW (classic mixed precision)\",\n",
    "        \"param_B\": 2, \"master_B\": 4, \"grad_B\": 2, \"opt_B\": 8,\n",
    "        \"notes\": \"Common in some stacks: FP16 model copy + FP32 master weights + FP32 optimizer moments\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    total_B = int(cfg[\"param_B\"] + cfg[\"master_B\"] + cfg[\"grad_B\"] + cfg[\"opt_B\"])\n",
    "    param_mb = n_params * cfg[\"param_B\"] / 1024**2\n",
    "    master_mb = n_params * cfg[\"master_B\"] / 1024**2\n",
    "    grad_mb = n_params * cfg[\"grad_B\"] / 1024**2\n",
    "    opt_mb = n_params * cfg[\"opt_B\"] / 1024**2\n",
    "    total_mb = param_mb + master_mb + grad_mb + opt_mb\n",
    "    rows.append({\n",
    "        \"config\": cfg[\"config\"],\n",
    "        \"bytes_per_param\": total_B,\n",
    "        \"param_MB\": float(f\"{param_mb:.3f}\"),\n",
    "        \"master_MB\": float(f\"{master_mb:.3f}\"),\n",
    "        \"grad_MB\": float(f\"{grad_mb:.3f}\"),\n",
    "        \"optimizer_MB\": float(f\"{opt_mb:.3f}\"),\n",
    "        \"total_fixed_MB\": float(f\"{total_mb:.3f}\"),\n",
    "        \"notes\": cfg[\"notes\"],\n",
    "    })\n",
    "\n",
    "print(f\"TinyGPT parameters: {n_params:,}\")\n",
    "print(\"Fixed-size bytes/param = params + master + grads + optimizer state (activations NOT included).\")\n",
    "print(\"(Real LLM AMP savings come mostly from activations, which scale with batch×seq_len.)\\n\")\n",
    "\n",
    "df_mem = pd.DataFrame(rows)\n",
    "display(df_mem)\n",
    "\n",
    "# Visualize fixed memory breakdown as stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "x = np.arange(len(rows))\n",
    "w = 0.5\n",
    "names_m = df_mem[\"config\"].tolist()\n",
    "param_mb = df_mem[\"param_MB\"].tolist()\n",
    "master_mb = df_mem[\"master_MB\"].tolist()\n",
    "grad_mb = df_mem[\"grad_MB\"].tolist()\n",
    "opt_mb = df_mem[\"optimizer_MB\"].tolist()\n",
    "\n",
    "ax.bar(x, param_mb, w, label=\"Model params\", color=\"C0\", alpha=0.8)\n",
    "ax.bar(x, master_mb, w, bottom=param_mb, label=\"Master params\", color=\"C4\", alpha=0.8)\n",
    "ax.bar(x, grad_mb, w, bottom=[p+m for p,m in zip(param_mb, master_mb)], label=\"Gradients\", color=\"C1\", alpha=0.8)\n",
    "ax.bar(\n",
    "    x, opt_mb, w,\n",
    "    bottom=[p+m+g for p,m,g in zip(param_mb, master_mb, grad_mb)],\n",
    "    label=\"Optimizer state\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names_m, rotation=25, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Memory (MB)\")\n",
    "ax.set_title(f\"Fixed memory breakdown by precision config ({n_params:,} params)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nNote: This shows only fixed-size memory (params + grads + optimizer).\")\n",
    "print(\"Activations (which scale with batch×seq_len) are the real memory win for AMP.\")\n",
    "print(\"For large models, activation memory often exceeds parameter memory by 5-10x.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Interpreting results\n",
    "\n",
    "### Loss curves\n",
    "- **FP32**: should decrease smoothly. This is your reference.\n",
    "- **FP16 naive**: may diverge, stagnate, or produce NaN. This demonstrates why casting everything to half precision is not safe.\n",
    "- **BF16 naive**: often converges to a similar loss as FP32. This is the dramatic proof that BF16's range (8-bit exponent) matters more than FP16's precision (10-bit mantissa) for training stability. The network tolerates noisy gradient values; it cannot tolerate zero gradient values.\n",
    "- **AMP FP16 (no scaler)**: may work but may show more zero gradients than the scaled version. This shows that autocast alone helps (per-op policy keeps sensitive ops in FP32) but doesn't fully solve the backward-pass underflow problem.\n",
    "- **AMP FP16 (with scaler)**: should be stable. The GradScaler rescues gradients from underflow.\n",
    "- **AMP BF16**: typically matches FP32 quality without needing a scaler.\n",
    "\n",
    "### Step time\n",
    "- On modern GPUs, AMP often reduces step time because matmuls hit Tensor Cores.\n",
    "- If you don't see speedup: model may be too small (overhead dominates), or you're CPU-bound.\n",
    "- The smallest models sometimes see AMP *overhead* because the per-op dispatch cost exceeds the Tensor Core gains. This goes away at realistic model sizes.\n",
    "\n",
    "### Gradient norms\n",
    "- Should be comparable across regimes for a well-behaved model.\n",
    "- If FP16 naive shows erratic norms, that's the underflow/overflow instability.\n",
    "- BF16 naive norms should be close to FP32 (same gradient magnitudes, just represented with fewer mantissa bits).\n",
    "\n",
    "### Zero gradients\n",
    "- A high `zero_grad_frac` indicates underflow or dead signal (exact zeros).\n",
    "- **FP16 naive** and **AMP FP16 no scaler** may show elevated zero-gradient fractions.\n",
    "- **BF16 naive** should show low zero-gradient fractions (gradients don't underflow with 8-bit exponent).\n",
    "- The difference between FP16+scaler and FP16 without scaler directly measures the benefit of loss scaling.\n",
    "\n",
    "### GradScaler scale\n",
    "- If scale drops repeatedly, the model is hitting overflow events and GradScaler is skipping steps.\n",
    "- If scale grows steadily, training is stable and GradScaler is increasing headroom.\n",
    "- A \"spiky\" pattern (rapid drops followed by slow climbs) is normal and expected.\n",
    "\n",
    "### Memory\n",
    "- AMP typically uses ~same or slightly more memory than FP32 for parameters+optimizer (due to master weights), but saves on activations.\n",
    "- Naive 16-bit uses less total memory but trades stability.\n",
    "- The savings become dramatic at larger batch sizes and sequence lengths.\n",
    "- At LLM scale with Adam: 16 bytes/param (mixed precision) vs 16 bytes/param (FP32). The *activation* memory savings are where AMP really wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Practical checklist\n",
    "\n",
    "### Defaults that usually work\n",
    "- **CUDA:** prefer **BF16 autocast** if your GPU supports it (Ampere+). No GradScaler needed.\n",
    "- **CUDA (no BF16):** use **FP16 autocast + GradScaler**.\n",
    "- **CPU:** use **BF16 autocast** (mostly for numerics; speedups depend on CPU/kernel support).\n",
    "- **MPS:** use **FP16 autocast** (operator coverage differs from CUDA; verify with probes in Section 3).\n",
    "- Keep optimizer state in FP32 (default for most PyTorch optimizers).\n",
    "\n",
    "### When things go wrong\n",
    "1. **Loss becomes `nan`/`inf`:** Check for overflow sources (attention logits, exp/log, unstable loss). Consider lowering learning rate or adding gradient clipping.\n",
    "2. **Gradients mostly zero in FP16:** Use GradScaler with higher initial scale. Consider switching to BF16.\n",
    "3. **Training \"does nothing\":** Check for weight update stagnation — are weights actually changing? Ensure you have FP32 master weights (standard when model is FP32 + autocast).\n",
    "4. **Unexplained dtype behavior:** Use dtype hooks (Section 3.3) to confirm what's running in what dtype.\n",
    "\n",
    "### Common gotchas\n",
    "- Mixing manual `.half()` casts with autocast can lead to unexpected behavior.\n",
    "- Gradient clipping should happen **after** `scaler.unscale_(optimizer)`.\n",
    "- `autocast` should cover forward + loss, but **not** the optimizer step.\n",
    "- On Ampere+ GPUs, your \"FP32 baseline\" may use TF32 for matmuls. Check `torch.backends.cuda.matmul.allow_tf32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Real-world AMP patterns (copy/paste templates)\n",
    "\n",
    "This section is intentionally practical: patterns that show up once you move from a notebook demo to a real training run.\n",
    "\n",
    "### 3.10.1 Gradient accumulation (microbatches)\n",
    "\n",
    "If you do gradient accumulation, the safest pattern is:\n",
    "\n",
    "```python\n",
    "scaler = GradScaler()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for micro in range(grad_accum_steps):\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        loss = loss_fn(model(x_micro), y_micro)\n",
    "        loss = loss / grad_accum_steps      # IMPORTANT: normalize\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "scaler.unscale_(optimizer)                  # IMPORTANT: before clipping / inspecting grads\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "### 3.10.2 Gradient clipping with FP16 AMP\n",
    "\n",
    "Clipping should happen **after** `scaler.unscale_(optimizer)`. If you clip *scaled* gradients, you're clipping the wrong numbers.\n",
    "\n",
    "### 3.10.3 Multiple optimizers / parameter groups\n",
    "\n",
    "Use **one scaler**. Call `scaler.step(optimizer_i)` for each optimizer, then `scaler.update()` once per iteration.\n",
    "\n",
    "### 3.10.4 Custom `autograd.Function` / custom ops\n",
    "\n",
    "If you write custom autograd functions (or CUDA extensions), make them autocast-safe. In PyTorch there are decorators for this (API varies by version):\n",
    "\n",
    "```python\n",
    "try:\n",
    "    from torch.amp import custom_fwd, custom_bwd\n",
    "except Exception:\n",
    "    from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "class MyFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @custom_fwd(cast_inputs=torch.float16)\n",
    "    def forward(ctx, x, w):\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_out):\n",
    "        ...\n",
    "```\n",
    "\n",
    "If you skip this, autocast may feed your op tensors in unexpected dtypes, and you'll get silent accuracy bugs or runtime errors.\n",
    "\n",
    "### 3.10.5 Inference is simpler than training\n",
    "\n",
    "For inference you usually only need:\n",
    "- `torch.inference_mode()` (or `no_grad()`)\n",
    "- `autocast(...)` (no GradScaler)\n",
    "\n",
    "### 3.10.6 Troubleshooting table\n",
    "\n",
    "| Symptom | Likely cause | Quick check | Fix |\n",
    "|---|---|---|---|\n",
    "| Loss becomes `nan`/`inf` quickly | overflow in activations (attention logits, exp/log) or too-high LR | check `torch.isfinite(loss)`; watch GradScaler `found_inf`/scale drops | lower LR, add grad clipping, prefer BF16, check initialization |\n",
    "| Gradients mostly 0 in FP16 | underflow | measure `zero_grad_frac` or histogram of `log10(|grad|)` | use GradScaler (bigger initial scale), or switch to BF16 |\n",
    "| GradScaler scale keeps collapsing | frequent overflow events | scale drops + many skipped steps | lower LR, clip grads, check for unstable ops, switch to BF16 |\n",
    "| Training \"does nothing\" | weight update below ULP (stagnation) | log `max(|w_{t+1}-w_t|)` in model dtype | keep FP32 master weights (standard in AMP), avoid FP16 optimizer state |\n",
    "| No speedup from AMP | model too small, CPU-bound, or bad matmul shapes | compare tokens/s; check shapes are multiples of 8 | increase batch/seq, use Tensor Core-friendly dims, benchmark with realistic sizes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix — References\n",
    "\n",
    "## Papers\n",
    "\n",
    "| Paper | Year | Key contribution |\n",
    "|---|---|---|\n",
    "| Gupta et al., *Deep Learning with Limited Numerical Precision* | 2015 | Low-precision training needs deliberate rounding/scaling; stochastic rounding intuition |\n",
    "| Micikevicius et al., *Mixed Precision Training* | 2017 | FP32 master weights + loss scaling + per-op policies |\n",
    "| Kalamkar et al., *A Study of BFLOAT16 for Deep Learning Training* | 2019 | BF16 empirical validation, no loss scaling needed |\n",
    "| Rajbhandari et al., *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models* | 2020 | Memory breakdown of mixed-precision training |\n",
    "| NVIDIA (and others), *FP8 Formats for Deep Learning* | 2022 | FP8 formats (E4M3/E5M2), scaling metadata, and accumulation strategies |\n",
    "| Dettmers et al., *8-bit Optimizers via Block-wise Quantization* | 2022 | Reducing optimizer-state memory beyond AMP |\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- [PyTorch `torch.amp` docs](https://pytorch.org/docs/stable/amp.html) — autocast op reference, GradScaler API\n",
    "- [PyTorch AMP examples](https://pytorch.org/docs/stable/notes/amp_examples.html) — canonical training loop\n",
    "- [NVIDIA mixed precision blog](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n",
    "- [Google BF16 blog](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    "- [DeepSpeed config docs](https://www.deepspeed.ai/docs/config-json/) — FP16/BF16 training config\n",
    "- [PyTorch FSDP mixed precision](https://pytorch.org/docs/stable/fsdp.html)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's experiments are intentionally small; the *mechanisms* are the same at scale."
   ]
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Autocast / AMP in PyTorch: a deep practical reference\n\nThis notebook is a **learning tool** for understanding and *actually feeling* the impact of `autocast` during training.\n\nIt is organized into **three sections**:\n\n1. **Theory** (floating-point range/precision, underflow/overflow, what AMP is really doing)\n2. **What the literature says** (paper- and doc-driven mental models, *written explanations*, no experiments)\n3. **Practicalities** (hands-on experiments + graphs: loss curves, stability, gradient underflow, and dtype flow through a tiny LLM)\n\n**Primary goal:** after completing this notebook, you should be able to answer (and debug) questions like:\n\n- Why does FP16 often need **loss scaling**, but BF16 often does not?\n- What does `autocast` *actually* do per operation (matmul vs softmax vs layernorm)?\n- Why do people talk about **FP32 master weights** and **optimizer state precision**?\n- How can you *see* autocast happening inside a transformer forward pass?\n- What fails if you try to “just train in half precision everywhere”?\n\n---\n\n## How to use this notebook\n\n- Read the markdown, then run the code cells.\n- Most experiments are designed to run in a few minutes on a single GPU.\n- CPU-only runs are supported for the *conceptual* demos, but some mixed-precision behaviors (and speedups) are fundamentally GPU-driven.\n\n---\n\n## Table of contents (high-level)\n\n- **Section 1 — Theory**\n  - Floating-point: range vs precision\n  - FP16 vs BF16 vs FP32 (tables you can trust)\n  - Underflow/overflow and why training cares\n  - What AMP is (autocast + grad scaling)\n  - Master weights, optimizer state, and accumulation\n\n- **Section 2 — What the literature says**\n  - Mixed Precision Training (Micikevicius et al.)\n  - NVIDIA mixed precision guidance\n  - BF16 design intent\n  - PyTorch AMP operator policy\n  - LLM training stacks (FSDP/ZeRO) and where AMP fits\n\n- **Section 3 — Practicalities**\n  - A minimal AMP training loop\n  - Build an operator policy table *from your local PyTorch*\n  - Visualize gradient underflow + the effect of loss scaling\n  - Train a tiny causal transformer (LLM) under different precision regimes\n  - Plot and interpret loss/time/scale curves\n\n---\n\n## Quick glossary\n\n- **AMP**: automatic mixed precision (in PyTorch: `torch.amp`)\n- **autocast**: chooses an op-specific dtype policy inside a context manager\n- **GradScaler / loss scaling**: rescales loss to avoid FP16 gradient underflow\n- **master weights**: keep weights in FP32 for updates, cast for compute\n- **underflow**: magnitude too small → becomes 0 (or subnormal/denormal)\n- **overflow**: magnitude too large → becomes `inf`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\nYou need:\n\n- Python 3.10+\n- PyTorch 2.x\n- `matplotlib`, `numpy`, `pandas`\n\n### Install (CPU-only quick start)\n\n```bash\npip install torch numpy pandas matplotlib\n```\n\n### Install (CUDA)\n\nInstall the correct PyTorch + CUDA build from the official PyTorch instructions for your platform.\n\n---\n\nThis notebook is written to *degrade gracefully*:\n\n- If BF16 is not supported on your GPU, BF16 experiments will be skipped.\n- If FP16 training without scaling explodes (often does), we record that as a result rather than pretending it “worked”.\n\n\nNote: on Apple Silicon, this notebook defaults to CPU (AMP/autocast behavior is best-defined on CUDA/CPU)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Core imports + environment report\nimport os\nimport math\nimport time\nimport random\nfrom dataclasses import dataclass\nfrom contextlib import nullcontext\nfrom datetime import date\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot defaults\nplt.rcParams.update({\n    \"figure.figsize\": (10, 4),\n    \"axes.grid\": True,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n})\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\nexcept ModuleNotFoundError as e:\n    raise ModuleNotFoundError(\n        \"PyTorch is required for this notebook. Install it, restart the kernel, then re-run.\\n\"\n        \"CPU-only (quick start): pip install torch\\n\"\n        \"CUDA: install the correct wheel from the official PyTorch site.\"\n    ) from e\n\n# Prefer torch.amp (newer API), but fall back for older versions.\nif hasattr(torch, \"amp\"):\n    autocast = torch.amp.autocast\n    GradScaler = torch.amp.GradScaler\nelse:\n    autocast = torch.cuda.amp.autocast\n    GradScaler = torch.cuda.amp.GradScaler\n\n\ndef set_seed(seed: int = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed(0)\n\n# Device selection: CUDA if available, else CPU.\n# (MPS exists on Apple, but this notebook is intentionally CUDA/CPU-centric.)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Date:\", date.today().isoformat())\nprint(\"Torch:\", torch.__version__)\nprint(\"Device:\", device)\nif device.type == \"cuda\":\n    print(\"CUDA:\", torch.version.cuda)\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n    print(\"BF16 supported:\", torch.cuda.is_bf16_supported())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Section 1 — Theory\n\nThe core trick of autocast is simple to state:\n\n> **Run the *right* operations in lower precision for speed/memory, while keeping *numerically sensitive* operations in FP32.**\n\nBut to understand *why* this works (and when it doesn’t), we need to understand what floating-point formats can and cannot represent."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.0 Where floating-point lives during training (and why autocast exists)\n\nA single training step can be decomposed into:\n\n1. **Forward**: parameters + activations → logits\n2. **Loss**: logits + targets → scalar loss\n3. **Backward**: loss → gradients for parameters\n4. **Optimizer update**: parameters + gradients (+ optimizer state) → new parameters\n\nDifferent tensors have different numeric requirements:\n\n| Object | Typical AMP dtype | Why |\n|---|---|---|\n| Activations / intermediate matmul results | FP16/BF16 (where safe) | saves memory + uses Tensor Cores |\n| Softmax / LayerNorm stats / big reductions | FP32 (often) | protects against overflow + rounding accumulation |\n| Parameter gradients | often FP32 *storage* (even if compute is mixed) | stable updates + compatibility with optimizers |\n| Parameters (“master weights”) | FP32 | prevents update stagnation |\n| Optimizer state (Adam moments) | FP32 | long-horizon accumulation is precision-sensitive |\n\nAutocast’s job is mostly about **(1) and (2)**: choose per-op dtypes.\nGrad scaling’s job is mostly about **(3)** when FP16 is involved: keep gradients from underflowing."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Floating-point is “range + precision”, not just “more bits = better”\n\nA binary floating-point number is roughly:\n\n\\[\n(-1)^{\\text{sign}} \\times (1.\\text{mantissa}) \\times 2^{\\text{exponent}}\n\\]\n\nThe bit budget is split across:\n\n- **Exponent bits** → *range* (how large/small magnitudes you can represent)\n- **Mantissa (fraction) bits** → *precision* (how many significant bits you keep)\n\nFor deep learning training, the key question is not “can I store 3.14159?” but:\n\n- Can I represent **tiny gradients** without them becoming 0 (underflow)?\n- Can I represent **large activations** without them becoming `inf` (overflow)?\n- Can I sum many numbers without destroying meaning via rounding?\n\nThese failure modes show up differently in FP16 and BF16."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1.1 IEEE 754 anatomy (sign, exponent bias, hidden bit)\n\nA *normalized* binary floating-point value is encoded as:\n\n- **sign bit** $s$\n- **exponent field** $E$ (stored with a **bias**)\n- **mantissa / fraction field** $m$\n\nFor normalized numbers:\n\n$$\n\\text{value} = (-1)^s \\times (1 + m) \\times 2^{(E - \\text{bias})}\n$$\n\nKey details:\n\n- The leading `1.` is **implicit** (the “hidden bit”), which is why you get 1 extra bit of precision.\n- Exponent all-zeros and all-ones are **reserved**:\n  - `E=0`: subnormals / zero\n  - `E=all ones`: `inf` / `nan`\n\nThe bias is what lets exponents represent both negative and positive powers.\n\nWe’ll decode π in FP32/FP16 (and BF16) to make this concrete."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Bit-level decoding demo (pi) for float32/float16/(optional) bfloat16\nimport struct\n\n\ndef bits_f32(x: float) -> str:\n    (u32,) = struct.unpack(\">I\", struct.pack(\">f\", float(x)))\n    return f\"{u32:032b}\"\n\n\ndef bits_f16(x: float) -> str:\n    u16 = np.frombuffer(np.float16(x).tobytes(), dtype=np.uint16)[0]\n    return f\"{int(u16):016b}\"\n\n\ndef bits_bf16(x: float) -> str:\n    t = torch.tensor(float(x), dtype=torch.bfloat16)\n    i16 = int(t.view(torch.int16).item()) & 0xFFFF\n    return f\"{i16:016b}\"\n\n\ndef decode(bits: str, exp_bits: int, mant_bits: int, bias: int):\n    s = int(bits[0], 2)\n    E = int(bits[1 : 1 + exp_bits], 2)\n    M_bits = bits[1 + exp_bits :]\n    assert len(M_bits) == mant_bits\n\n    if E == 0:\n        kind = \"subnormal/zero\"\n        exp = 1 - bias\n        mant = sum(int(b) * (2 ** (-(i + 1))) for i, b in enumerate(M_bits))\n        val = ((-1) ** s) * (mant) * (2 ** exp)\n        return kind, s, E, exp, mant, val\n\n    if E == (2**exp_bits - 1):\n        kind = \"inf/nan\"\n        return kind, s, E, None, None, None\n\n    kind = \"normal\"\n    exp = E - bias\n    mant = sum(int(b) * (2 ** (-(i + 1))) for i, b in enumerate(M_bits))\n    val = ((-1) ** s) * (1.0 + mant) * (2 ** exp)\n    return kind, s, E, exp, mant, val\n\n\nx = math.pi\n\nrows = []\n\nb32 = bits_f32(x)\nkind, s, E, exp, mant, val = decode(b32, exp_bits=8, mant_bits=23, bias=127)\nrows.append({\"dtype\": \"float32\", \"bits\": b32, \"kind\": kind, \"sign\": s, \"E(raw)\": E, \"exp\": exp, \"mant\": mant, \"decoded\": val})\n\nb16 = bits_f16(x)\nkind, s, E, exp, mant, val = decode(b16, exp_bits=5, mant_bits=10, bias=15)\nrows.append({\"dtype\": \"float16\", \"bits\": b16, \"kind\": kind, \"sign\": s, \"E(raw)\": E, \"exp\": exp, \"mant\": mant, \"decoded\": val})\n\nbbf = bits_bf16(x)\nkind, s, E, exp, mant, val = decode(bbf, exp_bits=8, mant_bits=7, bias=127)\nrows.append({\"dtype\": \"bfloat16\", \"bits\": bbf, \"kind\": kind, \"sign\": s, \"E(raw)\": E, \"exp\": exp, \"mant\": mant, \"decoded\": val})\n\npd.DataFrame(rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1.2 Normal vs subnormal numbers (and “flush-to-zero”)\n\n**Subnormals** extend representable values closer to 0 by giving up the implicit leading `1.`.\n\n- They matter because gradients can be very small.\n- But they can be slow on some hardware.\n\nMany compute paths enable **FTZ/DAZ** (“flush-to-zero” / “denormals-are-zero”), which means extremely small values become exactly 0.\n\nThe practical lesson:\n\n- It is not enough to know the *spec* of a dtype.\n- You also need to know what your hardware/kernel path does with denormals.\n\nLet’s probe whether the smallest subnormal survives on your device."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Subnormal survival probe (device-dependent)\n\ndef subnormal_survives(dtype: torch.dtype, device: torch.device):\n    z = torch.tensor(0.0, dtype=dtype, device=device)\n    o = torch.tensor(1.0, dtype=dtype, device=device)\n    sub = torch.nextafter(z, o)\n    return {\n        \"dtype\": str(dtype),\n        \"device\": device.type,\n        \"nextafter(0,1)\": float(sub) if sub.numel() == 1 else None,\n        \"is_zero\": bool((sub == 0).item()),\n    }\n\nrows = []\nfor dt in [torch.float16, torch.bfloat16, torch.float32]:\n    try:\n        rows.append(subnormal_survives(dt, device))\n    except Exception as e:\n        rows.append({\"dtype\": str(dt), \"device\": device.type, \"error\": type(e).__name__})\n\npd.DataFrame(rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1.3 ULP: spacing grows with magnitude\n\nA float format has *roughly constant relative precision* but *variable absolute precision*.\n\n- Near 1.0, FP16 spacing is ~`1e-3`.\n- Near 1024, spacing is ~`1`.\n\nThis is the concrete reason “tiny updates disappear” when weights are stored in low precision.\n\nLet’s plot ULP as a function of magnitude."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ULP vs magnitude\n\ndef ulp(x: torch.Tensor, dtype: torch.dtype):\n    x = x.to(dtype)\n    return (torch.nextafter(x, x * 2) - x).abs().to(torch.float32)\n\n# Use exact powers of two to avoid extra rounding.\nks = torch.arange(-10, 21, device=device)\nx = (2.0 ** ks).to(torch.float32)\n\nplt.figure(figsize=(10, 4))\nfor dt in [torch.float16, torch.bfloat16, torch.float32]:\n    if device.type == \"cpu\" and dt is torch.float16:\n        continue\n    u = ulp(x, dt).cpu().numpy()\n    plt.plot(ks.cpu().numpy(), np.log2(u + 1e-30), marker=\"o\", label=str(dt))\n\nplt.title(\"log2(ULP) vs log2(|x|) for powers of two\")\nplt.xlabel(\"log2(|x|)\")\nplt.ylabel(\"log2(ULP(x))\")\nplt.legend();",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 FP16 vs BF16 vs FP32 (the table you should memorize)\n\n### Bit layouts (IEEE-like)\n\n| dtype | total bits | exponent bits | mantissa bits | “precision bits” (incl. hidden 1) | what it’s good for |\n|---|---:|---:|---:|---:|---|\n| FP16 (IEEE half) | 16 | 5 | 10 | 11 | fast Tensor Core compute, but narrow range → underflow/overflow risks |\n| BF16 | 16 | 8 | 7 | 8 | FP32-like range, lower precision; often “drop-in” for training |\n| FP32 (single) | 32 | 8 | 23 | 24 | stable baseline; slower/more memory |\n\n### Two immediate consequences\n\n1. **FP16 has more mantissa bits than BF16** → better precision.\n2. **BF16 has the same exponent width as FP32** → dramatically better *range* than FP16.\n\nSo:\n\n- FP16 fails *first* due to **range** (underflow/overflow).\n- BF16 fails *first* due to **precision** (rounding / accumulation error).\n\nAutocast exists to route computations so that you get the performance of 16-bit compute *without* the worst numeric failure modes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# A numeric “format facts” table from your local PyTorch\n\ndef smallest_subnormal(dtype: torch.dtype) -> float:\n    z = torch.tensor(0.0, dtype=dtype)\n    o = torch.tensor(1.0, dtype=dtype)\n    return float(torch.nextafter(z, o))\n\n\ndef ulp_at_one(dtype: torch.dtype) -> float:\n    one = torch.tensor(1.0, dtype=dtype)\n    nxt = torch.nextafter(one, one + one)\n    return float(nxt - one)\n\n\ndef dtype_row(name: str, dtype: torch.dtype, exp_bits: int, mant_bits: int, exp_min: int, exp_max: int):\n    fi = torch.finfo(dtype)\n    return {\n        \"dtype\": name,\n        \"bits\": fi.bits,\n        \"exp_bits\": exp_bits,\n        \"mant_bits\": mant_bits,\n        \"precision_bits\": mant_bits + 1,\n        \"approx_decimal_digits\": round((mant_bits + 1) * math.log10(2), 2),\n        \"exp_min(normal)\": exp_min,\n        \"exp_max(normal)\": exp_max,\n        \"eps\": float(fi.eps),\n        \"ulp(1.0)\": ulp_at_one(dtype),\n        \"min_normal\": float(fi.tiny),\n        \"min_subnormal\": smallest_subnormal(dtype),\n        \"max_finite\": float(fi.max),\n    }\n\n\ndtype_info = pd.DataFrame([\n    dtype_row(\"float16\", torch.float16, exp_bits=5, mant_bits=10, exp_min=-14, exp_max=15),\n    dtype_row(\"bfloat16\", torch.bfloat16, exp_bits=8, mant_bits=7, exp_min=-126, exp_max=127),\n    dtype_row(\"float32\", torch.float32, exp_bits=8, mant_bits=23, exp_min=-126, exp_max=127),\n]).set_index(\"dtype\")\n\ndtype_info",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interpret the table\n\n- `eps` / `ulp(1.0)` are *precision* around 1.0.\n  - BF16’s spacing near 1.0 is ~`0.0078125`.\n  - FP16’s spacing near 1.0 is ~`0.0009765625`.\n  - FP32’s spacing near 1.0 is ~`1.19e-07`.\n\n- `min_normal` and `min_subnormal` are the smallest magnitudes you can represent.\n  - FP16’s smallest normal is around `6e-5`, and smallest subnormal around `6e-8`.\n  - BF16’s smallest normal is around `1e-38`, which is *enormously* smaller.\n\nThis is why FP16 can silently turn small gradients into exact 0, while BF16 usually won’t."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 The three numeric disasters that show up during training\n\n### (A) Underflow (values become 0)\n\n- Common in **gradients**, especially late in training or in deep nets with tiny signals.\n- Most harmful in FP16 due to narrow exponent range.\n\n### (B) Overflow (values become `inf`)\n\n- Common in **activations** (e.g., exponentials), attention logits, or badly-initialized models.\n- FP16 can overflow much earlier than BF16/FP32.\n\n### (C) Accumulation / cancellation error\n\nEven when values are in range, precision limits can corrupt sums/products.\n\nClassic example: adding many small numbers to a large accumulator.\n\n- FP16/BF16 have very limited mantissas.\n- For reductions (layernorm statistics, softmax normalization, large sums), frameworks often keep accumulation in FP32.\n\nAutocast is partly about **preventing A/B** (range disasters), and partly about routing sensitive reductions so **C** doesn’t destroy training."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# A quick visualization: where exp() overflows by dtype\n# This is not an AMP experiment; it's a numeric intuition builder.\n\nx = torch.linspace(-20, 20, 400, device=device)\n\ndef safe_exp(x, dtype):\n    y = torch.exp(x.to(dtype))\n    return y.to(torch.float32).cpu().numpy()\n\nys = {\n    \"float32\": safe_exp(x, torch.float32),\n}\n\n# float16 exp is meaningful on CUDA; on CPU it may be slower / less supported.\ntry:\n    ys[\"float16\"] = safe_exp(x, torch.float16)\nexcept Exception as e:\n    ys[\"float16\"] = None\n    print(\"float16 exp skipped:\", type(e).__name__, str(e)[:120])\n\ntry:\n    ys[\"bfloat16\"] = safe_exp(x, torch.bfloat16)\nexcept Exception as e:\n    ys[\"bfloat16\"] = None\n    print(\"bfloat16 exp skipped:\", type(e).__name__, str(e)[:120])\n\nplt.figure()\nfor k, y in ys.items():\n    if y is None:\n        continue\n    plt.plot(x.cpu().numpy(), np.log10(np.clip(y, 1e-30, 1e30)), label=k)\nplt.title(\"log10(exp(x)) computed in different dtypes\")\nplt.xlabel(\"x\")\nplt.ylabel(\"log10(exp(x)) (clipped)\")\nplt.legend();",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.1 Loss functions are “log-sum-exp machines” (and dtype matters)\n\nMany deep learning losses contain exponentials and logs.\n\nA classic example is the logistic / softplus component:\n\n$$\n\\log(1 + e^x)\n$$\n\n- The naïve formula overflows quickly in FP16.\n- Stable implementations (e.g., `F.softplus`) avoid overflow by rewriting the expression.\n\nWe’ll compare the naïve and stable versions across dtypes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Naïve vs stable softplus across dtypes\n\ndef naive_softplus(x: torch.Tensor):\n    return torch.log1p(torch.exp(x))\n\nx = torch.linspace(-80, 80, 2000, device=device)\nref = F.softplus(x.double()).float()  # high-precision reference\n\nplt.figure(figsize=(10, 4))\nfor dt in [torch.float16, torch.bfloat16, torch.float32]:\n    if device.type == \"cpu\" and dt is torch.float16:\n        continue\n    y_naive = naive_softplus(x.to(dt)).float()\n    y_stable = F.softplus(x.to(dt)).float()\n    err_naive = (y_naive - ref).abs().cpu().numpy()\n    err_stable = (y_stable - ref).abs().cpu().numpy()\n\n    plt.plot(x.cpu().numpy(), np.log10(err_naive + 1e-12), label=f\"naive {dt}\")\n    plt.plot(x.cpu().numpy(), np.log10(err_stable + 1e-12), linestyle=\"--\", label=f\"stable {dt}\")\n\nplt.title(\"log10(|error|) vs x for softplus implementations\")\nplt.xlabel(\"x\")\nplt.ylabel(\"log10 absolute error (vs FP64-stable ref)\")\nplt.legend(ncols=2);",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3.2 Softmax: the overflow trap and the stability rewrite\n\nSoftmax is everywhere in transformers (attention).\n\nNaïve softmax is:\n\n$$\n\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n$$\n\nThis can overflow in low precision because `exp(x)` explodes quickly.\n\nThe stable rewrite subtracts the maximum value before exponentiating:\n\n$$\n\\text{softmax}(x) = \\text{softmax}(x - \\max(x))\n$$\n\nPyTorch’s `F.softmax` uses a stable implementation.\nWe’ll demonstrate the difference."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Naïve vs stable softmax across dtypes\n\ndef naive_softmax(x: torch.Tensor, dim: int = -1):\n    ex = torch.exp(x)\n    return ex / ex.sum(dim=dim, keepdim=True)\n\nlogits = torch.tensor([0.0, 20.0, 40.0, 80.0], device=device)\nref = F.softmax(logits.double(), dim=0).float()\n\nrows = []\nfor dt in [torch.float16, torch.bfloat16, torch.float32]:\n    if device.type == \"cpu\" and dt is torch.float16:\n        continue\n\n    x = logits.to(dt)\n\n    try:\n        naive = naive_softmax(x, dim=0).float()\n        naive_ok = bool(torch.isfinite(naive).all().item())\n    except Exception:\n        naive = torch.full_like(ref, float(\"nan\"))\n        naive_ok = False\n\n    stable = F.softmax(x, dim=0).float()\n    stable_ok = bool(torch.isfinite(stable).all().item())\n\n    rows.append({\n        \"dtype\": str(dt),\n        \"naive_finite\": naive_ok,\n        \"stable_finite\": stable_ok,\n        \"max_abs_err(stable vs ref)\": float((stable - ref).abs().max().item()),\n    })\n\npd.DataFrame(rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.4 What AMP actually is\n\nIn PyTorch, AMP is two tools:\n\n1. **`autocast`** (forward + loss)\n   - A context manager that applies a **per-operation dtype policy**.\n   - It may cast inputs/weights *for the operation*.\n   - It does **not** permanently change your model parameters unless *you* cast them.\n\n2. **`GradScaler`** (backward + optimizer step)\n   - Primarily for **FP16 training**.\n   - Rescales the loss (and therefore gradients) to prevent gradient underflow.\n   - Dynamically adjusts the scale to avoid overflow.\n\nA clean mental model:\n\n- `autocast` protects you from **bad forward dtypes**.\n- `GradScaler` protects you from **bad backward magnitudes** (FP16 underflow)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.5 The canonical AMP training loop (four conceptual changes)\n\nStart with FP32 training:\n\n```python\noptimizer.zero_grad(set_to_none=True)\nlogits = model(x)\nloss = loss_fn(logits, y)\nloss.backward()\noptimizer.step()\n```\n\nAMP adds:\n\n1. Wrap forward + loss in autocast.\n2. Scale the loss before backward.\n3. Step via the scaler (it unscales + checks for inf/nan).\n4. Update the scaler.\n\n```python\nscaler = GradScaler()\n\noptimizer.zero_grad(set_to_none=True)\nwith autocast(device_type=\"cuda\", dtype=torch.float16):\n    logits = model(x)\n    loss = loss_fn(logits, y)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n```\n\nThat’s the “small code change” people talk about.\nBut the *reason* it works is the theory above."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.6 Master weights and optimizer state (why “just casting the model” is not the same)\n\nThere are **three** numeric objects in training that matter:\n\n1. **Parameters** (weights)\n2. **Gradients**\n3. **Optimizer state** (e.g., Adam moments)\n\nA classic mixed precision recipe is:\n\n- Keep a **master copy of weights in FP32**.\n- Do forward/backward compute in FP16/BF16 where safe.\n- Accumulate/maintain optimizer state (Adam moments) in FP32.\n\nWhy keep FP32 master weights?\n\n- Because 16-bit formats have coarse spacing.\n- A small update \\(\\Delta w\\) can be *below the spacing* at the magnitude of \\(w\\), so the weight does not change (stagnation).\n\nWe will demonstrate this in Section 3 with a tiny, concrete example."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.7 Autocast is an operator policy (not a global cast)\n\nAutocast does not “turn the whole model into FP16”.\nInstead it says (conceptually):\n\n- Matmul/linear/convolution → run in lower precision if possible (Tensor Cores)\n- Softmax, layernorm, large reductions, many losses → run in FP32 for stability\n\nThe exact policy is PyTorch-version- and backend-dependent.\n\nIn Section 3 we will *build a policy table by executing real ops on your machine*.\nThat is the most reliable way to learn it."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1 summary (cheat sheet)\n\n- FP16: better precision than BF16 but narrow range → needs **loss scaling** and careful op policies.\n- BF16: FP32-like range → often trains without scaling, but is coarse → reductions need care.\n- AMP = `autocast` (forward policy) + `GradScaler` (backward magnitude control).\n- Good AMP training typically keeps **optimizer state in FP32**, often keeps **master weights in FP32**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Section 2 — What the literature says (no experiments)\n\nThis section is intentionally *written*: the point is to build a paper-and-doc-driven mental model that you can carry into real training code.\n\nThink of this section as: “what each source contributes, and how it maps to PyTorch AMP”."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.0 A reading map (what each source gives you)\n\n| Source | What it gives you | How it maps to this notebook |\n|---|---|---|\n| Micikevicius et al. (2017) | the canonical mixed precision recipe (master weights + loss scaling) | Sections 1.4–1.6 and the scaling experiments |\n| NVIDIA mixed precision guidance | engineering intuition + failure modes | the underflow/overflow + “sensitive ops in FP32” story |\n| PyTorch `torch.amp` docs | the actual API + gotchas | `autocast` + `GradScaler` loops in Section 3 |\n| PyTorch autocast op reference | *the* per-op policy | we probe the policy empirically in Section 3.2 |\n| BF16 background (TPU/GPU) | why BF16 often works without scaling | Section 1.2 + BF16 training run |\n| Distributed training docs (FSDP/ZeRO/DeepSpeed) | where dtypes live in large systems | Section 2.5 mental model |\n\nIf you only read one thing: read the mixed precision paper, then read the PyTorch autocast op reference."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Micikevicius et al. (2017): *Mixed Precision Training*\n\n**Problem:** FP16 is fast, but naïvely training in FP16 breaks due to underflow/overflow and update precision.\n\n**Key ideas:**\n\n- Keep **FP32 master weights** for the update.\n- Use **loss scaling** to shift gradient magnitudes into FP16’s representable range.\n- Use mixed precision: compute some ops in FP16, keep others in FP32.\n\n**What to remember:**\n\n- The “magic” is not just `half()`; it is the *combination* of (a) op-level policies, (b) scaling, (c) FP32 updates.\n- AMP frameworks are automations of this recipe.\n\nReference: Micikevicius et al., arXiv:1710.03740."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 NVIDIA mixed precision guidance (engineering view)\n\nA useful engineering decomposition:\n\n1. **Tensor Core compute** wants FP16/BF16 inputs.\n2. **Some ops are numerically sensitive** (exp/softmax, normalization stats, large reductions).\n3. FP16’s range is narrow, so gradients can underflow.\n\nThis viewpoint is why modern stacks:\n\n- Run matmuls in FP16/BF16.\n- Accumulate many reductions in FP32.\n- Use dynamic loss scaling for FP16.\n\nThe practical outcome is the “few lines” AMP recipe, but the reason it works is the numeric analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.3 BF16: “FP32 range, fewer bits of precision”\n\nBF16 was designed so that you can get most of the training stability benefits of FP32 range while using 16-bit storage/compute.\n\nThe mental model:\n\n- BF16 is usually not endangered by underflow in the same way as FP16.\n- BF16 can still introduce training error via *coarse rounding*, so reductions and sensitive ops often remain FP32.\n\nIn practice, for many transformer trainings on modern GPUs:\n\n- **BF16 autocast** can be “drop-in” without loss scaling.\n- FP16 autocast often needs a `GradScaler`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.4 PyTorch AMP docs: the operator policy is the decoder ring\n\nA common mistake is to treat autocast like a global switch.\n\nBut autocast is really an **operator policy**:\n\n- Some ops are eligible for lower precision.\n- Some ops are forced to FP32.\n- Some ops “promote” to the widest input type.\n\nThe takeaway for practitioners:\n\n- When debugging AMP, you need to know which ops ran in which dtype.\n- The most robust way to learn is to *probe it empirically on your version*, which we’ll do in Section 3."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.5 LLM training stacks (FSDP / ZeRO / DeepSpeed): where precision choices multiply\n\nAt LLM scale, training systems often shard:\n\n- parameters\n- gradients\n- optimizer state\n\nMixed precision gets more complicated because:\n\n- You may store “working weights” in BF16/FP16.\n- You may keep FP32 master weights (sometimes sharded).\n- Optimizer states (Adam moments) are often FP32 for stability.\n\nPractical takeaway:\n\n- “AMP” in a distributed stack is not only about autocast; it is about **where each tensor lives and in what dtype**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.6 How to think about the autocast policy (conceptual categories)\n\nAutocast decisions usually fall into one of these buckets:\n\n1. **Lower precision eligible** (often matmul-like): run in FP16/BF16 for throughput.\n2. **Force FP32**: ops with high risk of overflow/underflow or large reduction error (softmax, normalization stats, some losses).\n3. **Promote to widest**: if one input is FP32, the op may run in FP32.\n\nTwo practitioner rules:\n\n- If an op creates very large/small magnitudes (exp/log/softmax), assume it needs FP32 unless proven otherwise.\n- If an op reduces many values (sum/mean/var), assume accumulation needs FP32 unless proven otherwise.\n\nIn Section 3.2 we probe your exact PyTorch build to see what happens."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.7 Dynamic loss scaling (what `GradScaler` is doing)\n\nLoss scaling multiplies the loss by a factor $S$.\n\n- Backprop gradients scale by $S$ as well.\n- This shifts gradients into FP16’s representable range.\n\nThen, before the optimizer step:\n\n- GradScaler **unscales** gradients by dividing by $S$.\n- If it detects `inf`/`nan` gradients (overflow), it **skips the step** and lowers $S$.\n- If things are stable, it can slowly increase $S$ to gain more headroom.\n\nThe reason it’s safe: scaling/unscaling cancels out *if no overflow occurs*.\n\nThis is why you should not clip gradients until after unscale."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2 summary\n\n- The original mixed precision recipe explains *why* AMP needs scaling + FP32 updates (especially FP16).\n- BF16 works well largely because its exponent range matches FP32.\n- PyTorch autocast is an operator policy; learning it by probing is more reliable than memorizing.\n- In real LLM training stacks, “precision” applies to parameters, grads, and optimizer state separately."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Section 3 — Practicalities (experiments + graphs)\n\nThis section is where we turn everything into measurements.\n\nPrinciples:\n\n- Prefer experiments that are **small, fast, and explain a single idea**.\n- Log everything you might need for debugging (loss, grad norms, scaler scale, step time, NaN/inf).\n- Make the results comparable across dtypes."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.0 Controlling confounders (TF32, randomness, and fair comparisons)\n\nWhen you compare FP32 vs AMP runs, you can accidentally compare the wrong thing.\n\nTwo common confounders:\n\n1. **TF32 on NVIDIA Ampere+**\n   - Many FP32 matmuls can use TF32 internally (10-bit mantissa) for speed.\n   - That means your “FP32 baseline” may not be strict FP32 precision.\n\n2. **Randomness**\n   - Dropout, data sampling, and nondeterministic kernels can introduce run-to-run variance.\n\nFor a learning notebook, it’s often useful to **print** these settings and optionally disable TF32 for “clean” comparisons."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# (Optional) inspect / control TF32\n\nif device.type == \"cuda\":\n    print(\"TF32 matmul allow:\", torch.backends.cuda.matmul.allow_tf32)\n    print(\"TF32 cuDNN allow:\", torch.backends.cudnn.allow_tf32)\n\n    # Set this to True if you want strict FP32 matmul (slower, but cleaner comparisons).\n    DISABLE_TF32 = False\n    if DISABLE_TF32:\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n        print(\"TF32 disabled\")\nelse:\n    print(\"CUDA not available; TF32 not applicable\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.1 Minimal AMP training loop (reference snippet)\n\nThis is the “copy/paste” template, but the rest of the notebook explains *why* each line exists."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Minimal AMP loop (template)\n\ndef amp_train_step(model, optimizer, x, y, *, device_type: str, amp_dtype: torch.dtype, scaler=None):\n    model.train()\n    optimizer.zero_grad(set_to_none=True)\n\n    ac = autocast(device_type=device_type, dtype=amp_dtype, enabled=(scaler is not None or amp_dtype is not None))\n    with ac:\n        logits = model(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n\n    if scaler is None:\n        loss.backward()\n        optimizer.step()\n        return float(loss)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    return float(loss)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 Build an “autocast operator policy table” from your local PyTorch\n\nInstead of trusting a static table from the internet, we can probe:\n\n- With autocast disabled\n- With autocast enabled (FP16 or BF16)\n\nand record the output dtype.\n\nThis turns autocast from “mystery magic” into an observable policy."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Operator policy probe\n\n@dataclass\nclass OpProbe:\n    name: str\n    fn: callable\n\n\ndef probe_ops(device: torch.device, amp_dtype: torch.dtype):\n    device_type = device.type if device.type != \"mps\" else \"cpu\"  # autocast is best-defined for cuda/cpu\n\n    ops = [\n        OpProbe(\"matmul\", lambda a, b: a @ b),\n        OpProbe(\"linear\", lambda a, w, b: F.linear(a, w, b)),\n        OpProbe(\"softmax\", lambda x: F.softmax(x, dim=-1)),\n        OpProbe(\"layer_norm\", lambda x: F.layer_norm(x, normalized_shape=(x.size(-1),))),\n        OpProbe(\"exp\", lambda x: torch.exp(x)),\n        OpProbe(\"sum\", lambda x: x.sum()),\n    ]\n\n    rows = []\n\n    # Inputs\n    a = torch.randn(128, 128, device=device, dtype=torch.float32)\n    b = torch.randn(128, 128, device=device, dtype=torch.float32)\n    x = torch.randn(1024, device=device, dtype=torch.float32)\n\n    w = torch.randn(128, 128, device=device, dtype=torch.float32)\n    bias = torch.randn(128, device=device, dtype=torch.float32)\n\n    def run(op: OpProbe, use_autocast: bool):\n        ctx = autocast(device_type=device_type, dtype=amp_dtype, enabled=use_autocast)\n        with ctx:\n            if op.name == \"matmul\":\n                y = op.fn(a, b)\n            elif op.name == \"linear\":\n                y = op.fn(a, w, bias)\n            else:\n                y = op.fn(x)\n        if isinstance(y, torch.Tensor):\n            return str(y.dtype)\n        return type(y).__name__\n\n    for op in ops:\n        rows.append({\n            \"op\": op.name,\n            \"autocast\": False,\n            \"out_dtype\": run(op, False),\n        })\n        rows.append({\n            \"op\": op.name,\n            \"autocast\": True,\n            \"out_dtype\": run(op, True),\n        })\n\n    return pd.DataFrame(rows)\n\n\nfor dtype in [torch.float16, torch.bfloat16]:\n    if device.type == \"cuda\" and dtype is torch.bfloat16 and not torch.cuda.is_bf16_supported():\n        continue\n    if device.type == \"cpu\" and dtype is torch.float16:\n        # CPU autocast is typically bfloat16-focused.\n        continue\n\n    df = probe_ops(device, dtype)\n    print()\n    print(\"=== amp_dtype =\", dtype, \"===\")\n    display(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Watch dtype flow through a tiny transformer (hooks)\n\nThis is the “1000x better version” of printing dtypes: instead of only showing that autocast exists, we’ll build a transformer-ish module and observe which submodules:\n\n- receive FP32 inputs\n- output BF16/FP16 activations\n- are forced to FP32 (e.g., normalization)\n\nThis makes the op policy visceral."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# A tiny transformer block (enough to exercise matmul/softmax/norm)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, n_embd: int, n_heads: int, dropout: float = 0.0):\n        super().__init__()\n        assert n_embd % n_heads == 0\n        self.n_heads = n_heads\n        self.head_dim = n_embd // n_heads\n\n        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D)\n        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n\n        # Attention scores\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, T, T)\n\n        # Causal mask\n        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n        att = att.masked_fill(mask, float(\"-inf\"))\n\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n\n        y = att @ v  # (B, H, T, D)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.proj(y)\n        return y\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_embd: int, hidden_mult: int = 4, dropout: float = 0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(n_embd, hidden_mult * n_embd)\n        self.fc2 = nn.Linear(hidden_mult * n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, n_embd: int, n_heads: int, dropout: float = 0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.mlp = MLP(n_embd, dropout=dropout)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\nclass TinyGPT(nn.Module):\n    def __init__(self, vocab_size: int, block_size: int, n_layer: int = 2, n_embd: int = 128, n_heads: int = 4, dropout: float = 0.0):\n        super().__init__()\n        self.block_size = block_size\n        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n        self.pos_emb = nn.Embedding(block_size, n_embd)\n        self.drop = nn.Dropout(dropout)\n        self.blocks = nn.ModuleList([Block(n_embd, n_heads, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n\n    def forward(self, idx):\n        B, T = idx.shape\n        assert T <= self.block_size\n        pos = torch.arange(0, T, device=idx.device)\n\n        x = self.tok_emb(idx) + self.pos_emb(pos)[None, :, :]\n        x = self.drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        return logits\n\n\ndef install_dtype_hooks(model: nn.Module, watch=(nn.Linear, nn.LayerNorm, nn.Embedding)):\n    hooks = []\n    records = []\n\n    def make_hook(name):\n        def hook(m, inp, out):\n            def dt(x):\n                if isinstance(x, torch.Tensor):\n                    return str(x.dtype)\n                return type(x).__name__\n            indt = dt(inp[0]) if isinstance(inp, (tuple, list)) and inp else dt(inp)\n            outt = dt(out) if not isinstance(out, (tuple, list)) else dt(out[0])\n            records.append({\"module\": name, \"type\": type(m).__name__, \"in\": indt, \"out\": outt})\n        return hook\n\n    for name, m in model.named_modules():\n        if isinstance(m, watch):\n            hooks.append(m.register_forward_hook(make_hook(name)))\n\n    return hooks, records\n\n\n# Build a tiny model and run a single forward pass under several regimes\nvocab_size = 128\nblock_size = 64\nidx = torch.randint(0, vocab_size, (2, block_size), device=device)\n\n\ndef run_dtype_trace(amp_enabled: bool, amp_dtype: torch.dtype | None, param_dtype: torch.dtype):\n    model = TinyGPT(vocab_size=vocab_size, block_size=block_size).to(device)\n    model = model.to(param_dtype)\n\n    hooks, rec = install_dtype_hooks(model)\n\n    device_type = device.type if device.type != \"mps\" else \"cpu\"\n    ctx = autocast(device_type=device_type, dtype=amp_dtype, enabled=amp_enabled)\n    with torch.inference_mode(), ctx:\n        _ = model(idx)\n\n    for h in hooks:\n        h.remove()\n\n    df = pd.DataFrame(rec)\n    df[\"count\"] = 1\n    summary = df.groupby([\"type\", \"in\", \"out\"], as_index=False)[\"count\"].sum().sort_values([\"type\", \"in\", \"out\"])\n    return summary\n\n\ntraces = []\ntraces.append((\"no autocast, params fp32\", run_dtype_trace(False, None, torch.float32)))\n\nif device.type == \"cuda\":\n    traces.append((\"autocast fp16, params fp32\", run_dtype_trace(True, torch.float16, torch.float32)))\n    if torch.cuda.is_bf16_supported():\n        traces.append((\"autocast bf16, params fp32\", run_dtype_trace(True, torch.bfloat16, torch.float32)))\n\nfor title, df in traces:\n    print()\n    print(\"---\", title, \"---\")\n    display(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Optional: dtype tracing on a real pretrained LLM (Hugging Face)\n\nIf you have `transformers` installed and can load a small model, the same dtype-hook method works on a real LLM.\n\nNotes:\n\n- Loading from Hugging Face may require internet access *or* a cached model.\n- The forward pass is enough to observe dtype flow; you don’t need to train the model.\n- This cell is optional; the notebook remains fully self-contained without it."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Optional: dtype tracing on a small pretrained causal LM\n\ntry:\n    from transformers import AutoModelForCausalLM, AutoTokenizer\nexcept Exception as e:\n    AutoModelForCausalLM = None\n    print(\"transformers not available; skipping pretrained LLM trace:\", type(e).__name__)\n\nif AutoModelForCausalLM is not None and device.type == \"cuda\":\n    model_name = \"sshleifer/tiny-gpt2\"  # tiny + fast; swap for OPT-125M if you have it cached\n\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name)\n        mdl = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device).eval()\n    except Exception as e:\n        print(\"Could not load model (maybe no internet / not cached):\", type(e).__name__, str(e)[:200])\n    else:\n        text = \"AMP dtype tracing on a pretrained LM.\"\n        inputs = tok(text, return_tensors=\"pt\").to(device)\n\n        hooks, rec = install_dtype_hooks(mdl)\n\n        dtype16 = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n        with torch.inference_mode(), autocast(device_type=\"cuda\", dtype=dtype16):\n            _ = mdl(**inputs)\n\n        for h in hooks:\n            h.remove()\n\n        df = pd.DataFrame(rec)\n        df[\"count\"] = 1\n        summary = df.groupby([\"type\", \"in\", \"out\"], as_index=False)[\"count\"].sum().sort_values([\"type\", \"in\", \"out\"])\n        display(summary)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.4 Gradient underflow and why loss scaling works\n\nWe’ll do a controlled experiment:\n\n1. Create a synthetic gradient distribution.\n2. Cast it to FP16 and count how many values become exactly 0.\n3. Apply a scale factor \\(S\\), cast again, then unscale.\n\nThis shows the core mechanism of loss scaling *without* needing a full training run."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Gradient underflow demo\n\ndef underflow_report(grads: torch.Tensor, dtype: torch.dtype):\n    g = grads.to(dtype)\n    zeros = (g == 0).float().mean().item()\n    finite = torch.isfinite(g).float().mean().item()\n    return {\"dtype\": str(dtype), \"zero_frac\": zeros, \"finite_frac\": finite}\n\n\n# Log-uniform magnitudes: covers many orders of magnitude.\nN = 200_000\nlog10_mag = torch.empty(N).uniform_(-12, 0)  # 1e-12 .. 1\nsign = torch.randint(0, 2, (N,)) * 2 - 1\nsynthetic = (10 ** log10_mag) * sign\nsynthetic = synthetic.to(torch.float32)\n\nrows = []\nrows.append({\"setting\": \"unscaled\", **underflow_report(synthetic, torch.float16)})\nrows.append({\"setting\": \"unscaled\", **underflow_report(synthetic, torch.bfloat16)})\n\nS = 2**13  # roughly 8192, common starting point\nscaled = synthetic * S\nrows.append({\"setting\": f\"scaled by {S}\", **underflow_report(scaled, torch.float16)})\nrows.append({\"setting\": f\"scaled by {S}\", **underflow_report(scaled, torch.bfloat16)})\n\npd.DataFrame(rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize the distribution and FP16 representable thresholds\n\nfi16 = torch.finfo(torch.float16)\nmin_normal = float(fi16.tiny)\nmin_sub = float(torch.nextafter(torch.tensor(0.0, dtype=torch.float16), torch.tensor(1.0, dtype=torch.float16)))\n\nvals = synthetic.abs().cpu().numpy()\nplt.figure()\nplt.hist(np.log10(vals + 1e-30), bins=200)\nplt.axvline(np.log10(min_normal), color=\"r\", linestyle=\"--\", label=\"FP16 min normal\")\nplt.axvline(np.log10(min_sub), color=\"m\", linestyle=\":\", label=\"FP16 min subnormal\")\nplt.title(\"Synthetic |grad| distribution (log10) with FP16 thresholds\")\nplt.xlabel(\"log10(|grad|)\")\nplt.ylabel(\"count\")\nplt.legend();",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4.1 Underflow in a real backward pass (FP16) + rescue via scaling\n\nThe synthetic demo above shows how casting kills tiny values.\n\nNow we’ll show the *actual training failure mode*:\n\n- Build a tiny FP16 network.\n- Choose inputs/targets that produce very small gradients.\n- Compare gradients with and without loss scaling.\n\nThis is the smallest end-to-end demonstration of why scaling exists."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Real backward underflow demo (tiny MLP)\n\ndef tiny_backward(use_scaling: bool, scale: float = 2**13):\n    model = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1)).to(device)\n    model = model.to(torch.float16)\n\n    x = (torch.randn(256, 128, device=device) * 1e-3).to(torch.float16)\n    y = (torch.randn(256, 1, device=device) * 1e-3).to(torch.float16)\n\n    pred = model(x)\n    loss = ((pred - y) ** 2).mean()  # MSE\n\n    if use_scaling:\n        (loss * scale).backward()\n        # Unscale grads (mimics GradScaler's unscale step)\n        for p in model.parameters():\n            if p.grad is not None:\n                p.grad.div_(scale)\n    else:\n        loss.backward()\n\n    grads = torch.cat([p.grad.flatten().abs().to(torch.float32) for p in model.parameters() if p.grad is not None])\n    return float(loss), float((grads == 0).float().mean()), float(torch.median(grads)), grads\n\n\nif device.type == \"cuda\":\n    loss0, z0, med0, g0 = tiny_backward(use_scaling=False)\n    loss1, z1, med1, g1 = tiny_backward(use_scaling=True)\n\n    pd.DataFrame([\n        {\"setting\": \"fp16 no scaling\", \"loss\": loss0, \"zero_grad_frac\": z0, \"median|grad|\": med0},\n        {\"setting\": \"fp16 scaled+unscaled\", \"loss\": loss1, \"zero_grad_frac\": z1, \"median|grad|\": med1},\n    ])\nelse:\n    print(\"This demo is most meaningful on CUDA (FP16 backward kernels).\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.5 Weight update stagnation (why FP32 master weights matter)\n\nEven if you avoid underflow, you can still lose learning signal if your **weight updates are below the spacing** of the weight’s dtype.\n\nExample idea:\n\n- If \\(w \\approx 1\\) in FP16, the spacing (ULP) is ~\\(2^{-10} \\approx 9.8\\times 10^{-4}\\).\n- Any update smaller than that can get rounded away.\n\nLet’s demonstrate by applying a sequence of tiny updates and checking whether the weight changes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Weight stagnation demo\n\ndef apply_updates(dtype: torch.dtype, w0: float, delta: float, steps: int = 2000):\n    w = torch.tensor(w0, dtype=dtype)\n    changed = 0\n    for _ in range(steps):\n        w_new = (w - torch.tensor(delta, dtype=dtype))\n        changed += int(w_new.item() != w.item())\n        w = w_new\n    return {\n        \"dtype\": str(dtype),\n        \"w0\": w0,\n        \"delta\": delta,\n        \"steps\": steps,\n        \"num_steps_where_w_changed\": changed,\n        \"final_w\": float(w),\n        \"ulp_at_one\": ulp_at_one(dtype),\n    }\n\nrows = []\nfor dtype in [torch.float16, torch.bfloat16, torch.float32]:\n    rows.append(apply_updates(dtype, w0=1.0, delta=1e-5, steps=2000))\n\npd.DataFrame(rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.6 The main event: train a tiny causal transformer under different precision regimes\n\nWe’ll train a small GPT-like model on a tiny in-notebook text corpus (character-level next-token prediction).\n\nWhy character-level?\n\n- No external downloads.\n- Stable and deterministic.\n- Still exercises the transformer mechanics that matter for autocast.\n\nWe will compare:\n\n- FP32 baseline\n- FP16 naïve (no autocast, no scaling) — expected to be unstable on many models\n- AMP FP16 (autocast + GradScaler)\n- AMP BF16 (autocast BF16, no scaler)\n\nWe will log and plot:\n\n- training loss\n- gradient norm\n- step time\n- (CUDA) memory\n- (FP16 AMP) dynamic loss scale\n\nTip: start with the default `FAST_DEV_RUN` setting in the suite-definition cell. For longer curves, increase `BASE_STEPS`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Tiny corpus + char-level tokenizer\n\nlines = [\n    \"Autocast is not a global cast.\",\n    \"It is an operator policy.\",\n    \"Some ops run in lower precision for speed.\",\n    \"Some ops run in float32 for stability.\",\n    \"\",\n    \"Loss scaling rescues fp16 gradients from underflow.\",\n    \"Bfloat16 usually has enough exponent range to avoid underflow.\",\n    \"\",\n    \"Transformers amplify numeric issues via softmax, layernorm, and large reductions.\",\n    \"AMP exists to route the right operations to the right dtype.\",\n]\n\ncorpus = \"\\n\".join(lines).strip() * 50\n\n# Build vocab\nchars = sorted(list(set(corpus)))\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for ch, i in stoi.items()}\nvocab_size = len(chars)\n\n\ndef encode(s: str):\n    return [stoi[c] for c in s]\n\n\ndef decode(ids):\n    return \"\".join(itos[i] for i in ids)\n\n\ndata = torch.tensor(encode(corpus), dtype=torch.long)\n\n# Train/val split\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\nprint(\"vocab_size:\", vocab_size)\nprint(\"train tokens:\", len(train_data), \"val tokens:\", len(val_data))\nprint(\"sample decode:\\n\")\nprint(decode(train_data[:200].tolist()))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Batch sampling\n\ndef get_batch(split: str, batch_size: int, block_size: int):\n    src = train_data if split == \"train\" else val_data\n    ix = torch.randint(len(src) - block_size - 1, (batch_size,))\n    x = torch.stack([src[i : i + block_size] for i in ix])\n    y = torch.stack([src[i + 1 : i + block_size + 1] for i in ix])\n    return x.to(device), y.to(device)\n\n\n@torch.no_grad()\ndef estimate_loss(model, block_size: int, batch_size: int, iters: int = 20):\n    model.eval()\n    out = {}\n    for split in [\"train\", \"val\"]:\n        losses = []\n        for _ in range(iters):\n            x, y = get_batch(split, batch_size, block_size)\n            logits = model(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n            losses.append(float(loss))\n        out[split] = float(np.mean(losses))\n    model.train()\n    return out",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Training utilities (logging loss, grad norms, speed, and optional eval)\n\n@dataclass\nclass TrainConfig:\n    name: str\n    steps: int = 200\n    batch_size: int = 32\n    block_size: int = 64\n    lr: float = 3e-4\n    weight_decay: float = 0.0\n\n    # Precision knobs\n    use_autocast: bool = False\n    amp_dtype: torch.dtype | None = None\n    use_grad_scaler: bool = False\n    param_dtype: torch.dtype = torch.float32\n\n    # Logging knobs\n    eval_interval: int | None = 50\n    eval_iters: int = 10\n\n\ndef global_grad_norm(model: nn.Module) -> float:\n    total_sq = 0.0\n    for p in model.parameters():\n        if p.grad is None:\n            continue\n        g = p.grad.detach().float()\n        total_sq += float(g.norm()) ** 2\n    return math.sqrt(total_sq)\n\n\ndef train_one(cfg: TrainConfig):\n    model = TinyGPT(\n        vocab_size=vocab_size,\n        block_size=cfg.block_size,\n        n_layer=2,\n        n_embd=128,\n        n_heads=4,\n        dropout=0.0,\n    ).to(device)\n    model = model.to(cfg.param_dtype)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    device_type = device.type if device.type != \"mps\" else \"cpu\"\n\n    scaler = None\n    if cfg.use_grad_scaler:\n        scaler = GradScaler(enabled=(device.type == \"cuda\"))\n\n    logs = {\n        \"step\": [],\n        \"train_loss\": [],\n        \"grad_norm\": [],\n        \"step_time_ms\": [],\n        \"tokens_per_s\": [],\n        \"scale\": [],\n        \"cuda_mem_mb\": [],\n        \"val_step\": [],\n        \"val_loss\": [],\n    }\n\n    status = \"ok\"\n\n    if device.type == \"cuda\":\n        torch.cuda.reset_peak_memory_stats()\n\n    tokens_per_step = cfg.batch_size * cfg.block_size\n\n    for step in range(cfg.steps):\n        t0 = time.perf_counter()\n\n        x, y = get_batch(\"train\", cfg.batch_size, cfg.block_size)\n        optimizer.zero_grad(set_to_none=True)\n\n        ctx = autocast(device_type=device_type, dtype=cfg.amp_dtype, enabled=cfg.use_autocast)\n        with ctx:\n            logits = model(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n\n        if not torch.isfinite(loss):\n            status = \"non_finite_loss\"\n            break\n\n        if scaler is None:\n            loss.backward()\n            grad_norm = global_grad_norm(model)\n            optimizer.step()\n            scale_val = float(\"nan\")\n        else:\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            grad_norm = global_grad_norm(model)\n            scaler.step(optimizer)\n            scaler.update()\n            scale_val = float(scaler.get_scale())\n\n        t1 = time.perf_counter()\n        dt = max(t1 - t0, 1e-12)\n\n        logs[\"step\"].append(step)\n        logs[\"train_loss\"].append(float(loss))\n        logs[\"grad_norm\"].append(float(grad_norm))\n        logs[\"step_time_ms\"].append(dt * 1000)\n        logs[\"tokens_per_s\"].append(tokens_per_step / dt)\n        logs[\"scale\"].append(scale_val)\n\n        if device.type == \"cuda\":\n            logs[\"cuda_mem_mb\"].append(torch.cuda.max_memory_allocated() / 1024**2)\n        else:\n            logs[\"cuda_mem_mb\"].append(float(\"nan\"))\n\n        if cfg.eval_interval is not None and (step % cfg.eval_interval == 0 or step == cfg.steps - 1):\n            try:\n                losses = estimate_loss(model, cfg.block_size, cfg.batch_size, iters=cfg.eval_iters)\n                logs[\"val_step\"].append(step)\n                logs[\"val_loss\"].append(losses[\"val\"])\n            except Exception:\n                logs[\"val_step\"].append(step)\n                logs[\"val_loss\"].append(float(\"nan\"))\n\n    return {\"config\": cfg, \"status\": status, \"logs\": logs}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Define the experiment suite (conditionally, based on device support)\n\n# A small “dev run” knob. CPU runs can be slow; CUDA runs are usually fast enough.\nFAST_DEV_RUN = (device.type != \"cuda\")\nBASE_STEPS = 60 if FAST_DEV_RUN else 200\n\nsuite = []\n\nsuite.append(TrainConfig(\n    name=\"fp32\",\n    steps=BASE_STEPS,\n    use_autocast=False,\n    amp_dtype=None,\n    use_grad_scaler=False,\n    param_dtype=torch.float32,\n))\n\nif device.type == \"cuda\":\n    # Naïve FP16: params in fp16, no autocast, no scaler.\n    suite.append(TrainConfig(\n        name=\"fp16_naive\",\n        steps=BASE_STEPS,\n        use_autocast=False,\n        amp_dtype=None,\n        use_grad_scaler=False,\n        param_dtype=torch.float16,\n    ))\n\n    # AMP FP16: params fp32, autocast fp16, scaler on.\n    suite.append(TrainConfig(\n        name=\"amp_fp16\",\n        steps=BASE_STEPS,\n        use_autocast=True,\n        amp_dtype=torch.float16,\n        use_grad_scaler=True,\n        param_dtype=torch.float32,\n    ))\n\n    # AMP BF16: params fp32, autocast bf16, no scaler.\n    if torch.cuda.is_bf16_supported():\n        suite.append(TrainConfig(\n            name=\"amp_bf16\",\n            steps=BASE_STEPS,\n            use_autocast=True,\n            amp_dtype=torch.bfloat16,\n            use_grad_scaler=False,\n            param_dtype=torch.float32,\n        ))\n\nelif device.type == \"cpu\":\n    # CPU autocast is typically bfloat16.\n    suite.append(TrainConfig(\n        name=\"amp_bf16_cpu\",\n        steps=BASE_STEPS,\n        use_autocast=True,\n        amp_dtype=torch.bfloat16,\n        use_grad_scaler=False,\n        param_dtype=torch.float32,\n    ))\n\nprint(\"Planned runs:\")\nfor cfg in suite:\n    print(\"-\", cfg.name, \"steps\", cfg.steps, \"param\", cfg.param_dtype, \"autocast\", cfg.use_autocast, cfg.amp_dtype, \"scaler\", cfg.use_grad_scaler)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run training experiments\n\nresults = []\n\nfor cfg in suite:\n    print()\n    print(\"=== Running:\", cfg.name, \"===\")\n    res = train_one(cfg)\n    print(\"status:\", res[\"status\"], \"steps:\", len(res[\"logs\"][\"step\"]))\n    results.append(res)\n\n# Quick summary table\nsummary_rows = []\nfor r in results:\n    cfg = r[\"config\"]\n    logs = r[\"logs\"]\n    steps_ran = len(logs[\"step\"])\n\n    final_train_loss = logs[\"train_loss\"][-1] if steps_ran else None\n    mean_step_ms = float(np.mean(logs[\"step_time_ms\"])) if steps_ran else None\n    mean_tokens_per_s = float(np.mean(logs[\"tokens_per_s\"])) if steps_ran else None\n\n    # Best-effort: latest val loss if we logged any\n    final_val_loss = logs[\"val_loss\"][-1] if len(logs[\"val_loss\"]) else None\n\n    peak_cuda_mem_mb = None\n    if device.type == \"cuda\" and steps_ran:\n        peak_cuda_mem_mb = float(np.nanmax(np.array(logs[\"cuda_mem_mb\"], dtype=np.float64)))\n\n    summary_rows.append({\n        \"name\": cfg.name,\n        \"status\": r[\"status\"],\n        \"steps_ran\": int(steps_ran),\n        \"final_train_loss\": final_train_loss,\n        \"final_val_loss\": final_val_loss,\n        \"mean_step_ms\": mean_step_ms,\n        \"mean_tokens_per_s\": mean_tokens_per_s,\n        \"peak_cuda_mem_mb\": peak_cuda_mem_mb,\n    })\n\npd.DataFrame(summary_rows)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot: training + validation loss curves\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nfor r in results:\n    name = r[\"config\"].name\n    logs = r[\"logs\"]\n\n    step = np.array(logs[\"step\"])\n    train_loss = np.array(logs[\"train_loss\"]) \n\n    if len(train_loss):\n        ax[0].plot(step, train_loss, label=f\"{name} ({r['status']})\")\n\n    if len(logs[\"val_loss\"]):\n        ax[1].plot(logs[\"val_step\"], logs[\"val_loss\"], marker=\"o\", linestyle=\"-\", label=f\"{name} ({r['status']})\")\n\nax[0].set_title(\"Train loss vs step\")\nax[0].set_xlabel(\"step\")\nax[0].set_ylabel(\"loss\")\n\nax[1].set_title(\"Val loss vs step (periodic eval)\")\nax[1].set_xlabel(\"step\")\nax[1].set_ylabel(\"loss\")\n\nax[0].legend();\nax[1].legend();",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot: step time and throughput\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nfor r in results:\n    name = r[\"config\"].name\n    logs = r[\"logs\"]\n\n    step = np.array(logs[\"step\"])\n    ms = np.array(logs[\"step_time_ms\"]) \n    tps = np.array(logs[\"tokens_per_s\"]) \n\n    if len(ms) == 0:\n        continue\n\n    ax[0].plot(step, ms, label=name)\n    ax[1].plot(step, tps, label=name)\n\nax[0].set_title(\"Step time (ms)\")\nax[0].set_xlabel(\"step\")\nax[0].set_ylabel(\"ms\")\n\nax[1].set_title(\"Throughput (tokens/s)\")\nax[1].set_xlabel(\"step\")\nax[1].set_ylabel(\"tokens/s\")\n\nax[0].legend();\nax[1].legend();",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot: GradScaler scale over time (only meaningful for amp_fp16)\n\nplt.figure(figsize=(10, 3))\nplotted = False\n\nfor r in results:\n    if r[\"config\"].name != \"amp_fp16\":\n        continue\n\n    logs = r[\"logs\"]\n    step = np.array(logs[\"step\"])\n    scale = np.array(logs[\"scale\"], dtype=np.float64)\n\n    if len(scale) == 0 or np.all(np.isnan(scale)):\n        continue\n\n    plt.plot(step, scale)\n    plotted = True\n\nplt.title(\"GradScaler scale (AMP FP16)\")\nplt.xlabel(\"step\")\nplt.ylabel(\"scale\")\n\nif not plotted:\n    print(\"No GradScaler scale data to plot (did amp_fp16 run?)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.7 Interpreting results (what to look for)\n\n### Loss curves\n\n- If `fp16_naive` diverges or hits `non_finite_loss`, that’s a *feature*: it illustrates why mixed precision needs scaling + policies.\n- `amp_fp16` should usually be stable if the model and learning rate are reasonable.\n- `amp_bf16` is often stable without scaling (when supported).\n\n### Step time\n\n- On modern GPUs, `amp_fp16`/`amp_bf16` often reduce step time because matmuls hit Tensor Cores.\n- If you don’t see speedup, common reasons:\n  - model is too small (overhead dominates)\n  - you’re CPU-bound\n  - you’re not actually using CUDA\n\n### GradScaler scale\n\n- If scale drops repeatedly, you’re hitting overflow (inf/nan) events.\n- If scale grows over time, training is stable and GradScaler is increasing “headroom”."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.8 Practical checklist (AMP in real code)\n\n### Defaults that usually work\n\n- Prefer **BF16 autocast** if your GPU supports it.\n- Otherwise use **FP16 autocast + GradScaler**.\n- Keep optimizer state in FP32 (default for most PyTorch optimizers).\n\n### Debugging steps\n\n1. If loss becomes `nan`/`inf`: check for overflow sources (attention logits, exp/log, unstable loss).\n2. If gradients are mostly 0 in FP16: use GradScaler / increase scale.\n3. If training “does nothing”: check for update stagnation (are weights changing?) and whether you inadvertently cast master weights.\n4. Use dtype hooks to confirm what ran in what dtype.\n\n### Common gotchas\n\n- Mixing manual `.half()` casts with autocast can lead to unexpected behavior.\n- Gradient clipping should happen **after** unscale (if using GradScaler).\n- `autocast` should cover forward + loss, not optimizer step."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot: CUDA memory (if available)\n\nif device.type == \"cuda\":\n    plt.figure(figsize=(10, 3))\n    for r in results:\n        name = r[\"config\"].name\n        logs = r[\"logs\"]\n        step = np.array(logs[\"step\"])\n        mem = np.array(logs[\"cuda_mem_mb\"], dtype=np.float64)\n        if len(mem) == 0:\n            continue\n        plt.plot(step, mem, label=name)\n\n    plt.title(\"Max CUDA memory allocated (MB)\")\n    plt.xlabel(\"step\")\n    plt.ylabel(\"MB\")\n    plt.legend();\nelse:\n    print(\"CUDA not available; skipping memory plot\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Appendix — References and further reading\n\n- Micikevicius et al., *Mixed Precision Training* (arXiv:1710.03740)\n- NVIDIA mixed precision training guides/blog posts (engineering implementation view)\n- PyTorch docs: `torch.amp` (`autocast`, `GradScaler`) and the autocast op reference\n- BF16 background materials (design intent: FP32 range with fewer mantissa bits)\n- LLM training system docs (FSDP / ZeRO / DeepSpeed): mixed precision interacts with sharding and optimizer state\n\nThis notebook’s experiments are intentionally small; the *mechanisms* are the same at scale.\n\nGenerated: 2026-02-22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  },
  "title": "Autocast / AMP in PyTorch: a deep practical reference",
  "generated": "2026-02-22"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}